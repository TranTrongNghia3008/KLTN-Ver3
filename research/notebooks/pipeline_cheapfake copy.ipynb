{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fa0e08",
   "metadata": {},
   "source": [
    "# Part 1: Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "import base64\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import concurrent.futures\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02175477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9b4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Chromedriver\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--window-size=1920x1080')  # Ensure the window size is large enough\n",
    "\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"accept-language=en-US,en;q=0.9\")\n",
    "chrome_options.add_argument(\"referer=https://www.google.com/\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.57 Safari/537.36\"\n",
    ")\n",
    "# chrome_options.binary_location = '/usr/bin/chromium-browser'\n",
    "\n",
    "GEMINI_API_KEYS = [\n",
    "    \"AIzaSyATfpBIyIKcajRMxGy9-1tbNP5xwgLua3U\",\n",
    "    \"AIzaSyDRSjpj3Gb6TUqrpLVKQfewxzGwJspjEIQ\",\n",
    "    \"AIzaSyBWaCqJzCHVHUeWuv9XEeSo6ne9cV39vec\",\n",
    "    \"AIzaSyAhm_7o7Lnj4C5FoWU9QkOcTacjnj_YiMU\"\n",
    "]\n",
    "\n",
    "def switch_api_key(current_key_index):\n",
    "    global model\n",
    "    new_key_index = (current_key_index + 1) % len(GEMINI_API_KEYS)\n",
    "    client = genai.Client(api_key=GEMINI_API_KEYS[new_key_index])\n",
    "    return new_key_index\n",
    "\n",
    "current_key_index = 0\n",
    "client = genai.Client(api_key=GEMINI_API_KEYS[current_key_index])\n",
    "\n",
    "MINIMUM_K = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e07c10",
   "metadata": {},
   "source": [
    "# Part 2: Crawl related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1aa6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_links(query):\n",
    "  driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "  prompt = f'https://www.bing.com/search?q={query}'\n",
    "  print(prompt)\n",
    "  driver.get(prompt)\n",
    "  time.sleep(random.uniform(1, 10))\n",
    "  # print(driver.page_source)\n",
    "\n",
    "  articles = driver.find_elements(By.CSS_SELECTOR, \"#b_results li.b_algo\")\n",
    "  link_articles = []\n",
    "  # link_articles.append({\n",
    "  #     'title': query[:30],\n",
    "  #     'link': prompt,\n",
    "  #     # 'summary': summary\n",
    "  # })\n",
    "  print(f\"Found {len(articles)} relevant links:\\n{articles}\")\n",
    "  for article in articles[:MINIMUM_K]:  # Giới hạn lấy 5 kết quả đầu tiên\n",
    "    try:\n",
    "      title_element = article.find_element(By.TAG_NAME, \"h2\").find_element(By.TAG_NAME, \"a\")\n",
    "      title = title_element.text\n",
    "      link = title_element.get_attribute('href')\n",
    "      # summary = article.find_element(By.CLASS_NAME, 'css-16nhkrn').text\n",
    "      # local = local_query(link)\n",
    "      link_articles.append({\n",
    "          'title': title,\n",
    "          'link': link,\n",
    "          # 'summary': summary\n",
    "      })\n",
    "      print(title)\n",
    "      print(link)\n",
    "    except Exception as e:\n",
    "        print(\"Lỗi khi trích xuất bài viết:\", e)\n",
    "  driver.quit()\n",
    "\n",
    "  print(f\"Found {len(link_articles)} relevant links:\\n{link_articles}\")\n",
    "\n",
    "  return link_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_dismiss_popups(driver):\n",
    "    try:\n",
    "        # Các nút phổ biến cần nhấn\n",
    "        popup_texts = [\n",
    "            \"Accept Cookies\", \"Accept All Cookies\", \"I Accept\",\n",
    "            \"Agree\", \"Press & Hold\", \"Continue\"\n",
    "        ]\n",
    "        for text in popup_texts:\n",
    "            try:\n",
    "                btn = driver.find_element(\n",
    "                    By.XPATH,\n",
    "                    f\"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{text.lower()}')]\"\n",
    "                )\n",
    "                btn.click()\n",
    "                print(f\"✅ Clicked popup button: '{text}'\")\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            except ElementClickInterceptedException:\n",
    "                continue\n",
    "\n",
    "        # Tìm các nút có class name chứa 'close'\n",
    "        close_candidates = driver.find_elements(By.XPATH, \"//button[contains(@class, 'close') or contains(translate(@aria-label, 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'close')]\")\n",
    "\n",
    "        for btn in close_candidates:\n",
    "            try:\n",
    "                btn.click()\n",
    "                print(\"✅ Clicked a close button\")\n",
    "                break\n",
    "            except (ElementClickInterceptedException, Exception):\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error while dismissing popup: {e}\")\n",
    "\n",
    "def process_article_link(article, max_retries=5):\n",
    "    \"\"\"Hàm xử lý một liên kết riêng lẻ và trả về nội dung gộp các thẻ <p>\"\"\"\n",
    "    article_crawl = {\n",
    "        \"title\": article['title'],\n",
    "        \"src\": article['link'],\n",
    "        \"contents\": \"\"  # gộp tất cả <p> vào 1 chuỗi\n",
    "    }\n",
    "\n",
    "    success = False\n",
    "    wait_time = 10  # thời gian chờ ban đầu (giây)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            print(f\"⏳ Attempt {attempt}: Crawling {article['link']} with wait_time={wait_time}s\")\n",
    "            driver.get(article['link'])\n",
    "            time.sleep(wait_time)\n",
    "            try_dismiss_popups(driver)\n",
    "\n",
    "            all_elements = driver.find_elements(By.XPATH, \".//p\")\n",
    "            contents = []\n",
    "\n",
    "            for element in all_elements:\n",
    "                if element.tag_name == \"p\":\n",
    "                    text_content = element.get_attribute(\"innerText\").strip()\n",
    "                    if text_content:\n",
    "                        contents.append(text_content)\n",
    "\n",
    "            article_crawl[\"contents\"] = \"\\n\".join(contents)\n",
    "            print(f'✅ Crawled content from {article[\"link\"]}:\\n{article_crawl[\"contents\"][:500]}...')  # in 500 ký tự đầu tiên\n",
    "            success = True\n",
    "            break  # thành công thì thoát\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt} failed for {article['link']}: {e}\")\n",
    "            wait_time += 300  # tăng thời gian chờ thêm 10s cho mỗi lần thử lại\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    if not success:\n",
    "        print(f\"❌ Failed to crawl article from {article['link']} after {max_retries} attempts.\")\n",
    "\n",
    "    return article_crawl\n",
    "\n",
    "\n",
    "\n",
    "def crawl_articles(query, article_url, crawl_json):\n",
    "    \"\"\"Hàm chính để crawl các trang khác\"\"\"\n",
    "    \n",
    "    url_articles = [{\n",
    "          'title': \"Article contains caption\",\n",
    "          'link': article_url,\n",
    "          # 'summary': summary\n",
    "    }]\n",
    "    \n",
    "    try:\n",
    "        url_articles.extend(search_relevant_links(query))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error while searching for relevant links: {e}\")\n",
    "\n",
    "    # Giới hạn số lượng link cần crawl\n",
    "    url_articles = url_articles[:MINIMUM_K + 1]\n",
    "    \n",
    "    print(url_articles)\n",
    "\n",
    "    # Sử dụng Multi-threading để chạy nhiều request song song\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        result = list(executor.map(process_article_link, url_articles))\n",
    "\n",
    "    # Gộp kết quả vào crawl_json\n",
    "    crawl_json.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd8e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Julian Castro at his announcement in San Antonio, Tex., on Saturday. Mr. Castro, the former secretary of housing and urban development, would be one of the youngest presidents if elected.\"\n",
    "# crawl_json = []\n",
    "# article_url = \"https://www.nytimes.com/2019/06/13/us/politics/julian-castro-fox-town-hall.html\"\n",
    "\n",
    "# crawl_articles(query, article_url , crawl_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawl_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e40a3",
   "metadata": {},
   "source": [
    "# Part 3: Fact-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def lable(item):\n",
    "  try:\n",
    "\n",
    "    image_path = f\"../data/images_test_mmsys/{item['img_local_path']}\"  # Điều chỉnh đường dẫn hình ảnh\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    image\n",
    "\n",
    "    caption1 = item['caption1']\n",
    "    caption2 = item['caption2']\n",
    "    article_texts = item['article_texts']\n",
    "    prompt = f\"\"\"A new is considered fake if the tag of the aritcle is one of these tags: 'Research In Progress', 'True', 'Mostly True', 'Mixture', 'Mostly False', 'False, Unproven', 'Unfounded', 'Outdated', 'Miscaptioned', 'Correct Attribution', 'Misattributed', 'Legend', 'Scam', 'Legit', 'Labeled Satire', 'Originated as Satire', 'Recall', 'Lost Legend', 'Fake'.\n",
    "    Return 1 if the news in file is fake or the caption not included in the content, else return 0.\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash',\n",
    "        contents=[article_texts, image, caption1, caption2, prompt],\n",
    "        config={\n",
    "            'response_mime_type': 'text/x.enum',\n",
    "            'response_schema': {\n",
    "                \"type\": \"STRING\",\n",
    "                \"enum\": [\"0\", \"1\"],\n",
    "            },\n",
    "            'safety_settings': [\n",
    "                types.SafetySetting(\n",
    "                    category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "                    threshold=types.HarmBlockThreshold.BLOCK_NONE,\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "        },\n",
    "\n",
    "    )\n",
    "    \n",
    "    response \n",
    "\n",
    "    if response and hasattr(response, 'text') and response.text:\n",
    "        result = response.text\n",
    "    else:\n",
    "        result = \"No response or blocked prompt\"\n",
    "\n",
    "  except Exception as e:\n",
    "        result = f\"Error: {str(e)}\"\n",
    "\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa37dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item = {\"img_local_path\": \"public_test_mmsys/0.jpg\", \"caption1\": \"Julian Castro at his announcement in San Antonio, Tex., on Saturday. Mr. Castro, the former secretary of housing and urban development, would be one of the youngest presidents if elected.\", \"caption2\": \"Julian Castro at his announcement in San Antonio, Tex., on Saturday, Jan. 12, 2019.\", \"context_label\": 0, \"article_url\": \"https://www.nytimes.com/2019/06/13/us/politics/julian-castro-fox-town-hall.html\", \"maskrcnn_bboxes\": [[389.9706726074219, 72.9228744506836, 505.0566711425781, 373.24993896484375], [89.46248626708984, 312.29644775390625, 190.55088806152344, 396.4997253417969], [116.25189971923828, 225.38841247558594, 161.36624145507812, 288.41522216796875], [180.07785034179688, 225.37646484375, 207.3575439453125, 271.2514953613281], [579.815185546875, 193.33509826660156, 597.6293334960938, 249.89108276367188], [217.98863220214844, 225.41282653808594, 256.5491638183594, 267.5371398925781], [67.05160522460938, 237.61740112304688, 92.31876373291016, 275.8415222167969], [29.469621658325195, 240.86349487304688, 64.6895980834961, 276.5841369628906], [229.984375, 298.4461669921875, 251.81227111816406, 330.3661804199219], [89.82146453857422, 205.71160888671875, 104.25022888183594, 228.6795196533203]], \"caption1_modified\": \"PERSON at his announcement in GPE, GPE, on DATE. Mr. PERSON, the former secretary of housing and urban development, would be one of the youngest presidents if elected.\", \"caption1_entities\": [[\"Julian Castro\", \"PERSON\"], [\"San Antonio\", \"GPE\"], [\"Tex.\", \"GPE\"], [\"Saturday\", \"DATE\"], [\"Castro\", \"PERSON\"]], \"caption2_modified\": \"PERSON at his announcement in GPE, GPE, on DATE.\", \"caption2_entities\": [[\"Julian Castro\", \"PERSON\"], [\"San Antonio\", \"GPE\"], [\"Tex.\", \"GPE\"], [\"Saturday, Jan. 12, 2019\", \"DATE\"]], \"bert_base_score\": \"0.5769946\", \"bert_large_score\": \"0.60118324\"}\n",
    "# crawl_json = [item for item in crawl_json if item is not None]\n",
    "\n",
    "# # Nối lại toàn bộ nội dung: thêm title và content mỗi bài\n",
    "# article_texts = \"\\n\\n\".join(\n",
    "#     f\"### {item.get('title', 'No Title')}\\n{item.get('contents', '').strip()}\"\n",
    "#     for item in crawl_json\n",
    "#     if item.get(\"contents\")\n",
    "# )\n",
    "\n",
    "# item['article_texts'] = article_texts\n",
    "\n",
    "# result = lable(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa5100f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c163c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error crawling caption2: name 'crawl_articles' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     51\u001b[39m article_texts = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     52\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNo Title\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mx.get(\u001b[33m'\u001b[39m\u001b[33mcontents\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m crawl_json \u001b[38;5;28;01mif\u001b[39;00m x.get(\u001b[33m\"\u001b[39m\u001b[33mcontents\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m )\n\u001b[32m     55\u001b[39m item[\u001b[33m'\u001b[39m\u001b[33marticle_texts\u001b[39m\u001b[33m'\u001b[39m] = article_texts\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m lable_item = \u001b[43mlable\u001b[49m(item)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m lable_item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33m0\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m}:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlable_item\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'lable' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Replace with your actual file path\n",
    "file_path = '../data/cosmos_anns_mmsys/mmsys_anns/public_test_mmsys_final.json'\n",
    "output_file = \"cheapfake_results.json\"\n",
    "\n",
    "START_INDEX = 0  # <- Sửa chỗ này để bắt đầu từ test thứ mấy\n",
    "current_key_index = 0  # Giả định bạn dùng nhiều API key\n",
    "\n",
    "# Load data\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            json_object = json.loads(line)\n",
    "            data.append(json_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Line: {line}, error: {e}\")\n",
    "\n",
    "# Nếu đã có file kết quả thì tiếp tục từ đó\n",
    "try:\n",
    "    with open(output_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    completed_ids = {item[\"img_local_path\"] for item in results}\n",
    "except FileNotFoundError:\n",
    "    results = []\n",
    "    completed_ids = set()\n",
    "\n",
    "correct_tests = 0\n",
    "total_tests = 0\n",
    "\n",
    "for i, item in enumerate(data[START_INDEX:], start=START_INDEX):\n",
    "    if item[\"img_local_path\"] in completed_ids:\n",
    "        continue  # Skip nếu đã xử lý rồi\n",
    "\n",
    "    crawl_json = []\n",
    "    # try:\n",
    "    #     crawl_articles(item['caption1'], crawl_json)  \n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error crawling caption1: {e}\")\n",
    "    \n",
    "    try:\n",
    "        crawl_articles(item['caption2'], item['article_url'], crawl_json)\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling caption2: {e}\")\n",
    "    # crawl_articles(item['caption1'], crawl_json)\n",
    "    # crawl_articles(item['caption2'], crawl_json)\n",
    "\n",
    "    crawl_json = [x for x in crawl_json if x is not None]\n",
    "\n",
    "    article_texts = \"\\n\\n\".join(\n",
    "        f\"### {x.get('title', 'No Title')}\\n{x.get('contents', '').strip()}\"\n",
    "        for x in crawl_json if x.get(\"contents\")\n",
    "    )\n",
    "    item['article_texts'] = article_texts\n",
    "\n",
    "    lable_item = lable(item)\n",
    "    while lable_item not in {'0', '1'}:\n",
    "        print(f\"Invalid label: {lable_item}\")\n",
    "        lable_item = lable(item)\n",
    "\n",
    "    item['cheapfake_label'] = int(lable_item)\n",
    "    results.append(item)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"[{i}] Result: {item['cheapfake_label']} - Ground Truth: {item['context_label']}\")\n",
    "\n",
    "    if item['cheapfake_label'] == item['context_label']:\n",
    "        correct_tests += 1\n",
    "    total_tests += 1\n",
    "\n",
    "    # Lưu kết quả ngay sau mỗi mẫu\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(f\"✅ Saved {len(results)} results so far. Accuracy: {correct_tests / total_tests:.2%}\")\n",
    "    current_key_index = switch_api_key(current_key_index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
