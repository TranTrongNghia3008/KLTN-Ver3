{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9adbf56",
   "metadata": {},
   "source": [
    "# Part 1: Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd2845",
   "metadata": {},
   "source": [
    "## 1.1 Install Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a46a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (1.93.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22027d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\nghia\\appdata\\local\\temp\\pip-req-build-mdfzmkgy\n",
      "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: more-itertools in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (10.7.0)\n",
      "Requirement already satisfied: numba in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (0.61.2)\n",
      "Requirement already satisfied: numpy in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (2.2.6)\n",
      "Requirement already satisfied: tiktoken in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (0.9.0)\n",
      "Requirement already satisfied: torch in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (2.7.1)\n",
      "Requirement already satisfied: tqdm in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from numba->openai-whisper==20250625) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
      "Requirement already satisfied: filelock in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (2025.5.1)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20250625) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\Nghia\\AppData\\Local\\Temp\\pip-req-build-mdfzmkgy'\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyannote.audio in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: asteroid-filterbanks>=0.4 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.4.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.33.2)\n",
      "Requirement already satisfied: lightning>=2.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.5.2)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.3.0)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (5.0.0)\n",
      "Requirement already satisfied: pyannote.database>=5.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (5.1.3)\n",
      "Requirement already satisfied: pyannote.metrics>=3.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (3.2.1)\n",
      "Requirement already satisfied: pyannote.pipeline>=3.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (3.0.1)\n",
      "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.8.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (14.0.0)\n",
      "Requirement already satisfied: semver>=3.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (3.0.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.13.1)\n",
      "Requirement already satisfied: speechbrain>=1.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (1.0.3)\n",
      "Requirement already satisfied: tensorboardX>=2.6 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.6.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.7.1)\n",
      "Requirement already satisfied: torch-audiomentations>=0.11.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.12.0)\n",
      "Requirement already satisfied: torchaudio>=2.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.7.1)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (1.7.3)\n",
      "Requirement already satisfied: numpy in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.2.6)\n",
      "Requirement already satisfied: typing-extensions in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.14.0)\n",
      "Requirement already satisfied: filelock in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from lightning>=2.0.1->pyannote.audio) (0.14.3)\n",
      "Requirement already satisfied: pytorch-lightning in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from lightning>=2.0.1->pyannote.audio) (2.5.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n",
      "Requirement already satisfied: pandas>=0.19 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.3.0)\n",
      "Requirement already satisfied: typer>=0.12.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.7.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.3)\n",
      "Requirement already satisfied: sympy>=1.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.14.0)\n",
      "Requirement already satisfied: optuna>=3.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (4.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
      "Requirement already satisfied: hyperpyyaml in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (1.2.2)\n",
      "Requirement already satisfied: joblib in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tensorboardX>=2.6->pyannote.audio) (6.31.1)\n",
      "Requirement already satisfied: networkx in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.5)\n",
      "Requirement already satisfied: pycparser in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (3.12.13)\n",
      "Requirement already satisfied: setuptools in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (65.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.16.2)\n",
      "Requirement already satisfied: colorlog in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
      "Requirement already satisfied: primePy>=1.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.0->pyannote.audio) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.18.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.6.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.20.1)\n",
      "Requirement already satisfied: Mako in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.10)\n",
      "Requirement already satisfied: six>=1.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.2.12)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: opencv-python in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (2.32.4)\n",
      "Requirement already satisfied: proglog<=1.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (0.1.12)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (2.2.6)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2025.6.15)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install pyannote.audio\n",
    "!pip install moviepy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca86f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt-dlp in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (2025.6.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a6a458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (0.33.2)\n",
      "Requirement already satisfied: filelock in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.14.0)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33261970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromedriver-autoinstaller in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from chromedriver-autoinstaller) (25.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install chromedriver-autoinstaller\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce3c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'dpkg' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'dpkg' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'CHROME_DRIVER_VERSION' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.34.0-py3-none-any.whl (9.4 MB)\n",
      "Collecting urllib3[socks]~=2.4.0\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Collecting trio~=0.30.0\n",
      "  Using cached trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Collecting trio-websocket~=0.12.2\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (4.14.0)\n",
      "Collecting websocket-client~=1.8.0\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: pycparser in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Installing collected packages: wsproto, websocket-client, urllib3, pysocks, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.34.0 trio-0.30.0 trio-websocket-0.12.2 urllib3-2.4.0 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!sudo apt -y update\n",
    "!sudo apt install -y wget curl unzip\n",
    "!wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
    "!dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
    "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "!dpkg -i google-chrome-stable_current_amd64.deb\n",
    "!CHROME_DRIVER_VERSION=$(curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE)\n",
    "!wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
    "!unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
    "!chmod +x /tmp/chromedriver\n",
    "!mv /tmp/chromedriver /usr/local/bin/chromedriver\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c97dc1",
   "metadata": {},
   "source": [
    "## 1.2 Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9327cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1411688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict\n",
    "import yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b13240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df685bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba2ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53b6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For crawling data\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import concurrent.futures\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b59196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8569af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c0161",
   "metadata": {},
   "source": [
    "## 1.3 Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5a81aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cf38943",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://localhost:8000/deepfake/detect/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99af49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Chromedriver\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--window-size=1920x1080')  # Ensure the window size is large enough\n",
    "\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"accept-language=en-US,en;q=0.9\")\n",
    "chrome_options.add_argument(\"referer=https://www.google.com/\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.57 Safari/537.36\"\n",
    ")\n",
    "# chrome_options.binary_location = '/usr/bin/chromium-browser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffd41f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9645c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a806",
   "metadata": {},
   "source": [
    "# Part 2: Extract statements that need fact-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "825bb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Extract audio from video ---\n",
    "def extract_audio(video_path: str, audio_dir: str = \"audios\") -> str:\n",
    "    if not os.path.exists(audio_dir):\n",
    "        os.makedirs(audio_dir)\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio_path = os.path.basename(video_path).replace('.mp4', '.wav')\n",
    "    audio_path = os.path.join(audio_dir, audio_path)\n",
    "    if os.path.exists(audio_path):\n",
    "        os.remove(audio_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    return audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fc2022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: Diarize audio (identify speakers) ---\n",
    "def diarize_audio(audio_path: str) -> List[Dict]:\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HUGGINGFACE_TOKEN)\n",
    "    diarization = pipeline(audio_path)\n",
    "    speakers = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speakers.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    return speakers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dc41532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: Transcribe audio ---\n",
    "def transcribe_audio(audio_path: str) -> List[Dict]:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"segments\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35273564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 4: Assign speakers to transcript segments ---\n",
    "def assign_speakers(segments: List[Dict], speakers: List[Dict]) -> List[Dict]:\n",
    "    output = []\n",
    "    for seg in segments:\n",
    "        speaker_label = \"unknown\"\n",
    "        for sp in speakers:\n",
    "            if sp[\"start\"] <= seg[\"start\"] <= sp[\"end\"]:\n",
    "                speaker_label = sp[\"speaker\"]\n",
    "                break\n",
    "        output.append({\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"speaker\": speaker_label,\n",
    "            \"text\": seg[\"text\"].strip()\n",
    "        })\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e23243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 5: Identify speaker names via text cues (OpenAI) ---\n",
    "class Speaker(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "\n",
    "class ListSpeakers(BaseModel):\n",
    "    listSpeakers: list[Speaker]\n",
    "\n",
    "\n",
    "def identify_speaker_names_via_text(transcript: List[Dict]) -> Dict:\n",
    "    transcript_text = \"\\n\".join(\n",
    "        [f\"{seg['speaker']}: {seg['text']}\" for seg in transcript]\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "    Below is the full transcript of a video, each line contains the speaker (SPEAKER_XX) and the dialogue.\n",
    "\n",
    "    Analyze to determine if there is any part where the speaker introduces himself or is introduced by someone else.\n",
    "\n",
    "    Returns a JSON result with the following structure:\n",
    "    {{\n",
    "      {{\n",
    "        id: \"SPEAKER_00\",\n",
    "        name: \"Name if available\",\n",
    "      }},\n",
    "      ...\n",
    "    }}\n",
    "\n",
    "    If not identified, returns the name field as \"Unnamed\".\n",
    "\n",
    "    Transcript:\n",
    "    {transcript_text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        text_format=ListSpeakers,\n",
    "    )\n",
    "\n",
    "    return response.output_parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9993052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_for_unknown_speakers(\n",
    "    video_path: str,\n",
    "    speaker_segments: List[Dict],\n",
    "    speaker_name_map,  # kiu: ListSpeakers ( cha list[Speaker(id, name)])\n",
    "    output_dir: str = \"frames\",\n",
    "    max_frames_per_speaker: int = 5\n",
    "):\n",
    "    import os\n",
    "    import cv2\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # To dict lookup tn t speaker_name_map\n",
    "    speaker_id_to_name = {s.id: s.name for s in speaker_name_map.listSpeakers}\n",
    "    speaker_frames = {}\n",
    "\n",
    "    for seg in speaker_segments:\n",
    "        spk = seg['speaker']\n",
    "        name = speaker_id_to_name.get(spk, \"\")\n",
    "        if name.startswith(\"Unnamed\"):\n",
    "            # Nu   5 frame th b qua\n",
    "            if spk in speaker_frames and len(speaker_frames[spk]) >= max_frames_per_speaker:\n",
    "                continue\n",
    "\n",
    "            mid_time = (seg['start'] + seg['end']) / 2\n",
    "            frame_num = int(mid_time * fps)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_path = os.path.join(output_dir, f\"{spk}_{int(seg['start'])}.jpg\")\n",
    "                cv2.imwrite(frame_path, frame)\n",
    "                if spk not in speaker_frames:\n",
    "                    speaker_frames[spk] = []\n",
    "                speaker_frames[spk].append({\n",
    "                    \"time\": mid_time,\n",
    "                    \"frame_path\": frame_path,\n",
    "                    \"text\": seg[\"text\"]\n",
    "                })\n",
    "\n",
    "    cap.release()\n",
    "    return speaker_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5f7b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def identify_unknown_speakers_with_gpt(speaker_frames: dict) -> dict:\n",
    "    \"\"\"\n",
    "    speaker_frames: {\n",
    "        \"SPEAKER_01\": [\n",
    "            {\"time\": ..., \"frame_path\": ..., \"text\": ...},\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    speaker_id_to_name = {}\n",
    "\n",
    "    for speaker_id, frames in speaker_frames.items():\n",
    "        print(f\"\\n ang x l {speaker_id}...\")\n",
    "\n",
    "        # Chun b prompt chnh\n",
    "        texts = [f'{f[\"text\"]}' for f in frames if f.get(\"text\")]\n",
    "        combined_text = \"\\n\".join(texts)  # Dng ti a 3 on transcript\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "This is a collection of frames extracted from a video showing one speaker. Based on their appearance and the following quotes, can you identify who they are or make an educated guess?\n",
    "\n",
    "Quotes:\n",
    "{combined_text}\n",
    "\n",
    "Returns only the speaker's name (no further explanation needed).\n",
    "\n",
    "If you can't tell, reply with \"Unnamed\".\n",
    "\"\"\"\n",
    "\n",
    "        # Chun b nh\n",
    "        content_items = [{\"type\": \"input_text\", \"text\": prompt}]\n",
    "        for f in frames:\n",
    "            image_path = f[\"frame_path\"]  # m bo ng path\n",
    "            try:\n",
    "                base64_image = encode_image(image_path)\n",
    "                content_items.append({\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\" Khng th c nh {image_path}: {e}\")\n",
    "\n",
    "        # Gi yu cu ln GPT\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                input=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content_items\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            name = response.output_text\n",
    "            speaker_id_to_name[speaker_id] = name\n",
    "            print(f\" {speaker_id}  {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error for {speaker_id}: {e}\")\n",
    "            speaker_id_to_name[speaker_id] = \"Unnamed\"\n",
    "\n",
    "    return speaker_id_to_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11cae5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_named_transcript(transcript, speaker_name_map, new_names):\n",
    "    # Bc 1: Gp tn t speaker_name_map v new_names\n",
    "    speaker_lookup = {}\n",
    "    for speaker in speaker_name_map.listSpeakers:\n",
    "        speaker_id = speaker.id\n",
    "        # u tin tn t new_names nu c\n",
    "        name = new_names.get(speaker_id, speaker.name)\n",
    "        speaker_lookup[speaker_id] = name\n",
    "\n",
    "    # Bc 2: To transcript cui cng\n",
    "    final_transcript = []\n",
    "    for seg in transcript:\n",
    "        spk = seg['speaker']\n",
    "        if spk == \"unknown\":\n",
    "            display_name = \"Unknown\"\n",
    "        else:\n",
    "            name = speaker_lookup.get(spk, spk)\n",
    "            display_name = name if name != \"Unnamed\" else spk\n",
    "\n",
    "        final_transcript.append({\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"speaker\": display_name,\n",
    "            \"text\": seg[\"text\"]\n",
    "        })\n",
    "\n",
    "    return final_transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4313bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statement(BaseModel):\n",
    "    start: float\n",
    "    end: float\n",
    "    speaker: str\n",
    "    text: str\n",
    "    reason: str  # Ti sao cn kim chng\n",
    "    context: str\n",
    "\n",
    "class ListStatement(BaseModel):\n",
    "    listStatment: List[Statement]\n",
    "\n",
    "\n",
    "def split_transcript(transcript, chunk_size=100):\n",
    "    \"\"\"Chia transcript thnh cc on nh  trnh qu di\"\"\"\n",
    "    return [transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size)]\n",
    "\n",
    "\n",
    "def find_checkworthy_statements(final_transcript, model=\"gpt-4o\"):\n",
    "    parts = split_transcript(final_transcript, chunk_size=300)\n",
    "    all_statements = []\n",
    "\n",
    "    for idx, part in enumerate(parts):\n",
    "        print(f\" ang x l phn {idx+1}/{len(parts)}...\")\n",
    "\n",
    "        # To vn bn nhp\n",
    "        lines = [f\"[{r['start']:.2f}-{r['end']:.2f}] {r['speaker']}: {r['text']}\" for r in part]\n",
    "        input_text = \"\\n\".join(lines)\n",
    "        print(input_text)\n",
    "\n",
    "        prompt = \"\"\"You are a professional fact-checking assistant.\n",
    "            Your job is to extract verbatim **checkworthy statements** from political transcripts, debates, interviews, or speeches.\n",
    "            Must find at least 1 statement, if the video is too short with 1 person speaking then extract the entire video\n",
    "\n",
    "            A **checkworthy statement** typically:\n",
    "            - Contains a factual claim or statistic.\n",
    "            - Mentions historical events, conflicts, or political actions.\n",
    "            - Suggests a cause-effect relationship (e.g. \"if I were president, this would never happen\").\n",
    "            - Blames or credits someone for an outcome (e.g. immigration, war, economy).\n",
    "            - Makes bold or potentially controversial assertions.\n",
    "\n",
    "            Avoid:\n",
    "            - Opinions or vague generalities (e.g. \"I love America\").\n",
    "            - Greetings, filler speech, or rhetorical questions with no factual basis.\n",
    "\n",
    "            ### Output Format\n",
    "            Return a list of structured statements in this format:\n",
    "            - `start`: float  start time in seconds\n",
    "            - `end`: float  end time in seconds\n",
    "            - `speaker`: str  name of the speaker\n",
    "            - `text`: str  the exact quote **verbatim** that is checkworthy\n",
    "            - `reason`: str  short explanation why this should be fact-checked\n",
    "            - `context`: str  describing the **context** of the statement. This must include:\n",
    "                + Where the quote was made (e.g., in a presidential debate, TV interview, campaign rally, etc.)  infer this if possible\n",
    "                + When it occurred (date or relative time, e.g., \"during the 2024 campaign\", or \"in June 2025\")  infer from available information\n",
    "                + What topic was being discussed immediately before and after the statement (e.g., foreign policy, immigration, etc.)\n",
    "                + If the speaker was responding to a question or another speaker, note that as well\n",
    "          \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.responses.parse(\n",
    "                model=model,\n",
    "                input=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": input_text,\n",
    "                    },\n",
    "                ],\n",
    "                text_format=ListStatement,\n",
    "            )\n",
    "            statements = response.output_parsed.listStatment\n",
    "            all_statements.extend(statements)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Li  phn {idx+1}: {e}\")\n",
    "\n",
    "    return all_statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "535eb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frame_for_statement(video_path: str, statement, output_dir=\"statement_frames\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    mid_time = (statement.start + statement.end) / 2\n",
    "    frame_num = int(mid_time * fps)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        filename = f\"{statement.speaker}_{int(statement.start*100):06d}.jpg\"\n",
    "        frame_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        return frame_path\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9249f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements_from_video(video_path: str):\n",
    "    \"\"\"\n",
    "    Extracts the transcript from a video file.\n",
    "    \"\"\"\n",
    "    print(\" Extracting audio...\")\n",
    "    audio_path = extract_audio(video_path)\n",
    "    \n",
    "    print(\" Diarizing speakers...\")\n",
    "    speakers = diarize_audio(audio_path)\n",
    "    \n",
    "    print(\" Transcribing...\")\n",
    "    segments = transcribe_audio(audio_path)\n",
    "    \n",
    "    print(\" Assigning speakers...\")\n",
    "    transcript = assign_speakers(segments, speakers)\n",
    "\n",
    "    print(\" Inferring speaker names from transcript...\")\n",
    "    speaker_name_map = identify_speaker_names_via_text(transcript)\n",
    "    \n",
    "    print(\" Extracting frames for unknown speakers...\")\n",
    "    speaker_frames = extract_frames_for_unknown_speakers(video_path, transcript, speaker_name_map)\n",
    "    \n",
    "    print(\" Identifying unknown speakers with GPT...\")\n",
    "    new_names = identify_unknown_speakers_with_gpt(speaker_frames)\n",
    "\n",
    "    print(\" Generating final transcript...\")\n",
    "    final = generate_named_transcript(transcript, speaker_name_map, new_names)\n",
    "    \n",
    "    print(\" Finding checkworthy statements...\")\n",
    "    final_statements = find_checkworthy_statements(final)\n",
    "    \n",
    "    print(f\" Found {len(final_statements)} checkworthy statements.\")\n",
    "    \n",
    "    final_statements_json = []\n",
    "\n",
    "    for s in final_statements:\n",
    "        frame_path = extract_frame_for_statement(video_path, s)\n",
    "\n",
    "        statement_dict = s.dict()\n",
    "        statement_dict[\"frame_path\"] = frame_path or \"N/A\"\n",
    "\n",
    "        final_statements_json.append(statement_dict)\n",
    "    \n",
    "    return final_statements_json\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb3c65",
   "metadata": {},
   "source": [
    "# Part 3: Deepfake prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "391d1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_video_into_clips(final_statements_json, video_path: str):\n",
    "    \"\"\"\n",
    "    Cuts the video into clips based on the provided statements.\n",
    "    \"\"\"\n",
    "    clip_dir = os.path.join(\"statement_clips\", os.path.splitext(os.path.basename(video_path))[0])\n",
    "    if not os.path.exists(clip_dir):\n",
    "        os.makedirs(clip_dir, exist_ok=True)     \n",
    "    else:\n",
    "        # otherwise empty the existing directory\n",
    "        for f in os.listdir(clip_dir):\n",
    "            file_path = os.path.join(clip_dir, f)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "\n",
    "    for i, s in enumerate(final_statements_json):\n",
    "        start, end = s['start'], s['end']\n",
    "        clip_path = os.path.join(clip_dir, f\"clip_{i+1}.mp4\")\n",
    "\n",
    "        print(f\" Cutting {clip_path} from {start}s to {end}s\")\n",
    "        clip = VideoFileClip(video_path).subclip(start, end)\n",
    "        clip.write_videofile(clip_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n",
    "    \n",
    "    print(f\" Finished cutting video into clips. Saved to {clip_dir}\")\n",
    "    return clip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2abe0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api_detect_deepfake(clip_dir: str):\n",
    "    \"\"\"\n",
    "    Detects deepfake in the video clips.\n",
    "    \"\"\"\n",
    "    # To danh sch file  gi\n",
    "    video_files = []\n",
    "    for filename in os.listdir(clip_dir):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            file_path = os.path.join(clip_dir, filename)\n",
    "            video_files.append((\"videos\", (filename, open(file_path, \"rb\"), \"video/mp4\")))\n",
    "\n",
    "    # Gi yu cu POST\n",
    "    print(\" Sending batch videos to deepfake API...\")\n",
    "    response = requests.post(API_URL, files=video_files)\n",
    "\n",
    "    # Kt qu\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\" Detection results:\")\n",
    "        for fname, r in result.items():\n",
    "            print(f\"{fname}: {r}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\" Error {response.status_code}: {response.text}\")\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_deepfake(final_statements_json, video_path: str):\n",
    "    \"\"\"\n",
    "    Main function to detect deepfake in the video.\n",
    "    \"\"\"\n",
    "    print(\" Cutting video into clips...\")\n",
    "    clip_dir = cut_video_into_clips(final_statements_json, video_path)\n",
    "\n",
    "    print(\" Detecting deepfake in clips...\")\n",
    "    results = call_api_detect_deepfake(clip_dir)\n",
    "\n",
    "    for idx, statement in enumerate(final_statements_json):\n",
    "        clip_name = f\"clip_{idx+1}.mp4\"\n",
    "        result = results.get(clip_name, {})\n",
    "        \n",
    "        # Ly nhn deepfake nu c\n",
    "        label = result.get(\"pred_label\", [\"unknown\"])[0]\n",
    "        \n",
    "        # Gn vo statement\n",
    "        statement[\"deepfake_label\"] = label\n",
    "    \n",
    "    print(\" Finished detecting deepfake.\")\n",
    "    return final_statements_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58286fb",
   "metadata": {},
   "source": [
    "# Part 4: Crawl related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb4cd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_links(query):\n",
    "  driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "  prompt = f'https://www.bing.com/search?q={query}'\n",
    "  print(prompt)\n",
    "  driver.get(prompt)\n",
    "  time.sleep(random.uniform(1, 10))\n",
    "  # print(driver.page_source)\n",
    "\n",
    "  articles = driver.find_elements(By.CSS_SELECTOR, \"#b_results li.b_algo\")\n",
    "  link_articles = []\n",
    "  link_articles.append({\n",
    "      'title': query[:30],\n",
    "      'link': prompt,\n",
    "      # 'summary': summary\n",
    "  })\n",
    "  print(f\"Found {len(articles)} relevant links:\\n{articles}\")\n",
    "  for article in articles[:MINIMUM_K]:  # Gii hn ly 5 kt qu u tin\n",
    "    try:\n",
    "      title_element = article.find_element(By.TAG_NAME, \"h2\").find_element(By.TAG_NAME, \"a\")\n",
    "      title = title_element.text\n",
    "      link = title_element.get_attribute('href')\n",
    "      # summary = article.find_element(By.CLASS_NAME, 'css-16nhkrn').text\n",
    "      # local = local_query(link)\n",
    "      link_articles.append({\n",
    "          'title': title,\n",
    "          'link': link,\n",
    "          # 'summary': summary\n",
    "      })\n",
    "      print(title)\n",
    "      print(link)\n",
    "    except Exception as e:\n",
    "        print(\"Li khi trch xut bi vit:\", e)\n",
    "  driver.quit()\n",
    "\n",
    "  print(f\"Found {len(link_articles)} relevant links:\\n{link_articles}\")\n",
    "\n",
    "  return link_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "376faf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_dismiss_popups(driver):\n",
    "    try:\n",
    "        # Cc nt ph bin cn nhn\n",
    "        popup_texts = [\n",
    "            \"Accept Cookies\", \"Accept All Cookies\", \"I Accept\",\n",
    "            \"Agree\", \"Press & Hold\", \"Continue\"\n",
    "        ]\n",
    "        for text in popup_texts:\n",
    "            try:\n",
    "                btn = driver.find_element(\n",
    "                    By.XPATH,\n",
    "                    f\"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{text.lower()}')]\"\n",
    "                )\n",
    "                btn.click()\n",
    "                print(f\" Clicked popup button: '{text}'\")\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            except ElementClickInterceptedException:\n",
    "                continue\n",
    "\n",
    "        # Tm cc nt c class name cha 'close'\n",
    "        close_candidates = driver.find_elements(By.XPATH, \"//button[contains(@class, 'close') or contains(translate(@aria-label, 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'close')]\")\n",
    "\n",
    "        for btn in close_candidates:\n",
    "            try:\n",
    "                btn.click()\n",
    "                print(\" Clicked a close button\")\n",
    "                break\n",
    "            except (ElementClickInterceptedException, Exception):\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error while dismissing popup: {e}\")\n",
    "\n",
    "def process_article_link(article, max_retries=5):\n",
    "    \"\"\"Hm x l mt lin kt ring l v tr v ni dung gp cc th <p>\"\"\"\n",
    "    article_crawl = {\n",
    "        \"title\": article['title'],\n",
    "        \"src\": article['link'],\n",
    "        \"contents\": \"\"  # gp tt c <p> vo 1 chui\n",
    "    }\n",
    "\n",
    "    success = False\n",
    "    wait_time = 10  # thi gian ch ban u (giy)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            print(f\" Attempt {attempt}: Crawling {article['link']} with wait_time={wait_time}s\")\n",
    "            driver.get(article['link'])\n",
    "            time.sleep(wait_time)\n",
    "            try_dismiss_popups(driver)\n",
    "\n",
    "            all_elements = driver.find_elements(By.XPATH, \".//p\")\n",
    "            contents = []\n",
    "\n",
    "            for element in all_elements:\n",
    "                if element.tag_name == \"p\":\n",
    "                    text_content = element.get_attribute(\"innerText\").strip()\n",
    "                    if text_content:\n",
    "                        contents.append(text_content)\n",
    "\n",
    "            article_crawl[\"contents\"] = \"\\n\".join(contents)\n",
    "            print(f' Crawled content from {article[\"link\"]}:\\n{article_crawl[\"contents\"][:500]}...')  # in 500 k t u tin\n",
    "            success = True\n",
    "            break  # thnh cng th thot\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Attempt {attempt} failed for {article['link']}: {e}\")\n",
    "            wait_time += 300  # tng thi gian ch thm 10s cho mi ln th li\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    if not success:\n",
    "        print(f\" Failed to crawl article from {article['link']} after {max_retries} attempts.\")\n",
    "\n",
    "    return article_crawl\n",
    "\n",
    "\n",
    "\n",
    "def crawl_articles(query, crawl_json):\n",
    "    \"\"\"Hm chnh  crawl cc trang khc\"\"\"\n",
    "    url_articles = search_relevant_links(query)\n",
    "\n",
    "    # Gii hn s lng link cn crawl\n",
    "    url_articles = url_articles[:MINIMUM_K]\n",
    "\n",
    "    # S dng Multi-threading  chy nhiu request song song\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        result = list(executor.map(process_article_link, url_articles))\n",
    "\n",
    "    # Gp kt qu vo crawl_json\n",
    "    crawl_json.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65af687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_statement(statement):\n",
    "    query = statement[\"text\"].strip('\"')\n",
    "    crawl_json = []\n",
    "\n",
    "    # Crawl theo statement\n",
    "    crawl_articles(query, crawl_json=crawl_json)\n",
    "\n",
    "    # Crawl thm theo context\n",
    "    context = statement[\"context\"]\n",
    "    crawl_articles(query=context, crawl_json=crawl_json)\n",
    "\n",
    "    # Loi b cc kt qu None\n",
    "    crawl_json = [item for item in crawl_json if item is not None]\n",
    "\n",
    "    # Ni li ton b ni dung: thm title v content mi bi\n",
    "    article_texts = \"\\n\\n\".join(\n",
    "        f\"### {item.get('title', 'No Title')}\\n{item.get('contents', '').strip()}\"\n",
    "        for item in crawl_json\n",
    "        if item.get(\"contents\")\n",
    "    )\n",
    "\n",
    "    # Tr li enriched statement\n",
    "    enriched_statement = statement.copy()\n",
    "    enriched_statement[\"article_texts\"] = article_texts.strip()\n",
    "\n",
    "    return enriched_statement\n",
    "\n",
    "\n",
    "def enrich_statements_with_articles(final_statements_json):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        return list(executor.map(process_single_statement, final_statements_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577ea3f",
   "metadata": {},
   "source": [
    "# Part 5: Fact-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7666d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactCheck(BaseModel):\n",
    "    sentence: str\n",
    "    label: bool  # True = SUPPORTED, False = REFUTED\n",
    "    explanation: str\n",
    "    revised_sentence: str\n",
    "\n",
    "def fact_check(statement, article_texts=\"\"):\n",
    "    article_texts = statement['article_texts']\n",
    "    prompt = f\"\"\"\n",
    "You are a professional fact-checking assistant. Your task is to verify whether a given statement made by a public figure is factually accurate, using the reference documents provided.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "Follow this two-step fact-checking process:\n",
    "\n",
    "1. **Verify if the speaker actually made this statement**:\n",
    "   - Search the reference documents to determine whether the speaker is directly or credibly quoted as having said this, or something semantically equivalent.\n",
    "   - If such a quote or statement from the speaker is found, then:\n",
    "     - **Label = true**\n",
    "     - Provide an explanation saying the speaker did make this statement.\n",
    "     - Return the original sentence as `revised_sentence`.\n",
    "     - Do not evaluate the factual accuracy of the content  if the quote is confirmed, assume it is real.\n",
    "\n",
    "2. **If there is no evidence that the speaker made this statement**, proceed to assess the **factual accuracy** of the statement based on the reference documents:\n",
    "   - If it is supported by evidence, label as **true**, provide reasoning, and return the original sentence.\n",
    "   - If it is misleading or incorrect, label as **false**, explain why, and rewrite it correctly using only facts from the documents.\n",
    "\n",
    "Use only the information in the documents. Do not speculate or assume intent. Be concise and precise.\n",
    "\n",
    "### Context:\n",
    "{statement['context']}\n",
    "\n",
    "### Speaker:\n",
    "{statement['speaker']}\n",
    "\n",
    "### Statement:\n",
    "\"{statement['text']}\"\n",
    "\n",
    "### Documents:\n",
    "{article_texts}\n",
    "\n",
    "### Output format:\n",
    "- sentence: original statement\n",
    "- label: true (supported) or false (refuted)\n",
    "- explanation: why the statement is supported/refuted\n",
    "- revised_sentence: corrected version if refuted, or original statement if supported\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.responses.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical fact-checking expert.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            text_format=FactCheck\n",
    "        )\n",
    "        result = response.output_parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Problem with API: {e}\")\n",
    "        result = \"Unverified\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9219172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check_single_statement(statement):\n",
    "    print(statement)\n",
    "    text = statement[\"text\"]\n",
    "\n",
    "    print(f\" Fact-checking: {text[:80]}...\")\n",
    "\n",
    "    result = fact_check(statement)\n",
    "\n",
    "    if result == \"Unverified\":\n",
    "        statement[\"label\"] = None\n",
    "        statement[\"explanation\"] = \"Unverified due to API error.\"\n",
    "        statement[\"revised_statement\"] = text\n",
    "    else:\n",
    "        statement[\"label\"] = result.label\n",
    "        statement[\"explanation\"] = result.explanation\n",
    "        statement[\"revised_statement\"] = result.revised_sentence\n",
    "\n",
    "    return statement\n",
    "\n",
    "\n",
    "def run_fact_checks_parallel(enriched_statements, max_workers=4):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(fact_check_single_statement, enriched_statements))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check_video(video_path: str):\n",
    "    \"\"\"\n",
    "    Main function to fact-check the video.\n",
    "    \"\"\"\n",
    "    print(\" Extracting statements from video...\")\n",
    "    final_statements_json = extract_statements_from_video(video_path)\n",
    "\n",
    "    print(\" Detecting deepfake...\")\n",
    "    final_statements_json = detect_deepfake(final_statements_json, video_path)\n",
    "    \n",
    "    for statement in final_statements_json:\n",
    "        if statement[\"deepfake_label\"] == \"FAKE\":\n",
    "            print(f\" Statement {statement['text']} is marked as deepfake. Skipping fact-check.\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    print(\" Enriching statements with articles...\")\n",
    "    enriched_statements = enrich_statements_with_articles(final_statements_json)\n",
    "\n",
    "    print(\" Finished enriching statements with articles.\")\n",
    "\n",
    "    print(\" Running fact-checks on statements...\")\n",
    "    fact_checked_results = run_fact_checks_parallel(enriched_statements)\n",
    "    fact_checked_results\n",
    "\n",
    "    for result in fact_checked_results:\n",
    "        if result[\"label\"] == \"False\":\n",
    "            print(f\" Statement '{result.sentence}' is refuted: {result.explanation}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    \n",
    "    print(\" All statements are supported or unverified.\")\n",
    "    return True\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615209e",
   "metadata": {},
   "source": [
    "# Part 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3336f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: ../data/dfw_youtube_release\\940.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\940.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-3.40] Unknown: I'm going to have to go.\n",
      "[3.40-6.40] Addison: Gollum, introducing music tonight.\n",
      "[6.40-9.20] Addison: Where are my precious?\n",
      "[9.20-10.80] Addison: It's time for music.\n",
      "[10.80-14.80] Addison: You didn't get no, it's not such a surprise.\n",
      "[14.80-17.00] Addison: This is precious.\n",
      "[17.00-21.20] Addison: I love this thing for so many years now, you did not precious.\n",
      "[21.20-24.20] Addison: I did!\n",
      "[24.20-28.20] Addison: What's in a precious per name is Addison.\n",
      "[28.20-30.20] SPEAKER_00: Are you for a while?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12304\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\940\\clip_1.mp4 from 24.2s to 28.2s\n",
      " Finished cutting video into clips. Saved to statement_clips\\940\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['15623146458d4d1a9907b45d56cdd751_clip_1.mp4'], 'pred': [0.8758717775344849], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      "[{'start': 24.2, 'end': 28.2, 'speaker': 'Addison', 'text': \"What's in a precious per name is Addison.\", 'reason': 'This is a potentially cryptic statement that could imply a reference or allusion to something specific, warranting clarification.', 'context': 'The statement was made in what appears to be a performance or artistic setting, during a segment involving music and character play. The timing and thematic references suggest a creative exploration of identity or role-playing, but the specific meaning or reference is not clear without context.', 'frame_path': 'statement_frames\\\\Addison_002420.jpg', 'deepfake_label': 'FAKE'}]\n",
      " Statement What's in a precious per name is Addison. is marked as deepfake. Skipping fact-check.\n",
      " Enriching statements with articles...\n",
      "https://www.bing.com/search?q=What's in a precious per name is Addison.\n",
      "Found 9 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.26\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.27\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.28\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.29\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"60328f8b902bac80faf6267797a932f3\", element=\"f.22DAFF6651696246700337FF64EB95A9.d.9FE9ACA8EAD9A66AA35E5CC3F1B1AB6C.e.30\")>]\n",
      "Addison Name Rankings, Meanings, and Facts\n",
      "https://www.bing.com/ck/a?!&&p=4598eae646fcdfe3005197fd07976cbb8ab910df3ab74b83c79810ff67ceba44JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=29dc2034-be56-6a2f-19b9-3614bf666b2c&u=a1aHR0cHM6Ly93b3JsZG5hbWVkYXRhLmNvbS9uYW1lL2FkZGlzb24&ntb=1\n",
      "Addison spiritual meaning of the name\n",
      "https://www.bing.com/ck/a?!&&p=57b072aef0be945f2e405912dc260b48de8eef4ab2f5e134223e697184abe9d2JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=29dc2034-be56-6a2f-19b9-3614bf666b2c&u=a1aHR0cHM6Ly9zcGlyaXR1YWxtZWFuaW5nYWNhZGVteS5jb20vYWRkaXNvbi1zcGlyaXR1YWwtbWVhbmluZy1vZi10aGUtbmFtZS8&ntb=1\n",
      "Found 3 relevant links:\n",
      "[{'title': \"What's in a precious per name \", 'link': \"https://www.bing.com/search?q=What's in a precious per name is Addison.\"}, {'title': 'Addison Name Rankings, Meanings, and Facts', 'link': 'https://www.bing.com/ck/a?!&&p=4598eae646fcdfe3005197fd07976cbb8ab910df3ab74b83c79810ff67ceba44JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=29dc2034-be56-6a2f-19b9-3614bf666b2c&u=a1aHR0cHM6Ly93b3JsZG5hbWVkYXRhLmNvbS9uYW1lL2FkZGlzb24&ntb=1'}, {'title': 'Addison spiritual meaning of the name', 'link': 'https://www.bing.com/ck/a?!&&p=57b072aef0be945f2e405912dc260b48de8eef4ab2f5e134223e697184abe9d2JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=29dc2034-be56-6a2f-19b9-3614bf666b2c&u=a1aHR0cHM6Ly9zcGlyaXR1YWxtZWFuaW5nYWNhZGVteS5jb20vYWRkaXNvbi1zcGlyaXR1YWwtbWVhbmluZy1vZi10aGUtbmFtZS8&ntb=1'}]\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=What's in a precious per name is Addison. with wait_time=10s Attempt 1: Crawling https://www.bing.com/ck/a?!&&p=4598eae646fcdfe3005197fd07976cbb8ab910df3ab74b83c79810ff67ceba44JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=29dc2034-be56-6a2f-19b9-3614bf666b2c&u=a1aHR0cHM6Ly93b3JsZG5hbWVkYXRhLmNvbS9uYW1lL2FkZGlzb24&ntb=1 with wait_time=10s\n",
      "\n",
      " Crawled content from https://www.bing.com/search?q=What's in a precious per name is Addison.:\n",
      "See how popular Addison is in countries all over the world and whether it is used as a girls name or a boys name. Discover what Addison means in other languages and if it has any negative \n",
      "Sep 12, 2024 Hebrew adds being precious as jewels while Arabic reminds about personal beauty. Their spiritual essence always pushes them towards truth and deep bonding. Having \n",
      "4 days ago Contemporary and cool with a cute, sassy nickname, Addison has been a popular choice for celebrities. Baseball player...\n",
      " Crawled content from https://www.bing.com/ck/a?!&&p=4598eae646fcdfe3005197fd07976cbb8ab910df3ab74b83c79810ff67ceba44JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=29dc2034-be56-6a2f-19b9-3614bf666b2c&u=a1aHR0cHM6Ly93b3JsZG5hbWVkYXRhLmNvbS9uYW1lL2FkZGlzb24&ntb=1:\n",
      "Learn about the name Addison. See how popular Addison is in countries all over the world and whether it is used as a girls name or a boys name. Discover what Addison means in other languages and if it has any negative meanings.\n",
      "See how to say, spell, type, and pronounce Addison.\n",
      "Addison ARPAbet pronounciation: AE1 D AH0 S AH0 N\n",
      "Addison IPA pronounciation: dsn\n",
      "Addison in readable ASCII: addison\n",
      "Addison in hex: addison\n",
      "English words that rhyme with Addison.\n",
      "See what countries the name Addison ...\n",
      "https://www.bing.com/search?q=The statement was made in what appears to be a performance or artistic setting, during a segment involving music and character play. The timing and thematic references suggest a creative exploration of identity or role-playing, but the specific meaning or reference is not clear without context.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"b2f932b79801eb689a1d111096fd9279\", element=\"f.EE5C472248FECBEA4F2C2121C6AC0941.d.5238FB314E2A7B165447FA29F3E995A2.e.26\")>]\n",
      "Art Appreciation Ch. 2.8 Flashcards | Quizlet\n",
      "https://www.bing.com/ck/a?!&&p=0fb652290734c13a40a3413e1ac50574f9334af0e47db3f734d0ce3ccdd30e5cJmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=38ab47db-d4a0-667e-0f66-51fbd58e67b9&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8xOTMyNzk1MTYvYXJ0LWFwcHJlY2lhdGlvbi1jaC0yOC1mbGFzaC1jYXJkcy8&ntb=1\n",
      "Art Appreciation InQuizitive 4.9 - The Body in Art Flashcards\n",
      "https://www.bing.com/ck/a?!&&p=b3d939105c1b0eab520cce677eda5508133037bfb9358b9a096eeb1edfb25b43JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=38ab47db-d4a0-667e-0f66-51fbd58e67b9&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS80MzA4Mjc4MzcvYXJ0LWFwcHJlY2lhdGlvbi1pbnF1aXppdGl2ZS00OS10aGUtYm9keS1pbi1hcnQtZmxhc2gtY2FyZHMv&ntb=1\n",
      "Found 3 relevant links:\n",
      "[{'title': 'The statement was made in what', 'link': 'https://www.bing.com/search?q=The statement was made in what appears to be a performance or artistic setting, during a segment involving music and character play. The timing and thematic references suggest a creative exploration of identity or role-playing, but the specific meaning or reference is not clear without context.'}, {'title': 'Art Appreciation Ch. 2.8 Flashcards | Quizlet', 'link': 'https://www.bing.com/ck/a?!&&p=0fb652290734c13a40a3413e1ac50574f9334af0e47db3f734d0ce3ccdd30e5cJmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=38ab47db-d4a0-667e-0f66-51fbd58e67b9&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8xOTMyNzk1MTYvYXJ0LWFwcHJlY2lhdGlvbi1jaC0yOC1mbGFzaC1jYXJkcy8&ntb=1'}, {'title': 'Art Appreciation InQuizitive 4.9 - The Body in Art Flashcards', 'link': 'https://www.bing.com/ck/a?!&&p=b3d939105c1b0eab520cce677eda5508133037bfb9358b9a096eeb1edfb25b43JmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=38ab47db-d4a0-667e-0f66-51fbd58e67b9&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS80MzA4Mjc4MzcvYXJ0LWFwcHJlY2lhdGlvbi1pbnF1aXppdGl2ZS00OS10aGUtYm9keS1pbi1hcnQtZmxhc2gtY2FyZHMv&ntb=1'}]\n",
      " Attempt 1: Crawling https://www.bing.com/ck/a?!&&p=0fb652290734c13a40a3413e1ac50574f9334af0e47db3f734d0ce3ccdd30e5cJmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=38ab47db-d4a0-667e-0f66-51fbd58e67b9&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8xOTMyNzk1MTYvYXJ0LWFwcHJlY2lhdGlvbi1jaC0yOC1mbGFzaC1jYXJkcy8&ntb=1 with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=The statement was made in what appears to be a performance or artistic setting, during a segment involving music and character play. The timing and thematic references suggest a creative exploration of identity or role-playing, but the specific meaning or reference is not clear without context. with wait_time=10s\n",
      " Crawled content from https://www.bing.com/search?q=The statement was made in what appears to be a performance or artistic setting, during a segment involving music and character play. The timing and thematic references suggest a creative exploration of identity or role-playing, but the specific meaning or reference is not clear without context.:\n",
      "Because the camera appears to capture an image of an event exactly as it occurred, viewers often believe that the resulting photograph is ________ record of events. 1. Fundamentals of \n",
      "In a series of works, the performance artist ORLAN transformed her appearance with plastic surgery in order to match famously beautiful depictions of women in art.\n",
      "Study with Quizlet and memorize flashcards containing terms like What is an art song?, Identify the correct definition of \"through-composed.\", Which f...\n",
      " Crawled content from https://www.bing.com/ck/a?!&&p=0fb652290734c13a40a3413e1ac50574f9334af0e47db3f734d0ce3ccdd30e5cJmltdHM9MTc1MTY3MzYwMA&ptn=3&ver=2&hsh=4&fclid=38ab47db-d4a0-667e-0f66-51fbd58e67b9&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8xOTMyNzk1MTYvYXJ0LWFwcHJlY2lhdGlvbi1jaC0yOC1mbGFzaC1jYXJkcy8&ntb=1:\n",
      " 2025 Quizlet, Inc.\n",
      "Were running a quick security check to verify youre not a bot.\n",
      "Ray ID: 95a7482cc9acafdd-SIN...\n",
      " Finished enriching statements with articles.\n",
      " Running fact-checks on statements...\n",
      "{'start': 24.2, 'end': 28.2, 'speaker': 'Addison', 'text': \"What's in a precious per name is Addison.\", 'reason': 'This is a potentially cryptic statement that could imply a reference or allusion to something specific, warranting clarification.', 'context': 'The statement was made in what appears to be a performance or artistic setting, during a segment involving music and character play. The timing and thematic references suggest a creative exploration of identity or role-playing, but the specific meaning or reference is not clear without context.', 'frame_path': 'statement_frames\\\\Addison_002420.jpg', 'deepfake_label': 'FAKE', 'article_texts': '### What\\'s in a precious per name \\nSee how popular Addison is in countries all over the world and whether it is used as a girls name or a boys name. Discover what Addison means in other languages and if it has any negative \\nSep 12, 2024\\xa0 Hebrew adds being precious as jewels while Arabic reminds about personal beauty. Their spiritual essence always pushes them towards truth and deep bonding. Having \\n4 days ago\\xa0 Contemporary and cool with a cute, sassy nickname, Addison has been a popular choice for celebrities. Baseball player Jason Bay, TV \\nThe name Addison is of English origin and has multiple meanings. It is derived from the Old English surname meaning \"son of Adam.\" It can also be \\n\\n### Addison Name Rankings, Meanings, and Facts\\nLearn about the name Addison. See how popular Addison is in countries all over the world and whether it is used as a girls name or a boys name. Discover what Addison means in other languages and if it has any negative meanings.\\nSee how to say, spell, type, and pronounce Addison.\\nAddison ARPAbet pronounciation: AE1 D AH0 S AH0 N\\nAddison IPA pronounciation: dsn\\nAddison in readable ASCII: addison\\nAddison in hex: addison\\nEnglish words that rhyme with Addison.\\nSee what countries the name Addison is most popular in.\\nSee where Addison is used as a girls name.\\nSee where Addison is used as a boys name.\\nSee how popular Addison is for boys or girls in 6 country data sets.\\nGlobal data on the ethnicity of the name Addison.\\nGender, generation, birth year, and other predictions for the name Addison.\\nSomeone with the name Addison was most likely born in 2007.\\nSomeone with the name Addison is most likely to be andy.\\nSomeone with the name Addison is most likely from this generation: Generation Z.\\nFind similar names to Addison.\\n\\n### The statement was made in what\\nBecause the camera appears to capture an image of an event exactly as it occurred, viewers often believe that the resulting photograph is ________ record of events. 1. Fundamentals of \\nIn a series of works, the performance artist ORLAN transformed her appearance with plastic surgery in order to match famously beautiful depictions of women in art.\\nStudy with Quizlet and memorize flashcards containing terms like What is an art song?, Identify the correct definition of \"through-composed.\", Which form is often used for setting hymns and \\nApr 18, 2024\\xa0 An artist statement is a written explanation of an artists work, which can include their artistic process, influences and intentions. Artists use artist statements as a way to \\n\\n### Art Appreciation Ch. 2.8 Flashcards | Quizlet\\n 2025 Quizlet, Inc.\\nWere running a quick security check to verify youre not a bot.\\nRay ID: 95a7482cc9acafdd-SIN'}\n",
      " Fact-checking: What's in a precious per name is Addison....\n",
      " All statements are supported or unverified.\n",
      " Video ../data/dfw_youtube_release\\940.mp4 passed the fact-check.\n"
     ]
    }
   ],
   "source": [
    "VIDEO_DIR = \"../data/dfw_youtube_release\"\n",
    "\n",
    "video_files = [\n",
    "    os.path.join(VIDEO_DIR, f) for f in os.listdir(VIDEO_DIR)\n",
    "    if f.endswith((\".mp4\", \".avi\", \".mov\"))\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for video_path in video_files:\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    try:\n",
    "        result = fact_check_video(video_path)\n",
    "        if result:\n",
    "            print(f\" Video {video_path} passed the fact-check.\")\n",
    "        else:\n",
    "            print(f\" Video {video_path} failed the fact-check.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing video {video_path}: {e}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"video_path\": video_path,   \n",
    "        \"result\": result\n",
    "    })\n",
    "\n",
    "# Save results to a file\n",
    "import json\n",
    "with open(\"fact_check_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
