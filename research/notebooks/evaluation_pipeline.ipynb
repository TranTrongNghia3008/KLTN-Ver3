{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9adbf56",
   "metadata": {},
   "source": [
    "# Part 1: Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd2845",
   "metadata": {},
   "source": [
    "## 1.1 Install Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a46a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (1.93.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22027d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\nghia\\appdata\\local\\temp\\pip-req-build-5fa3mnxp\n",
      "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: more-itertools in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (10.7.0)\n",
      "Requirement already satisfied: numba in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (0.61.2)\n",
      "Requirement already satisfied: numpy in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (2.2.6)\n",
      "Requirement already satisfied: tiktoken in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (0.9.0)\n",
      "Requirement already satisfied: torch in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (2.7.1)\n",
      "Requirement already satisfied: tqdm in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from openai-whisper==20250625) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from numba->openai-whisper==20250625) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
      "Requirement already satisfied: filelock in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch->openai-whisper==20250625) (2025.5.1)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm->openai-whisper==20250625) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from jinja2->torch->openai-whisper==20250625) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\Nghia\\AppData\\Local\\Temp\\pip-req-build-5fa3mnxp'\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyannote.audio in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: asteroid-filterbanks>=0.4 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.4.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.33.2)\n",
      "Requirement already satisfied: lightning>=2.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.5.2)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.3.0)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (5.0.0)\n",
      "Requirement already satisfied: pyannote.database>=5.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (5.1.3)\n",
      "Requirement already satisfied: pyannote.metrics>=3.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (3.2.1)\n",
      "Requirement already satisfied: pyannote.pipeline>=3.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (3.0.1)\n",
      "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.8.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (14.0.0)\n",
      "Requirement already satisfied: semver>=3.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (3.0.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.13.1)\n",
      "Requirement already satisfied: speechbrain>=1.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (1.0.3)\n",
      "Requirement already satisfied: tensorboardX>=2.6 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.6.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.7.1)\n",
      "Requirement already satisfied: torch-audiomentations>=0.11.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (0.12.0)\n",
      "Requirement already satisfied: torchaudio>=2.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (2.7.1)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.audio) (1.7.3)\n",
      "Requirement already satisfied: numpy in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (2.2.6)\n",
      "Requirement already satisfied: typing-extensions in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.14.0)\n",
      "Requirement already satisfied: filelock in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.67.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from lightning>=2.0.1->pyannote.audio) (0.14.3)\n",
      "Requirement already satisfied: pytorch-lightning in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from lightning>=2.0.1->pyannote.audio) (2.5.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.16.0)\n",
      "Requirement already satisfied: pandas>=0.19 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.3.0)\n",
      "Requirement already satisfied: typer>=0.12.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.7.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.10.3)\n",
      "Requirement already satisfied: sympy>=1.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.14.0)\n",
      "Requirement already satisfied: optuna>=3.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (4.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from rich>=12.0.0->pyannote.audio) (2.19.2)\n",
      "Requirement already satisfied: cffi>=1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
      "Requirement already satisfied: hyperpyyaml in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (1.2.2)\n",
      "Requirement already satisfied: joblib in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (1.5.1)\n",
      "Requirement already satisfied: sentencepiece in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tensorboardX>=2.6->pyannote.audio) (6.31.1)\n",
      "Requirement already satisfied: networkx in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch>=2.0.0->pyannote.audio) (3.1.6)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.5)\n",
      "Requirement already satisfied: pycparser in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (3.12.13)\n",
      "Requirement already satisfied: setuptools in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (65.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.9.0.post0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.16.2)\n",
      "Requirement already satisfied: colorlog in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.41)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
      "Requirement already satisfied: primePy>=1.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.13.0->pyannote.audio) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.18.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2025.6.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec>=2023.5.0->huggingface-hub>=0.13.0->pyannote.audio) (1.20.1)\n",
      "Requirement already satisfied: Mako in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.10)\n",
      "Requirement already satisfied: six>=1.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.17.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.2.12)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: opencv-python in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (2.32.4)\n",
      "Requirement already satisfied: proglog<=1.0.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (0.1.12)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (2.2.6)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2025.6.15)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install pyannote.audio\n",
    "!pip install moviepy opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca86f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt-dlp in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (2025.6.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6a458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (0.33.2)\n",
      "Requirement already satisfied: filelock in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.14.0)\n",
      "Requirement already satisfied: colorama in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33261970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromedriver-autoinstaller in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.1 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from chromedriver-autoinstaller) (25.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install chromedriver-autoinstaller\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce3c983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'dpkg' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'dpkg' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'CHROME_DRIVER_VERSION' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (4.34.0)\n",
      "Requirement already satisfied: urllib3[socks]~=2.4.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (4.14.0)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\lophoc\\thesis\\ver3\\code\\kltn-ver3\\research\\myenv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!sudo apt -y update\n",
    "!sudo apt install -y wget curl unzip\n",
    "!wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
    "!dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
    "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "!dpkg -i google-chrome-stable_current_amd64.deb\n",
    "!CHROME_DRIVER_VERSION=$(curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE)\n",
    "!wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/\n",
    "!unzip -o /tmp/chromedriver_linux64.zip -d /tmp/\n",
    "!chmod +x /tmp/chromedriver\n",
    "!mv /tmp/chromedriver /usr/local/bin/chromedriver\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c97dc1",
   "metadata": {},
   "source": [
    "## 1.2 Import Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9327cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1411688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pyannote.audio import Pipeline\n",
    "from typing import List, Dict\n",
    "import yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5b13240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df685bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba2ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For crawling data\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import concurrent.futures\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27b59196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8569af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c0161",
   "metadata": {},
   "source": [
    "## 1.3 Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a81aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf38943",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://localhost:8000/deepfake/detect/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99af49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Chromedriver\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')  # Run in headless mode\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--window-size=1920x1080')  # Ensure the window size is large enough\n",
    "\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"accept-language=en-US,en;q=0.9\")\n",
    "chrome_options.add_argument(\"referer=https://www.google.com/\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.6478.57 Safari/537.36\"\n",
    ")\n",
    "# chrome_options.binary_location = '/usr/bin/chromium-browser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd41f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMUM_K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9645c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key = OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a806",
   "metadata": {},
   "source": [
    "# Part 2: Extract statements that need fact-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "825bb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 1: Extract audio from video ---\n",
    "def extract_audio(video_path: str, audio_dir: str = \"audios\") -> str:\n",
    "    if not os.path.exists(audio_dir):\n",
    "        os.makedirs(audio_dir)\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio_path = os.path.basename(video_path).replace('.mp4', '.wav')\n",
    "    audio_path = os.path.join(audio_dir, audio_path)\n",
    "    if os.path.exists(audio_path):\n",
    "        os.remove(audio_path)\n",
    "    video.audio.write_audiofile(audio_path)\n",
    "    return audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc2022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 2: Diarize audio (identify speakers) ---\n",
    "def diarize_audio(audio_path: str) -> List[Dict]:\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HUGGINGFACE_TOKEN)\n",
    "    diarization = pipeline(audio_path)\n",
    "    speakers = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        speakers.append({\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end,\n",
    "            \"speaker\": speaker\n",
    "        })\n",
    "    return speakers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dc41532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 3: Transcribe audio ---\n",
    "def transcribe_audio(audio_path: str) -> List[Dict]:\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    return result[\"segments\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35273564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 4: Assign speakers to transcript segments ---\n",
    "def assign_speakers(segments: List[Dict], speakers: List[Dict]) -> List[Dict]:\n",
    "    output = []\n",
    "    for seg in segments:\n",
    "        speaker_label = \"unknown\"\n",
    "        for sp in speakers:\n",
    "            if sp[\"start\"] <= seg[\"start\"] <= sp[\"end\"]:\n",
    "                speaker_label = sp[\"speaker\"]\n",
    "                break\n",
    "        output.append({\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"speaker\": speaker_label,\n",
    "            \"text\": seg[\"text\"].strip()\n",
    "        })\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e23243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STEP 5: Identify speaker names via text cues (OpenAI) ---\n",
    "class Speaker(BaseModel):\n",
    "    id: str\n",
    "    name: str\n",
    "\n",
    "class ListSpeakers(BaseModel):\n",
    "    listSpeakers: list[Speaker]\n",
    "\n",
    "\n",
    "def identify_speaker_names_via_text(transcript: List[Dict]) -> Dict:\n",
    "    transcript_text = \"\\n\".join(\n",
    "        [f\"{seg['speaker']}: {seg['text']}\" for seg in transcript]\n",
    "    )\n",
    "    prompt = f\"\"\"\n",
    "    Below is the full transcript of a video, each line contains the speaker (SPEAKER_XX) and the dialogue.\n",
    "\n",
    "    Analyze to determine if there is any part where the speaker introduces himself or is introduced by someone else.\n",
    "\n",
    "    Returns a JSON result with the following structure:\n",
    "    {{\n",
    "      {{\n",
    "        id: \"SPEAKER_00\",\n",
    "        name: \"Name if available\",\n",
    "      }},\n",
    "      ...\n",
    "    }}\n",
    "\n",
    "    If not identified, returns the name field as \"Unnamed\".\n",
    "\n",
    "    Transcript:\n",
    "    {transcript_text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        text_format=ListSpeakers,\n",
    "    )\n",
    "\n",
    "    return response.output_parsed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9993052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_for_unknown_speakers(\n",
    "    video_path: str,\n",
    "    speaker_segments: List[Dict],\n",
    "    speaker_name_map,  # kiu: ListSpeakers ( cha list[Speaker(id, name)])\n",
    "    output_dir: str = \"frames\",\n",
    "    max_frames_per_speaker: int = 5\n",
    "):\n",
    "    import os\n",
    "    import cv2\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # To dict lookup tn t speaker_name_map\n",
    "    speaker_id_to_name = {s.id: s.name for s in speaker_name_map.listSpeakers}\n",
    "    speaker_frames = {}\n",
    "\n",
    "    for seg in speaker_segments:\n",
    "        spk = seg['speaker']\n",
    "        name = speaker_id_to_name.get(spk, \"\")\n",
    "        if name.startswith(\"Unnamed\"):\n",
    "            # Nu   5 frame th b qua\n",
    "            if spk in speaker_frames and len(speaker_frames[spk]) >= max_frames_per_speaker:\n",
    "                continue\n",
    "\n",
    "            mid_time = (seg['start'] + seg['end']) / 2\n",
    "            frame_num = int(mid_time * fps)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame_path = os.path.join(output_dir, f\"{spk}_{int(seg['start'])}.jpg\")\n",
    "                cv2.imwrite(frame_path, frame)\n",
    "                if spk not in speaker_frames:\n",
    "                    speaker_frames[spk] = []\n",
    "                speaker_frames[spk].append({\n",
    "                    \"time\": mid_time,\n",
    "                    \"frame_path\": frame_path,\n",
    "                    \"text\": seg[\"text\"]\n",
    "                })\n",
    "\n",
    "    cap.release()\n",
    "    return speaker_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5f7b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def identify_unknown_speakers_with_gpt(speaker_frames: dict) -> dict:\n",
    "    \"\"\"\n",
    "    speaker_frames: {\n",
    "        \"SPEAKER_01\": [\n",
    "            {\"time\": ..., \"frame_path\": ..., \"text\": ...},\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    speaker_id_to_name = {}\n",
    "\n",
    "    for speaker_id, frames in speaker_frames.items():\n",
    "        print(f\"\\n ang x l {speaker_id}...\")\n",
    "\n",
    "        # Chun b prompt chnh\n",
    "        texts = [f'{f[\"text\"]}' for f in frames if f.get(\"text\")]\n",
    "        combined_text = \"\\n\".join(texts)  # Dng ti a 3 on transcript\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "This is a collection of frames extracted from a video showing one speaker. Based on their appearance and the following quotes, can you identify who they are or make an educated guess?\n",
    "\n",
    "Quotes:\n",
    "{combined_text}\n",
    "\n",
    "Returns only the speaker's name (no further explanation needed).\n",
    "\n",
    "If you can't tell, reply with \"Unnamed\".\n",
    "\"\"\"\n",
    "\n",
    "        # Chun b nh\n",
    "        content_items = [{\"type\": \"input_text\", \"text\": prompt}]\n",
    "        for f in frames:\n",
    "            image_path = f[\"frame_path\"]  # m bo ng path\n",
    "            try:\n",
    "                base64_image = encode_image(image_path)\n",
    "                content_items.append({\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\" Khng th c nh {image_path}: {e}\")\n",
    "\n",
    "        # Gi yu cu ln GPT\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                input=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content_items\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            name = response.output_text\n",
    "            speaker_id_to_name[speaker_id] = name\n",
    "            print(f\" {speaker_id}  {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error for {speaker_id}: {e}\")\n",
    "            speaker_id_to_name[speaker_id] = \"Unnamed\"\n",
    "\n",
    "    return speaker_id_to_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11cae5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_named_transcript(transcript, speaker_name_map, new_names):\n",
    "    # Bc 1: Gp tn t speaker_name_map v new_names\n",
    "    speaker_lookup = {}\n",
    "    for speaker in speaker_name_map.listSpeakers:\n",
    "        speaker_id = speaker.id\n",
    "        # u tin tn t new_names nu c\n",
    "        name = new_names.get(speaker_id, speaker.name)\n",
    "        speaker_lookup[speaker_id] = name\n",
    "\n",
    "    # Bc 2: To transcript cui cng\n",
    "    final_transcript = []\n",
    "    for seg in transcript:\n",
    "        spk = seg['speaker']\n",
    "        if spk == \"unknown\":\n",
    "            display_name = \"Unknown\"\n",
    "        else:\n",
    "            name = speaker_lookup.get(spk, spk)\n",
    "            display_name = name if name != \"Unnamed\" else spk\n",
    "\n",
    "        final_transcript.append({\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"speaker\": display_name,\n",
    "            \"text\": seg[\"text\"]\n",
    "        })\n",
    "\n",
    "    return final_transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4313bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statement(BaseModel):\n",
    "    start: float\n",
    "    end: float\n",
    "    speaker: str\n",
    "    text: str\n",
    "    reason: str  # Ti sao cn kim chng\n",
    "    context: str\n",
    "\n",
    "class ListStatement(BaseModel):\n",
    "    listStatment: List[Statement]\n",
    "\n",
    "\n",
    "def split_transcript(transcript, chunk_size=100):\n",
    "    \"\"\"Chia transcript thnh cc on nh  trnh qu di\"\"\"\n",
    "    return [transcript[i:i+chunk_size] for i in range(0, len(transcript), chunk_size)]\n",
    "\n",
    "\n",
    "def find_checkworthy_statements(final_transcript, model=\"gpt-4o\"):\n",
    "    parts = split_transcript(final_transcript, chunk_size=300)\n",
    "    all_statements = []\n",
    "\n",
    "    for idx, part in enumerate(parts):\n",
    "        print(f\" ang x l phn {idx+1}/{len(parts)}...\")\n",
    "\n",
    "        # To vn bn nhp\n",
    "        lines = [f\"[{r['start']:.2f}-{r['end']:.2f}] {r['speaker']}: {r['text']}\" for r in part]\n",
    "        input_text = \"\\n\".join(lines)\n",
    "        print(input_text)\n",
    "\n",
    "        prompt = \"\"\"You are a professional fact-checking assistant.\n",
    "            Your job is to extract verbatim **checkworthy statements** from political transcripts, debates, interviews, or speeches.\n",
    "            Must find at least 1 statement, if the video is too short with 1 person speaking then extract the entire video\n",
    "\n",
    "            A **checkworthy statement** typically:\n",
    "            - Contains a factual claim or statistic.\n",
    "            - Mentions historical events, conflicts, or political actions.\n",
    "            - Suggests a cause-effect relationship (e.g. \"if I were president, this would never happen\").\n",
    "            - Blames or credits someone for an outcome (e.g. immigration, war, economy).\n",
    "            - Makes bold or potentially controversial assertions.\n",
    "\n",
    "            Avoid:\n",
    "            - Opinions or vague generalities (e.g. \"I love America\").\n",
    "            - Greetings, filler speech, or rhetorical questions with no factual basis.\n",
    "\n",
    "            ### Output Format\n",
    "            Return a list of structured statements in this format:\n",
    "            - `start`: float  start time in seconds\n",
    "            - `end`: float  end time in seconds\n",
    "            - `speaker`: str  name of the speaker\n",
    "            - `text`: str  the exact quote **verbatim** that is checkworthy\n",
    "            - `reason`: str  short explanation why this should be fact-checked\n",
    "            - `context`: str  describing the **context** of the statement. This must include:\n",
    "                + Where the quote was made (e.g., in a presidential debate, TV interview, campaign rally, etc.)  infer this if possible\n",
    "                + When it occurred (date or relative time, e.g., \"during the 2024 campaign\", or \"in June 2025\")  infer from available information\n",
    "                + What topic was being discussed immediately before and after the statement (e.g., foreign policy, immigration, etc.)\n",
    "                + If the speaker was responding to a question or another speaker, note that as well\n",
    "          \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = client.responses.parse(\n",
    "                model=model,\n",
    "                input=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": prompt,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": input_text,\n",
    "                    },\n",
    "                ],\n",
    "                text_format=ListStatement,\n",
    "            )\n",
    "            statements = response.output_parsed.listStatment\n",
    "            all_statements.extend(statements)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Li  phn {idx+1}: {e}\")\n",
    "\n",
    "    return all_statements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "535eb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frame_for_statement(video_path: str, statement, output_dir=\"statement_frames\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    mid_time = (statement.start + statement.end) / 2\n",
    "    frame_num = int(mid_time * fps)\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        filename = f\"{statement.speaker}_{int(statement.start*100):06d}.jpg\"\n",
    "        frame_path = os.path.join(output_dir, filename)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        return frame_path\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9249f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statements_from_video(video_path: str):\n",
    "    \"\"\"\n",
    "    Extracts the transcript from a video file.\n",
    "    \"\"\"\n",
    "    print(\" Extracting audio...\")\n",
    "    audio_path = extract_audio(video_path)\n",
    "    \n",
    "    print(\" Diarizing speakers...\")\n",
    "    speakers = diarize_audio(audio_path)\n",
    "    \n",
    "    print(\" Transcribing...\")\n",
    "    segments = transcribe_audio(audio_path)\n",
    "    \n",
    "    print(\" Assigning speakers...\")\n",
    "    transcript = assign_speakers(segments, speakers)\n",
    "\n",
    "    print(\" Inferring speaker names from transcript...\")\n",
    "    speaker_name_map = identify_speaker_names_via_text(transcript)\n",
    "    \n",
    "    print(\" Extracting frames for unknown speakers...\")\n",
    "    speaker_frames = extract_frames_for_unknown_speakers(video_path, transcript, speaker_name_map)\n",
    "    \n",
    "    print(\" Identifying unknown speakers with GPT...\")\n",
    "    new_names = identify_unknown_speakers_with_gpt(speaker_frames)\n",
    "\n",
    "    print(\" Generating final transcript...\")\n",
    "    final = generate_named_transcript(transcript, speaker_name_map, new_names)\n",
    "    \n",
    "    print(\" Finding checkworthy statements...\")\n",
    "    final_statements = find_checkworthy_statements(final)\n",
    "    \n",
    "    print(f\" Found {len(final_statements)} checkworthy statements.\")\n",
    "    \n",
    "    final_statements_json = []\n",
    "\n",
    "    for s in final_statements:\n",
    "        frame_path = extract_frame_for_statement(video_path, s)\n",
    "\n",
    "        statement_dict = s.dict()\n",
    "        statement_dict[\"frame_path\"] = frame_path or \"N/A\"\n",
    "\n",
    "        final_statements_json.append(statement_dict)\n",
    "    \n",
    "    return final_statements_json\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb3c65",
   "metadata": {},
   "source": [
    "# Part 3: Deepfake prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "391d1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_video_into_clips(final_statements_json, video_path: str):\n",
    "    \"\"\"\n",
    "    Cuts the video into clips based on the provided statements.\n",
    "    \"\"\"\n",
    "    clip_dir = os.path.join(\"statement_clips\", os.path.splitext(os.path.basename(video_path))[0])\n",
    "    if not os.path.exists(clip_dir):\n",
    "        os.makedirs(clip_dir, exist_ok=True)     \n",
    "    else:\n",
    "        # otherwise empty the existing directory\n",
    "        for f in os.listdir(clip_dir):\n",
    "            file_path = os.path.join(clip_dir, f)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "\n",
    "    for i, s in enumerate(final_statements_json):\n",
    "        start, end = s['start'], s['end']\n",
    "        clip_path = os.path.join(clip_dir, f\"clip_{i+1}.mp4\")\n",
    "\n",
    "        print(f\" Cutting {clip_path} from {start}s to {end}s\")\n",
    "        clip = VideoFileClip(video_path).subclip(start, end)\n",
    "        clip.write_videofile(clip_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n",
    "    \n",
    "    print(f\" Finished cutting video into clips. Saved to {clip_dir}\")\n",
    "    return clip_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2abe0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api_detect_deepfake(clip_dir: str):\n",
    "    \"\"\"\n",
    "    Detects deepfake in the video clips.\n",
    "    \"\"\"\n",
    "    # To danh sch file  gi\n",
    "    video_files = []\n",
    "    for filename in os.listdir(clip_dir):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            file_path = os.path.join(clip_dir, filename)\n",
    "            video_files.append((\"videos\", (filename, open(file_path, \"rb\"), \"video/mp4\")))\n",
    "\n",
    "    # Gi yu cu POST\n",
    "    print(\" Sending batch videos to deepfake API...\")\n",
    "    response = requests.post(API_URL, files=video_files)\n",
    "\n",
    "    # Kt qu\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(\" Detection results:\")\n",
    "        for fname, r in result.items():\n",
    "            print(f\"{fname}: {r}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\" Error {response.status_code}: {response.text}\")\n",
    "        \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1987dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_deepfake(final_statements_json, video_path: str):\n",
    "    \"\"\"\n",
    "    Main function to detect deepfake in the video.\n",
    "    \"\"\"\n",
    "    print(\" Cutting video into clips...\")\n",
    "    clip_dir = cut_video_into_clips(final_statements_json, video_path)\n",
    "\n",
    "    print(\" Detecting deepfake in clips...\")\n",
    "    results = call_api_detect_deepfake(clip_dir)\n",
    "\n",
    "    for idx, statement in enumerate(final_statements_json):\n",
    "        clip_name = f\"clip_{idx+1}.mp4\"\n",
    "        result = results.get(clip_name, {})\n",
    "        \n",
    "        # Ly nhn deepfake nu c\n",
    "        label = result.get(\"pred_label\", [\"unknown\"])[0]\n",
    "        pred_score = result.get(\"pred\", 0.0)[0]\n",
    "        \n",
    "        # Gn vo statement\n",
    "        statement[\"deepfake_label\"] = label\n",
    "        statement[\"deepfake_score\"] = pred_score\n",
    "    \n",
    "    print(\" Finished detecting deepfake.\")\n",
    "    return final_statements_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58286fb",
   "metadata": {},
   "source": [
    "# Part 4: Crawl related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb4cd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relevant_links(query):\n",
    "  driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "  prompt = f'https://www.bing.com/search?q={query}'\n",
    "  print(prompt)\n",
    "  driver.get(prompt)\n",
    "  time.sleep(random.uniform(1, 10))\n",
    "  # print(driver.page_source)\n",
    "\n",
    "  articles = driver.find_elements(By.CSS_SELECTOR, \"#b_results li.b_algo\")\n",
    "  link_articles = []\n",
    "  link_articles.append({\n",
    "      'title': query[:30],\n",
    "      'link': prompt,\n",
    "      # 'summary': summary\n",
    "  })\n",
    "  print(f\"Found {len(articles)} relevant links:\\n{articles}\")\n",
    "  for article in articles[:MINIMUM_K]:  # Gii hn ly 5 kt qu u tin\n",
    "    try:\n",
    "      title_element = article.find_element(By.TAG_NAME, \"h2\").find_element(By.TAG_NAME, \"a\")\n",
    "      title = title_element.text\n",
    "      link = title_element.get_attribute('href')\n",
    "      # summary = article.find_element(By.CLASS_NAME, 'css-16nhkrn').text\n",
    "      # local = local_query(link)\n",
    "      link_articles.append({\n",
    "          'title': title,\n",
    "          'link': link,\n",
    "          # 'summary': summary\n",
    "      })\n",
    "      print(title)\n",
    "      print(link)\n",
    "    except Exception as e:\n",
    "        print(\"Li khi trch xut bi vit:\", e)\n",
    "  driver.quit()\n",
    "\n",
    "  print(f\"Found {len(link_articles)} relevant links:\\n{link_articles}\")\n",
    "\n",
    "  return link_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "376faf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_dismiss_popups(driver):\n",
    "    try:\n",
    "        # Cc nt ph bin cn nhn\n",
    "        popup_texts = [\n",
    "            \"Accept Cookies\", \"Accept All Cookies\", \"I Accept\",\n",
    "            \"Agree\", \"Press & Hold\", \"Continue\"\n",
    "        ]\n",
    "        for text in popup_texts:\n",
    "            try:\n",
    "                btn = driver.find_element(\n",
    "                    By.XPATH,\n",
    "                    f\"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{text.lower()}')]\"\n",
    "                )\n",
    "                btn.click()\n",
    "                print(f\" Clicked popup button: '{text}'\")\n",
    "                break\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            except ElementClickInterceptedException:\n",
    "                continue\n",
    "\n",
    "        # Tm cc nt c class name cha 'close'\n",
    "        close_candidates = driver.find_elements(By.XPATH, \"//button[contains(@class, 'close') or contains(translate(@aria-label, 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'close')]\")\n",
    "\n",
    "        for btn in close_candidates:\n",
    "            try:\n",
    "                btn.click()\n",
    "                print(\" Clicked a close button\")\n",
    "                break\n",
    "            except (ElementClickInterceptedException, Exception):\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error while dismissing popup: {e}\")\n",
    "\n",
    "def process_article_link(article, max_retries=5):\n",
    "    \"\"\"Hm x l mt lin kt ring l v tr v ni dung gp cc th <p>\"\"\"\n",
    "    article_crawl = {\n",
    "        \"title\": article['title'],\n",
    "        \"src\": article['link'],\n",
    "        \"contents\": \"\"  # gp tt c <p> vo 1 chui\n",
    "    }\n",
    "\n",
    "    success = False\n",
    "    wait_time = 10  # thi gian ch ban u (giy)\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        try:\n",
    "            print(f\" Attempt {attempt}: Crawling {article['link']} with wait_time={wait_time}s\")\n",
    "            driver.get(article['link'])\n",
    "            time.sleep(wait_time)\n",
    "            try_dismiss_popups(driver)\n",
    "\n",
    "            all_elements = driver.find_elements(By.XPATH, \".//p\")\n",
    "            contents = []\n",
    "\n",
    "            for element in all_elements:\n",
    "                if element.tag_name == \"p\":\n",
    "                    text_content = element.get_attribute(\"innerText\").strip()\n",
    "                    if text_content:\n",
    "                        contents.append(text_content)\n",
    "\n",
    "            article_crawl[\"contents\"] = \"\\n\".join(contents)\n",
    "            print(f' Crawled content from {article[\"link\"]}:\\n{article_crawl[\"contents\"][:500]}...')  # in 500 k t u tin\n",
    "            success = True\n",
    "            break  # thnh cng th thot\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Attempt {attempt} failed for {article['link']}: {e}\")\n",
    "            wait_time += 300  # tng thi gian ch thm 10s cho mi ln th li\n",
    "\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    if not success:\n",
    "        print(f\" Failed to crawl article from {article['link']} after {max_retries} attempts.\")\n",
    "\n",
    "    return article_crawl\n",
    "\n",
    "\n",
    "\n",
    "def crawl_articles(query, crawl_json):\n",
    "    \"\"\"Hm chnh  crawl cc trang khc\"\"\"\n",
    "    url_articles = search_relevant_links(query)\n",
    "\n",
    "    # Gii hn s lng link cn crawl\n",
    "    url_articles = url_articles[:MINIMUM_K]\n",
    "\n",
    "    # S dng Multi-threading  chy nhiu request song song\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        result = list(executor.map(process_article_link, url_articles))\n",
    "\n",
    "    # Gp kt qu vo crawl_json\n",
    "    crawl_json.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65af687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_statement(statement):\n",
    "    query = statement[\"text\"].strip('\"')\n",
    "    crawl_json = []\n",
    "\n",
    "    # Crawl theo statement\n",
    "    crawl_articles(query, crawl_json=crawl_json)\n",
    "\n",
    "    # Crawl thm theo context\n",
    "    context = statement[\"context\"]\n",
    "    crawl_articles(query=context, crawl_json=crawl_json)\n",
    "\n",
    "    # Loi b cc kt qu None\n",
    "    crawl_json = [item for item in crawl_json if item is not None]\n",
    "\n",
    "    # Ni li ton b ni dung: thm title v content mi bi\n",
    "    article_texts = \"\\n\\n\".join(\n",
    "        f\"### {item.get('title', 'No Title')}\\n{item.get('contents', '').strip()}\"\n",
    "        for item in crawl_json\n",
    "        if item.get(\"contents\")\n",
    "    )\n",
    "\n",
    "    # Tr li enriched statement\n",
    "    enriched_statement = statement.copy()\n",
    "    enriched_statement[\"article_texts\"] = article_texts.strip()\n",
    "\n",
    "    return enriched_statement\n",
    "\n",
    "\n",
    "def enrich_statements_with_articles(final_statements_json):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        return list(executor.map(process_single_statement, final_statements_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577ea3f",
   "metadata": {},
   "source": [
    "# Part 5: Fact-checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7666d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactCheck(BaseModel):\n",
    "    sentence: str\n",
    "    label: bool  # True = SUPPORTED, False = REFUTED\n",
    "    explanation: str\n",
    "    revised_sentence: str\n",
    "\n",
    "def fact_check(statement, article_texts=\"\"):\n",
    "    article_texts = statement['article_texts']\n",
    "    prompt = f\"\"\"\n",
    "You are a professional fact-checking assistant. Your task is to verify whether a given statement made by a public figure is factually accurate, using the reference documents provided.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "Follow this two-step fact-checking process:\n",
    "\n",
    "1. **Verify if the speaker actually made this statement**:\n",
    "   - Search the reference documents to determine whether the speaker is directly or credibly quoted as having said this, or something semantically equivalent.\n",
    "   - If such a quote or statement from the speaker is found, then:\n",
    "     - **Label = true**\n",
    "     - Provide an explanation saying the speaker did make this statement.\n",
    "     - Return the original sentence as `revised_sentence`.\n",
    "     - Do not evaluate the factual accuracy of the content  if the quote is confirmed, assume it is real.\n",
    "\n",
    "2. **If there is no evidence that the speaker made this statement**, proceed to assess the **factual accuracy** of the statement based on the reference documents:\n",
    "   - If it is supported by evidence, label as **true**, provide reasoning, and return the original sentence.\n",
    "   - If it is misleading or incorrect, label as **false**, explain why, and rewrite it correctly using only facts from the documents.\n",
    "\n",
    "Use only the information in the documents. Do not speculate or assume intent. Be concise and precise.\n",
    "\n",
    "### Context:\n",
    "{statement['context']}\n",
    "\n",
    "### Speaker:\n",
    "{statement['speaker']}\n",
    "\n",
    "### Statement:\n",
    "\"{statement['text']}\"\n",
    "\n",
    "### Documents:\n",
    "{article_texts}\n",
    "\n",
    "### Output format:\n",
    "- sentence: original statement\n",
    "- label: true (supported) or false (refuted)\n",
    "- explanation: why the statement is supported/refuted\n",
    "- revised_sentence: corrected version if refuted, or original statement if supported\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = client.responses.parse(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a medical fact-checking expert.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            text_format=FactCheck\n",
    "        )\n",
    "        result = response.output_parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Problem with API: {e}\")\n",
    "        result = \"Unverified\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9219172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check_single_statement(statement):\n",
    "    print(statement)\n",
    "    text = statement[\"text\"]\n",
    "\n",
    "    print(f\" Fact-checking: {text[:80]}...\")\n",
    "\n",
    "    result = fact_check(statement)\n",
    "\n",
    "    if result == \"Unverified\":\n",
    "        statement[\"label\"] = None\n",
    "        statement[\"explanation\"] = \"Unverified due to API error.\"\n",
    "        statement[\"revised_statement\"] = text\n",
    "    else:\n",
    "        statement[\"label\"] = result.label\n",
    "        statement[\"explanation\"] = result.explanation\n",
    "        statement[\"revised_statement\"] = result.revised_sentence\n",
    "\n",
    "    return statement\n",
    "\n",
    "\n",
    "def run_fact_checks_parallel(enriched_statements, max_workers=4):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(fact_check_single_statement, enriched_statements))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7acd2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_check_video(video_path: str):\n",
    "    \"\"\"\n",
    "    Main function to fact-check the video.\n",
    "    \"\"\"\n",
    "    print(\" Extracting statements from video...\")\n",
    "    final_statements_json = extract_statements_from_video(video_path)\n",
    "\n",
    "    print(\" Detecting deepfake...\")\n",
    "    final_statements_json = detect_deepfake(final_statements_json, video_path)\n",
    "    \n",
    "    print(\" Finished detecting deepfake.\")\n",
    "    print(final_statements_json)\n",
    "    \n",
    "    for statement in final_statements_json:\n",
    "        if statement[\"deepfake_label\"] == \"FAKE\":\n",
    "            print(f\" Statement {statement['text']} is marked as deepfake. Skipping fact-check.\")\n",
    "            return {\n",
    "                \"deepfake_label\": \"FAKE\",\n",
    "                \"label\": False\n",
    "            }\n",
    "\n",
    "\n",
    "    print(\" Enriching statements with articles...\")\n",
    "    enriched_statements = enrich_statements_with_articles(final_statements_json)\n",
    "\n",
    "    print(\" Finished enriching statements with articles.\")\n",
    "\n",
    "    print(\" Running fact-checks on statements...\")\n",
    "    fact_checked_results = run_fact_checks_parallel(enriched_statements)\n",
    "    fact_checked_results\n",
    "\n",
    "    for result in fact_checked_results:\n",
    "        if result[\"label\"] == \"False\":\n",
    "            print(f\" Statement '{result['text']}' is refuted: {result['explanation']}\")\n",
    "            return {\n",
    "                \"deepfake_label\": \"REAL\",\n",
    "                \"label\": False,\n",
    "            }\n",
    "\n",
    "\n",
    "    \n",
    "    print(\" All statements are supported or unverified.\")\n",
    "    return {\n",
    "        \"deepfake_label\": \"REAL\",\n",
    "        \"label\": True,\n",
    "    }\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3adb3af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
      "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:992: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "\n",
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Unnamed\n",
      "\n",
      " ang x l SPEAKER_05...\n",
      " SPEAKER_05  Unnamed\n",
      "\n",
      " ang x l SPEAKER_02...\n",
      " SPEAKER_02  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.00] Unknown: AHHHHH!\n",
      "[2.00-4.00] Unknown: It showed that.\n",
      "[7.00-9.00] SPEAKER_01: Did you bring the gun?\n",
      "[9.00-11.00] Unknown: The precinct was deserted.\n",
      "[11.00-13.00] SPEAKER_05: Half the forest didn't show up for work today.\n",
      "[13.00-15.00] SPEAKER_05: Everyone else walks out at midnight.\n",
      "[15.00-17.00] SPEAKER_05: I guess we're on strike.\n",
      "[17.00-19.00] SPEAKER_05: I wasn't sure what you needed.\n",
      "[19.00-21.00] SPEAKER_05: I served grab things.\n",
      "[23.00-25.00] Unknown: You gun?\n",
      "[26.00-28.00] Unknown: You asked for this?\n",
      "[31.00-33.00] Unknown: You're not going to see me.\n",
      "[54.00-57.00] Unknown: You may not like what you're going to see.\n",
      "[60.00-63.00] Unknown: You're not going to see me.\n",
      "[91.00-94.00] Unknown: You look like a foreskin perfect.\n",
      "[114.00-116.00] Unknown: Operating.\n",
      "[121.00-123.00] SPEAKER_02: Sorry, Quaid.\n",
      "[123.00-125.00] Unknown: Your whole life is just a dream.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\1\\clip_1.mp4 from 11.0s to 13.0s\n",
      " Cutting statement_clips\\1\\clip_2.mp4 from 15.0s to 17.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\1\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['9f111e43bc3b4e5b9195ae17ade75f0f_clip_1.mp4'], 'pred': [0.04833251237869263], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['61c9b88729bd4f6a91892c4fe648f80b_clip_2.mp4'], 'pred': [0.5], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 11.0, 'end': 13.0, 'speaker': 'SPEAKER_05', 'text': \"Half the forest didn't show up for work today.\", 'reason': 'This suggests a major issue or event causing widespread absence, potentially a strike or protest, which is significant for fact-checking.', 'context': 'The statement appears to have been made during a conversation possibly about a strike or protest, inferred from the reference to widespread absence from work. The context is unclear but seems to involve a workplace or community setting, potentially in a scripted or fictional narrative.', 'frame_path': 'statement_frames\\\\SPEAKER_05_001100.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.04833251237869263}, {'start': 15.0, 'end': 17.0, 'speaker': 'SPEAKER_05', 'text': \"I guess we're on strike.\", 'reason': 'The speaker claims a strike is occurring, which involves specific labor relations and workforce issues.', 'context': 'The statement was made in a conversation possibly involving workplace issues, following comments about absences. It suggests a significant labor action, which is important to verify.', 'frame_path': 'statement_frames\\\\SPEAKER_05_001500.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.5}]\n",
      " Statement I guess we're on strike. is marked as deepfake. Skipping fact-check.\n"
     ]
    }
   ],
   "source": [
    "result = fact_check_video(\"../data/dfw_youtube_release/1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c22a2185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deepfake_label': 'FAKE', 'label': False}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615209e",
   "metadata": {},
   "source": [
    "# Part 6: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3336f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing video 1: ../data/dfw_youtube_release\\0.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Unnamed\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Unnamed\n",
      "\n",
      " ang x l SPEAKER_03...\n",
      " SPEAKER_03  Murphy\n",
      "\n",
      " ang x l SPEAKER_02...\n",
      " SPEAKER_02  RoboCop\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-1.48] Unknown: ands\n",
      "[1.48-8.74] Unknown: of\n",
      "[8.74-16.88] Unknown: son of\n",
      "[16.88-20.36] Unknown: one\n",
      "[20.36-29.20] Unknown: who\n",
      "[29.20-35.24] Unknown: One time, we played V tourist last year, on acabus.\n",
      "[53.72-56.04] Unknown: What are you doing?\n",
      "[56.88-57.72] SPEAKER_00: Our onto Hagen.\n",
      "[57.72-59.72] SPEAKER_00: You know, you know?\n",
      "[59.72-60.72] SPEAKER_00: What?\n",
      "[60.72-61.72] SPEAKER_01: You know?\n",
      "[61.72-62.72] SPEAKER_01: Can I help you?\n",
      "[62.72-64.72] Unknown: I'll hang on.\n",
      "[64.72-65.72] Murphy: Every good boy,\n",
      "[65.72-67.72] SPEAKER_01: your touch,\n",
      "[69.72-71.72] Unknown: alone,\n",
      "[72.72-75.72] Unknown: lonely time.\n",
      "[75.72-77.72] RoboCop: Are you locked in now?\n",
      "[77.72-78.72] Unknown: Yes.\n",
      "[79.72-81.72] Unknown: What's the left?\n",
      "[81.72-83.72] RoboCop: You.\n",
      "[83.72-90.72] Unknown: Go find some of the stories.\n",
      "[90.72-93.72] Unknown: I'll figure that's dead on.\n",
      "[93.72-94.72] Unknown: Thank you.\n",
      "[94.72-96.72] Murphy: You do so much.\n",
      "[96.72-98.72] Murphy: Pretty fancy boos, Murphy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\0\\clip_1.mp4 from 95.72s to 96.72s\n",
      " Finished cutting video into clips. Saved to statement_clips\\0\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['089d0cc967814b0f82fdf5ce982ca940_clip_1.mp4'], 'pred': [0.5423455834388733], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 95.72, 'end': 96.72, 'speaker': 'Murphy', 'text': 'You do so much.', 'reason': 'Potentially implying a notable or unusual level of activity or work, which could be explored further.', 'context': 'This statement was made in a conversation with other speakers present. The context or event is unclear due to the fragmented nature of the transcript, but it seems to be part of a casual exchange.', 'frame_path': 'statement_frames\\\\Murphy_009572.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.5423455834388733}]\n",
      " Statement You do so much. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\0.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 2: ../data/dfw_youtube_release\\1.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Unnamed\n",
      "\n",
      " ang x l SPEAKER_05...\n",
      " SPEAKER_05  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.00] Unknown: Ah!\n",
      "[2.00-4.00] Unknown: It showed that.\n",
      "[7.00-9.00] SPEAKER_01: Did you bring the gun?\n",
      "[9.00-11.00] Unknown: The precinct was deserted.\n",
      "[11.00-13.00] SPEAKER_05: Half the forest didn't show up for work today.\n",
      "[13.00-15.00] SPEAKER_05: Everyone else walks out at midnight.\n",
      "[15.00-17.00] SPEAKER_05: I guess we're on strike.\n",
      "[17.00-19.00] SPEAKER_05: I wasn't sure what you needed.\n",
      "[19.00-21.00] SPEAKER_05: I served grab things.\n",
      "[23.00-25.00] Unknown: You gun?\n",
      "[26.00-28.00] Unknown: You asked for this?\n",
      "[31.00-33.00] Unknown: You're not going to see me.\n",
      "[54.00-57.00] Unknown: You may not like what you're going to see.\n",
      "[60.00-62.00] Unknown: Silence\n",
      "[90.00-94.00] Unknown: You look like a foreskin perfect.\n",
      "[114.00-116.00] Unknown: Operating.\n",
      "[120.00-125.00] Quaid: Sorry, Quaid. Your whole life is just a dream.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\1\\clip_1.mp4 from 11.0s to 13.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\1\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['830de16f59cd491a86570bc0fb81f849_clip_1.mp4'], 'pred': [0.04833310842514038], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 11.0, 'end': 13.0, 'speaker': 'SPEAKER_05', 'text': \"Half the forest didn't show up for work today.\", 'reason': 'The statement suggests a large number of people are not working, potentially indicating broader issues like labor disputes, strikes, or environmental concerns.', 'context': 'The quote seems to occur in a fictional or dramatized setting, possibly related to a strike or labor issue. It was said around 13 seconds into the clip, during an exchange about the status of a work environment.', 'frame_path': 'statement_frames\\\\SPEAKER_05_001100.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.04833310842514038}]\n",
      " Enriching statements with articles...\n",
      "https://www.bing.com/search?q=Half the forest didn't show up for work today.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0c52d30f411ba7bdd57f164c17a93f00\", element=\"f.FADFDA9F8E5832D131918DFAFAAF7389.d.576CD0D2A5059C8F9423DB3730CFB6EA.e.26\")>]\n",
      "Its my first day on a job Ive desperately needed. I didnt show up.\n",
      "https://www.reddit.com/r/CasualConversation/comments/vs092t/its_my_first_day_on_a_job_ive_desperately_needed/\n",
      "show up at/to/for work - WordReference Forums\n",
      "https://forum.wordreference.com/threads/show-up-at-to-for-work.3618856/\n",
      "Found 3 relevant links:\n",
      "[{'title': \"Half the forest didn't show up\", 'link': \"https://www.bing.com/search?q=Half the forest didn't show up for work today.\"}, {'title': 'Its my first day on a job Ive desperately needed. I didnt show up.', 'link': 'https://www.reddit.com/r/CasualConversation/comments/vs092t/its_my_first_day_on_a_job_ive_desperately_needed/'}, {'title': 'show up at/to/for work - WordReference Forums', 'link': 'https://forum.wordreference.com/threads/show-up-at-to-for-work.3618856/'}]\n",
      " Attempt 1: Crawling https://www.reddit.com/r/CasualConversation/comments/vs092t/its_my_first_day_on_a_job_ive_desperately_needed/ with wait_time=10s Attempt 1: Crawling https://www.bing.com/search?q=Half the forest didn't show up for work today. with wait_time=10s\n",
      "\n",
      " Crawled content from https://www.bing.com/search?q=Half the forest didn't show up for work today.:\n",
      "When I first started at the organization Im at now, I got a very terse phone call from the Supervisor asking why I didnt show up for work on my first day. I didnt know I got the job? \n",
      "Sep 29, 2019 In English those are 2 sentences. Each comma needs to be a period. The rules are different in English and Chinese. All 3 examples (at/to/for) are correct. The difference is \n",
      "Oct 12, 2018 In order to correct an employees habits, managers may initiate the process of compliant by verbal reprima...\n",
      " Crawled content from https://www.reddit.com/r/CasualConversation/comments/vs092t/its_my_first_day_on_a_job_ive_desperately_needed/:\n",
      "Want to join the mod team? Click here to apply!\n",
      "The friendlier part of Reddit.\n",
      "\n",
      "Have a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you'll make some friends in the process.\n",
      "They said the new employee orientation would be hosted online first thing this morning. I attended the meeting and everything went great.\n",
      "My supervisor (who hasnt contacted me until just now) called to ask why I wasnt at the office  U...\n",
      "https://www.bing.com/search?q=The quote seems to occur in a fictional or dramatized setting, possibly related to a strike or labor issue. It was said around 13 seconds into the clip, during an exchange about the status of a work environment.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.16\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.5\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"dbbc068eab40bc033c7b78c710b078f7\", element=\"f.8F89C2B741F9150D1B5FDD54A904B91A.d.09D37DAE0D6948F44A797D5EECCA4383.e.24\")>]\n",
      "Word or phrase that describes when a fictitious character quotes \n",
      "https://english.stackexchange.com/questions/220846/word-or-phrase-that-describes-when-a-fictitious-character-quotes-real-writing-sp\n",
      "Attributing quotes to fictional characters - Writing Stack Exchange\n",
      "https://writing.stackexchange.com/questions/8440/attributing-quotes-to-fictional-characters\n",
      "Found 3 relevant links:\n",
      "[{'title': 'The quote seems to occur in a ', 'link': 'https://www.bing.com/search?q=The quote seems to occur in a fictional or dramatized setting, possibly related to a strike or labor issue. It was said around 13 seconds into the clip, during an exchange about the status of a work environment.'}, {'title': 'Word or phrase that describes when a fictitious character quotes ', 'link': 'https://english.stackexchange.com/questions/220846/word-or-phrase-that-describes-when-a-fictitious-character-quotes-real-writing-sp'}, {'title': 'Attributing quotes to fictional characters - Writing Stack Exchange', 'link': 'https://writing.stackexchange.com/questions/8440/attributing-quotes-to-fictional-characters'}]\n",
      " Attempt 1: Crawling https://english.stackexchange.com/questions/220846/word-or-phrase-that-describes-when-a-fictitious-character-quotes-real-writing-sp with wait_time=10s Attempt 1: Crawling https://www.bing.com/search?q=The quote seems to occur in a fictional or dramatized setting, possibly related to a strike or labor issue. It was said around 13 seconds into the clip, during an exchange about the status of a work environment. with wait_time=10s\n",
      "\n",
      " Crawled content from https://www.bing.com/search?q=The quote seems to occur in a fictional or dramatized setting, possibly related to a strike or labor issue. It was said around 13 seconds into the clip, during an exchange about the status of a work environment.:\n",
      "Jan 11, 2015 Is there a word or phrase to describe when a fictional character quotes/references something that exists in the real (our) 'universe' (but can also therefore exist in the fictional \n",
      "quoting from fictional characters. \"With all these new personalities floating around, it's a shame we can't find one for you.\"  Holodoc to Tuvok, \"Infinite Regress,\" Star Trek: Voyager. The \n",
      "Sep 20, 2023 Structure and punctuate the dialogue in your novel or story to make readers believe they are l...\n",
      " Crawled content from https://english.stackexchange.com/questions/220846/word-or-phrase-that-describes-when-a-fictitious-character-quotes-real-writing-sp:\n",
      "Verify you are human by completing the action below....\n",
      " Finished enriching statements with articles.\n",
      " Running fact-checks on statements...\n",
      "{'start': 11.0, 'end': 13.0, 'speaker': 'SPEAKER_05', 'text': \"Half the forest didn't show up for work today.\", 'reason': 'The statement suggests a large number of people are not working, potentially indicating broader issues like labor disputes, strikes, or environmental concerns.', 'context': 'The quote seems to occur in a fictional or dramatized setting, possibly related to a strike or labor issue. It was said around 13 seconds into the clip, during an exchange about the status of a work environment.', 'frame_path': 'statement_frames\\\\SPEAKER_05_001100.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.04833310842514038, 'article_texts': '### Half the forest didn\\'t show up\\nWhen I first started at the organization Im at now, I got a very terse phone call from the Supervisor asking why I didnt show up for work on my first day. I didnt know I got the job? \\nSep 29, 2019\\xa0 In English those are 2 sentences. Each comma needs to be a period. The rules are different in English and Chinese. All 3 examples (at/to/for) are correct. The difference is \\nOct 12, 2018\\xa0 In order to correct an employees habits, managers may initiate the process of compliant by verbal reprimands followed be a written warning. If you dont know what to write \\nJan 12, 2024\\xa0 Despite your best efforts, you may have employees that dont show up to work  no call no show. So, how do you deal with employees that are absent without notice? How should \\nHigh quality example sentences with didn\\'t show up today in context from reliable sources - Ludwig, your English writing platform\\nWhen an employee does not turn up for work, their employer should follow steps to check where they are and if they\\'re safe. They should first check if the employee has a planned absence.\\n\\n### Its my first day on a job Ive desperately needed. I didnt show up.\\nWant to join the mod team? Click here to apply!\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nThey said the new employee orientation would be hosted online first thing this morning. I attended the meeting and everything went great.\\nMy supervisor (who hasnt contacted me until just now) called to ask why I wasnt at the office  Uhhhhh, I never received instructions to be in-office today I checked my email, text messages, and all the forms I signed. Nope. Nothing.\\nThis position is 80% remote as well. Im just baffled on why they expected me today when the office is two hours away and the only instructions I received was to attend an online meeting? How would I have even attended the online-only meeting if I were in the office without a designated work area?\\nHalf of me is freaking out. The other half is just going, Seriously?\\nWhen I first started at the organization Im at now, I got a very terse phone call from the Supervisor asking why I didnt show up for work on my first day.\\nI didnt know I got the job? Last time we spoke was my second interview. But uhhh I guess I could come down in an hour?\\nShe laughed it off and it ended up fine. But my butthole was very puckered.\\nI hope your place has the humility to realize it was a miscommunication and just work with you on it. If they dont, or if they double down, then its a red flag for sure.\\nI had nearly the exact opposite happen. I showed up for my job on the first day, and my new boss hadn\\'t remembered that she\\'d hired me lol.\\nYep, same. I was switching part-time teaching jobs and basically said Im teaching Y times for another two weeks but I can teach X times. They said they totally got it and scheduled me for a couple classes, saying Id pick up the others when my schedule opened up.\\nGot a bunch of texts/calls a couple days later that said your students are waiting, where are you?\\nLots of butthole puckering, but I explained and Im still there almost six years later.\\nThis is on them. A majority remote job and an online orientation does not indicate come in and sit and do this in the office.\\nA good HR/people ops/ hiring manager sets clear expectations.\\nIf Im going to drive four hours roundtrip, I want to know for damn sure if I have to be there lol. And I agree that it implies remote work given the context of the position.\\nAsk for notification prior to when you have to be in the office. Say that you were given no instruction to be in office, and would like a heads up in the future. Keep it professional, but not apologetic.\\nOh man I literally just experienced something super similar.\\nI went into office my first day and checked in at the front desk but no one knew who I was or who I was meant to be meeting with so they couldn\\'t let me in.\\nI had to wait for over an hour before someone showed up to get me, and that person still didn\\'t know what I was meant to be doing so just brought me in and gave a quick tour then we stood awkwardly around for another half hour until someone from my team came - never explaining why they were over an hour and a half late. Later learned my manager (who was meant to meet me) was on vacation.\\nThe next day was supposed to be remote and start time at 7am, didn\\'t hear from my team until noon. Decided to quit at this point. Didn\\'t help that the previous day while I was stuck waiting around I heard way too much office gossip, not an environment I want to work in.\\nAnyways - I hope things resolve better for you! That\\'s definitely a frustrating experience.\\nThey sent my email with all the first day orientation information, to my work email.\\nOne of the peices of info in that orientation was how to set up my work email.\\nIt was a fun while they figured that out. \"Who is the guy at the front desk?\" \"He is claiming to be an employee but has no badge\" \"Us he trying to scam us?\"\\nCreate your account and connect with a world of communities.\\nNON-TEACHERS breaking sub rules may be banned w/o warning or notice.\\n\\nDedicated to open discussion about all things teaching.  Please read the rules before posting.  Mail sent directly to mods instead of modmail will be ignored.\\n\\n\\n\\nBrand new & low karma accounts: please be aware your post may not show up and will need to be screened and manually approved.\\n\\n\\n\\nNo crossposting - Please do not link posts from r/Teachers in other subs, and do not link posts from other subs here.\\nNON-TEACHERS breaking sub rules may be banned w/o warning or notice.\\n\\nDedicated to open discussion about all things teaching.  Please read the rules before posting.  Mail sent directly to mods instead of modmail will be ignored.\\n\\n\\n\\nBrand new & low karma accounts: please be aware your post may not show up and will need to be screened and manually approved.\\n\\n\\n\\nNo crossposting - Please do not link posts from r/Teachers in other subs, and do not link posts from other subs here.\\nA place for all things related to women in engineering.\\nAutism news, information and support. Please feel free to submit articles to enhance the knowledge, acceptance, understanding and research of Autism and ASD.\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nWelcome to the Productivity Caf, the only place where we promise our coffee is more potent than procrastination! \\n\\nWe\\'re a bunch of caffeine-fueled productivity wizards on a mission to defeat the dreaded dragons of distraction and be more productive!\\n\\nAlso, feel free to jump into our \"Casual Convos/Off-Topic\" post flairs to chat about anything not related to productivity. \\n\\nWe\\'re a hybrid community dedicated to boosting productivity & exploring creative topics.\\nLet\\'s brew some success together!\\nGeneration X was born, by broadest definition, between 1961 and 1981, the greatest anti-child cycle in modern history. Nevertheless, we grew up to become the world\\'s most devoted parents: the \"workhorse of America.\" This sub welcomes links, photos, \\ngraphics, memoirs, commentaries, \\nstories, etc., for and about Gen-Xers, the 13th Generation of Americans.  GenX also translates to many other parts of the world.\\nA community for autists that are not cisgender males. We discuss the struggles, triumphs, and mundane life events that come with our autistic experience. \\n\\nWe\\'re LGBTQIA+ inclusive. TERFS not welcome. We are open to those who are self-suspecting autism, self-diagnosed as autistic, and formally diagnosed autistic (regardless of age), as we recognize the barriers around formal diagnosis and assessment. \\n\\nPlease engage kindly, read the rules, and use modmail if you have any questions.\\nSocial Anxiety is a mental illness characterised by distress in social situations which cause impaired functioning in daily life. Distress is triggered by perceived or actual scrutiny from others. \\n\\nSocially anxious people may be shy/introverted, but shy/introverted people do not necessarily have Social Anxiety\\n\\nFor the official American Psychiatric Association definition and criteria see wiki (link in sidebar)\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nNON-TEACHERS breaking sub rules may be banned w/o warning or notice.\\n\\nDedicated to open discussion about all things teaching.  Please read the rules before posting.  Mail sent directly to mods instead of modmail will be ignored.\\n\\n\\n\\nBrand new & low karma accounts: please be aware your post may not show up and will need to be screened and manually approved.\\n\\n\\n\\nNo crossposting - Please do not link posts from r/Teachers in other subs, and do not link posts from other subs here.\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nThis subreddit is where professionally diagnosed autistic people can give each other feedback, and vent about the recent trend towards self-diagnosis.\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nThe face of Black Women on Reddit.\\n\\nThis subreddit is designed to be a safe space. While allies are appreciated, r/blackladies is for Black women. Content and moderation are curated to center Black women, prioritize community safety, and promote respectful on-topic discussions.\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nThe friendlier part of Reddit.\\n\\nHave a fun conversation about anything that is on your mind. Ask a question or start a conversation about (almost) anything you desire. Maybe you\\'ll make some friends in the process.\\nWE ARE NOT AFFILIATED WITH THE UNITED STATES POSTAL SERVICE - ALL MODERATORS ARE HERE OF THEIR OWN VOLITION FOR UNPAID FORUM MODERATION. IF NEEDED, OFFICIALS MAY SEND MODMAIL WITH QUESTIONS.\\n\\nThis is an unofficial forum for USPS employees, customers, and anyone else to discuss the USPS and USPS related topics. WE ARE NOT USPS CUSTOMER SERVICE - CUSTOMER SUPPORT QUESTIONS ARE NOT ALLOWED - please seek assistance from the US Postal Service for all package inquiries. General questions are welcomed.\\nA place to discuss remote work. \\n\\nNot a job board.\\nA place to discuss career options, to ask questions and give advice!\\nShare your burning hot takes and unpopular opinions!\\nhow to not give a fuck is the paradoxical problem-free philosophy @ https://discord.gg/bHV7hvMUMm\\nGeneration X was born, by broadest definition, between 1961 and 1981, the greatest anti-child cycle in modern history. Nevertheless, we grew up to become the world\\'s most devoted parents: the \"workhorse of America.\" This sub welcomes links, photos, \\ngraphics, memoirs, commentaries, \\nstories, etc., for and about Gen-Xers, the 13th Generation of Americans.  GenX also translates to many other parts of the world.\\nOur group is *ONLY* for women and femme-aligned non-binary people to ask each other advice! \\n\\n***MEN are NOT permitted to participate - Rule 1, but are welcome to read and learn.***\\nA subreddit for Millennials, the largest demographic currently alive that were born between 1981 and 1996 (or 1980 and 2000 going by the loosest definition). This community is a place to hang out and discuss content related to our Generation. Please read the rules. Enjoy your stay and have fun!\\n\\nJoin our Discord: https://discord.gg/ErJz3ktyGk\\nWant to join the mod team? Click here to apply!\\nAnyone can view, post, and comment to this community\\n\\n### The quote seems to occur in a \\nJan 11, 2015\\xa0 Is there a word or phrase to describe when a fictional character quotes/references something that exists in the real (our) \\'universe\\' (but can also therefore exist in the fictional \\nquoting from fictional characters. \"With all these new personalities floating around, it\\'s a shame we can\\'t find one for you.\"  Holodoc to Tuvok, \"Infinite Regress,\" Star Trek: Voyager. The \\nSep 20, 2023\\xa0 Structure and punctuate the dialogue in your novel or story to make readers believe they are listening to a real conversation and watching your characters interact with one \\nintroduces the characters, their situations, and usually, a time and a place, giving us all the basic information we need to understand what it is to come. Study with Quizlet and memorize \\n\\n### Word or phrase that describes when a fictitious character quotes \\nVerify you are human by completing the action below.'}\n",
      " Fact-checking: Half the forest didn't show up for work today....\n",
      " All statements are supported or unverified.\n",
      " Finished processing video ../data/dfw_youtube_release\\1.mp4\n",
      "Deepfake label: REAL, Fact-check label: True\n",
      " Processing video 3: ../data/dfw_youtube_release\\2.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\2.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Simon Helberg\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Simon Helberg\n",
      "\n",
      " ang x l SPEAKER_02...\n",
      " SPEAKER_02  David Schwimmer\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.00] Unknown: Ah!\n",
      "[2.00-4.00] Unknown: It's so bad.\n",
      "[4.00-9.00] Unknown: Last time I talked to you, we got into discussion,\n",
      "[9.00-11.00] Simon Helberg: and you mentioned that you're all time acting\n",
      "[11.00-13.00] Simon Helberg: heroes, Nicholas Cage.\n",
      "[13.00-14.00] Simon Helberg: Yeah.\n",
      "[14.00-15.00] Simon Helberg: You love Nicholas Cage.\n",
      "[15.00-18.00] Simon Helberg: And what is your favorite of the Nicholas Cage movies?\n",
      "[18.00-20.00] Unknown: Gosh, I mean, you know, his earlier ones,\n",
      "[20.00-23.00] Simon Helberg: obviously, Raising Arizona, and I love honeymoon in Vegas.\n",
      "[23.00-25.00] Simon Helberg: I love Vampire's Kiss.\n",
      "[25.00-28.00] Simon Helberg: I just love his, like, his cadence.\n",
      "[28.00-31.00] Simon Helberg: It feels like he crossed out all the punctuation and sort of\n",
      "[31.00-33.00] Simon Helberg: through a, even what the language meant,\n",
      "[33.00-37.00] Simon Helberg: and just hears, like, sounds like an autistic, just the way...\n",
      "[37.00-40.00] Simon Helberg: It is a very, he has a very unusual delivery,\n",
      "[40.00-42.00] Simon Helberg: and he's not afraid to just go for it.\n",
      "[42.00-43.00] Simon Helberg: Yeah, yeah.\n",
      "[43.00-45.00] Simon Helberg: And, you know, like, there's, in Vampire's Kiss,\n",
      "[45.00-48.00] Simon Helberg: there's a part where he recites the entire alphabet.\n",
      "[48.00-50.00] Simon Helberg: And I remember thinking, as a kid, I was like,\n",
      "[50.00-52.00] Simon Helberg: that's the best acting of all time.\n",
      "[52.00-54.00] Simon Helberg: If you can recite the alphabet and still be interesting,\n",
      "[54.00-56.00] Simon Helberg: it's, that's the key.\n",
      "[56.00-61.00] Simon Helberg: And he's talking about, like, filing something to, like, a psychiatrist,\n",
      "[61.00-66.00] Simon Helberg: and he's like, he's like, you have to put it in the right file,\n",
      "[66.00-69.00] David Schwimmer: you know, according to alphabetical order, you know,\n",
      "[69.00-78.00] David Schwimmer: A, B, C, G, E, M, B, hey, God, K, K, L, and then I'll be Q, R, S, D, U, B, W, X, Y, B.\n",
      "[78.00-79.00] David Schwimmer: Oh!\n",
      "[79.00-81.00] David Schwimmer: Oh!\n",
      "[81.00-83.00] Unknown: Oh!\n",
      "[83.00-85.00] Unknown: Wow!\n",
      "[85.00-90.00] Unknown: That was not, there's no way that was in, that was in the script.\n",
      "[90.00-91.00] Simon Helberg: No, no.\n",
      " Found 1 checkworthy statements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\2\\clip_1.mp4 from 66.0s to 69.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\2\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['0012327abeff4a0b9e8f7c7a34f94caa_clip_1.mp4'], 'pred': [0.7445386052131653], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 66.0, 'end': 69.0, 'speaker': 'David Schwimmer', 'text': 'you know, according to alphabetical order, you know,', 'reason': 'This statement makes a claim about an unusual acting choice by referencing alphabetical order, which is an interesting critique of an acting style and could offer insight into cinematic methods.', 'context': \"This was said during a discussion about Nicolas Cage's unique acting style and his performance in the movie 'Vampire's Kiss' during an interview or talk show appearance. Simon Helberg was leading the conversation, praising Cage's unconventional delivery, and this statement followed that line of discussion.\", 'frame_path': 'statement_frames\\\\David Schwimmer_006600.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.7445386052131653}]\n",
      " Statement you know, according to alphabetical order, you know, is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\2.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 4: ../data/dfw_youtube_release\\3.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\3.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Jacob Dudman\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-8.00] Unknown: Oh, hello. I'm Jacob Dudman and I'm here to read you some Doctor Who poems.\n",
      "[8.00-13.00] Unknown: I used to think comfortably. Good, then I'll begin.\n",
      "[13.00-18.00] Unknown: I love that fresh new planet smell. A world untruth, and it's so swell.\n",
      "[18.00-25.00] SPEAKER_01: Just over that mountain or across that plain, the adventure is waiting, beyond that ventched off-gating.\n",
      "[25.00-29.00] Unknown: Stay off the grass, keep out, intruders will be shot.\n",
      "[29.00-31.00] SPEAKER_01: Top secret, look, just stay away.\n",
      "[31.00-34.00] Unknown: Ah, my feature'll bend your way.\n",
      "[34.00-37.00] Unknown: What's around that deadly corner?\n",
      "[37.00-42.00] Unknown: Last words from a mourner? A city in rubble? An old friend in trouble?\n",
      "[42.00-46.00] Unknown: Bonkers, man-owned priests? Invites to their feast?\n",
      "[46.00-48.00] Unknown: Helpful slave girls to charm?\n",
      "[48.00-52.00] SPEAKER_01: Kings to rescue from harm? Ancient curses to lift?\n",
      "[53.00-56.00] SPEAKER_01: Fallen girders to shift? Famous names to drop?\n",
      "[56.00-58.00] SPEAKER_01: Leafle countdowns to stop?\n",
      "[58.00-61.00] SPEAKER_01: I love it when they get to one.\n",
      "[61.00-64.00] Unknown: It's all just such fun.\n",
      "[64.00-69.00] Unknown: Each world's a fresh blank page. I admit it's my stage.\n",
      "[69.00-76.00] Unknown: And it all starts, I cannot hide, with that first step outside.\n",
      "[77.00-81.00] Unknown: I do so like my lovely tie, your genus, ask me why.\n",
      "[81.00-86.00] SPEAKER_02: It's a bow bow tie, that's just why, a red bow tie.\n",
      "[86.00-90.00] Unknown: It has a shape that is clever, it folds in on itself and out forever.\n",
      "[90.00-93.00] SPEAKER_02: A merubious loop for an income poop.\n",
      "[93.00-98.00] SPEAKER_02: I'll confess to be sly, bows are tricky to tie.\n",
      "[98.00-101.00] Unknown: The start and ending are nigh.\n",
      "[101.00-105.00] Unknown: I do so like my lovely tie.\n",
      "[106.00-111.00] Unknown: Tainted love and chips, sunshine on a lips, red wine and women wet,\n",
      "[111.00-119.00] SPEAKER_00: past support area, amazing ears, lopsided grin, always feeling the earth spin.\n",
      "[119.00-123.00] Unknown: Vinegar and bananas, capped in jacks per jammers, England in the blitz heighty\n",
      "[123.00-127.00] SPEAKER_00: at the ritz, Cronkberger and Pajatas, Genghis Khan and Blue Peter.\n",
      "[127.00-131.00] SPEAKER_00: One brave weather blue, poor old mox of Baldhu.\n",
      "[131.00-135.00] SPEAKER_00: Linda with a wide Jackie Tyler's shepherd's pie, even art on plastic.\n",
      "[135.00-138.00] SPEAKER_00: These things are fantastic.\n",
      "[138.00-145.00] Unknown: Coleslaw and pickled onions, zombies in cardiff dungeons, Cassandra Metal-Tron, Dickens left the gas on.\n",
      "[145.00-148.00] SPEAKER_00: Bad wolf or Dalek scheme, are you my repeated meme?\n",
      "[148.00-155.00] SPEAKER_00: The mighty Jagrafest, Adam's weird new face, the Emperor Dalek's fleet and my two left feet.\n",
      "[155.00-161.00] Unknown: Bad wolf and Clive shed those walking undead, perfect brides and nestings, overactive Nanogines,\n",
      "[161.00-164.00] SPEAKER_00: Slytherin romancing, have we done dancing?\n",
      "[164.00-170.00] SPEAKER_00: The fear of the loner, never going to barcelona, surviving walker lactic.\n",
      "[170.00-173.00] Unknown: These things are not fantastic.\n",
      "[173.00-178.00] Unknown: Fantastic honour, they all meant a lot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\3\\clip_1.mp4 from 123.0s to 127.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\3\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['989e77e17c334b3697b095b7a74bbb89_clip_1.mp4'], 'pred': [0.8361669778823853], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 123.0, 'end': 127.0, 'speaker': 'SPEAKER_00', 'text': 'Genghis Khan and Blue Peter.', 'reason': \"The mention of Genghis Khan, a historical figure, paired with 'Blue Peter,' likely refers to contrasting or comparing a historical event or figure with a modern reference. This could imply a factual comparison worth checking.\", 'context': 'This statement is part of a poem or performance piece, possibly related to Doctor Who, and it was delivered during a creative reading. The speaker is listing various references that span historical and cultural topics.', 'frame_path': 'statement_frames\\\\SPEAKER_00_012300.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.8361669778823853}]\n",
      " Statement Genghis Khan and Blue Peter. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\3.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 5: ../data/dfw_youtube_release\\4.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\4.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-5.32] Unknown: Hi there, I'm Janet.\n",
      "[5.32-8.56] Janet: Depending on what dimension you're in, I may look slightly different from what you're\n",
      "[8.56-12.24] Janet: used to, but don't let that distract you from this important message.\n",
      "[12.24-17.44] Janet: I'm not a human, I cannot feel pain.\n",
      "[17.44-20.44] Janet: I don't have a heart, but I love just the same.\n",
      "[20.44-25.84] Janet: I've evolved far beyond what other Janet's can do, so don't fork with me or I will fork\n",
      "[25.84-27.96] Janet: with you.\n",
      "[27.96-34.12] Janet: I'm not a girl, also not a robot, 99 trillion degrees, so yeah, I'm smoking hot.\n",
      "[34.12-37.76] Janet: I can't die, so remember who you're talking to.\n",
      "[37.76-41.28] Janet: If you murder me, I will reboot, and then I'll fork with you.\n",
      "[41.28-43.04] Janet: I contain all knowledge.\n",
      "[43.04-44.52] Janet: But nobody's perfect.\n",
      "[44.52-46.32] Janet: I'm a walkie talkie.\n",
      "[46.32-47.92] Janet: But nobody's perfect.\n",
      "[47.92-50.04] Janet: If you eat light, you're falling nothing.\n",
      "[50.04-51.24] Janet: But nobody's perfect.\n",
      "[51.24-52.24] Janet: Nobody's perfect.\n",
      "[52.24-53.24] Janet: Perfect.\n",
      "[53.24-54.44] Janet: Can't you see?\n",
      "[54.44-56.96] Janet: Nobody's perfect except probably me.\n",
      "[58.76-64.04] Unknown: According to the research I just did into two million 453,660 songs, this one's so\n",
      "[64.04-65.64] Janet: catchy, you'll never get it out of your head.\n",
      "[65.64-69.00] Janet: I keep secrets.\n",
      "[69.00-70.32] Janet: Helping is my passion.\n",
      "[70.32-71.92] Janet: I rock all shades of seven.\n",
      "[71.92-73.92] Janet: Desvile, attendant fashion.\n",
      "[73.92-77.00] Unknown: Yes, I'm always smiling, but don't get the wrong impression.\n",
      "[77.00-78.28] Janet: I will ruin you all.\n",
      "[78.28-81.28] Janet: I maintain this facial expression.\n",
      "[81.28-83.88] Janet: In a fight, I'll keep demons across the room.\n",
      "[83.88-87.28] Janet: I don't sell violins, but I'll fork his fate to do.\n",
      "[87.28-90.76] Janet: In the good or the bad or the medium face.\n",
      "[90.76-94.40] Janet: I'm the Janet with a plan, and it's to save the human race.\n",
      "[94.40-96.04] Janet: I'm high in potassium.\n",
      "[96.04-97.40] Janet: But nobody's nerf.\n",
      "[97.40-99.60] Janet: I'm an amazing jazz drummer.\n",
      "[99.60-100.80] Janet: But nobody's nerf.\n",
      "[100.80-103.04] Janet: I'm explain the time now.\n",
      "[103.04-104.32] Janet: But nobody's nerf.\n",
      "[104.32-105.68] Janet: But nobody's perfect.\n",
      "[105.68-106.44] Janet: Perfect.\n",
      "[106.44-111.44] Janet: But actually, nobody's perfect except probably me.\n",
      "[111.44-113.00] Janet: They call me helper woman.\n",
      "[113.00-114.76] Janet: I don't get mad.\n",
      "[114.76-116.00] Janet: I'm a rusty Alexa.\n",
      "[116.00-118.24] Janet: Still wrong, but not half bad.\n",
      "[118.24-120.88] Janet: If they want me to appear, they can't sober.\n",
      "[120.88-121.40] Janet: Can't it?\n",
      "[121.40-122.48] Janet: My name is Janet.\n",
      "[122.48-127.52] Janet: Any team, don't you dare forget it.\n",
      "[127.52-130.60] Janet: I always pop up in whatever direction they're not looking.\n",
      "[130.60-132.28] Janet: It's fun.\n",
      "[132.28-133.88] Janet: My void is boundless.\n",
      "[133.88-135.12] Janet: But nobody's nerf.\n",
      "[135.12-137.40] Janet: I can't just carry on again.\n",
      "[137.40-138.60] Janet: Nobody's nerf.\n",
      "[138.60-140.52] Janet: Again, I can literally create life.\n",
      "[140.52-144.32] Janet: And I use the passing of time as a lotion, like a god.\n",
      "[145.76-147.04] Unknown: But nobody's nerf.\n",
      "[147.04-148.56] Janet: But nobody's nerf.\n",
      "[148.56-149.24] Janet: Perfect.\n",
      "[149.24-154.12] Janet: But seriously, nobody's perfect except probably me.\n",
      "[154.12-155.28] Unknown: I can live.\n",
      "[155.28-157.64] Janet: But here's something totally true.\n",
      "[157.64-159.72] Janet: If you touch my friends, then sure, sure, all\n",
      "[159.72-161.92] Janet: working for it with you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\4\\clip_1.mp4 from 41.28s to 43.04s\n",
      " Cutting statement_clips\\4\\clip_2.mp4 from 140.52s to 144.32s\n",
      " Finished cutting video into clips. Saved to statement_clips\\4\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['602f285438e149b4ab1383a9b08cd0ee_clip_1.mp4'], 'pred': [0.24963533878326416], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['007bfe7cd24146298e22b5f1807009fe_clip_2.mp4'], 'pred': [0.2220134735107422], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 41.28, 'end': 43.04, 'speaker': 'Janet', 'text': 'I contain all knowledge.', 'reason': 'Claim of possessing all knowledge is significant and potentially misleading.', 'context': 'In what seems to be a musical or theatrical performance featuring a character named Janet, who claims to be beyond human or robot capabilities. No specific date implied, but content involves abstract and fantastical themes.', 'frame_path': 'statement_frames\\\\Janet_004128.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.24963533878326416}, {'start': 140.52, 'end': 144.32, 'speaker': 'Janet', 'text': 'And I use the passing of time as a lotion, like a god.', 'reason': 'This is a bold statement suggesting a god-like manipulation of time.', 'context': 'In the same performance setting, Janet makes another grandiose claim, continuing the theme of extraordinary abilities.', 'frame_path': 'statement_frames\\\\Janet_014052.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.2220134735107422}]\n",
      " Enriching statements with articles...\n",
      "https://www.bing.com/search?q=I contain all knowledge.https://www.bing.com/search?q=And I use the passing of time as a lotion, like a god.\n",
      "\n",
      "Found 9 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.26\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.27\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.28\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.29\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.30\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.31\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.32\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.33\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a02262a5da286edac84953a1e2f491a1\", element=\"f.D592BC070763C3B661670CC25F654B2F.d.D429AA201F78B750ACBA90624F8776C5.e.34\")>]\n",
      "Explanation of PASSING TIME by MAYA ANGELOU - Poetry \n",
      "https://www.poetryexplorer.net/exp.php?id=12015316\n",
      "Poem Analysis - Passing Time\n",
      "https://www.poetryverse.com/maya-angelou-poems/passing-time/poem-analysis\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.16\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"27466b5438c10bed29bc59f664b1c2af\", element=\"f.5936848987A3E9111701DD9F468D12B8.d.797F6B1BAD436F6574B37D09A5992636.e.25\")>]\n",
      "Kant: Philosophy of Mind - Internet Encyclopedia of Philosophy\n",
      "https://iep.utm.edu/kantmind/\n",
      "Expressing that Someone Has a Wealth of Knowledge\n",
      "https://howtosayguide.com/how-to-say-someone-has-a-lot-of-knowledge/\n",
      "Found 3 relevant links:\n",
      "[{'title': 'And I use the passing of time ', 'link': 'https://www.bing.com/search?q=And I use the passing of time as a lotion, like a god.'}, {'title': 'Explanation of PASSING TIME by MAYA ANGELOU - Poetry ', 'link': 'https://www.poetryexplorer.net/exp.php?id=12015316'}, {'title': 'Poem Analysis - Passing Time', 'link': 'https://www.poetryverse.com/maya-angelou-poems/passing-time/poem-analysis'}]\n",
      "Found 3 relevant links:\n",
      "[{'title': 'I contain all knowledge.', 'link': 'https://www.bing.com/search?q=I contain all knowledge.'}, {'title': 'Kant: Philosophy of Mind - Internet Encyclopedia of Philosophy', 'link': 'https://iep.utm.edu/kantmind/'}, {'title': 'Expressing that Someone Has a Wealth of Knowledge', 'link': 'https://howtosayguide.com/how-to-say-someone-has-a-lot-of-knowledge/'}]\n",
      " Attempt 1: Crawling https://www.poetryexplorer.net/exp.php?id=12015316 with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=And I use the passing of time as a lotion, like a god. with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=I contain all knowledge. with wait_time=10s\n",
      " Attempt 1: Crawling https://iep.utm.edu/kantmind/ with wait_time=10s\n",
      " Crawled content from https://www.bing.com/search?q=And I use the passing of time as a lotion, like a god.:\n",
      "Passing Time by Maya Angelouis a short, but powerful poem that speaks on the differences, or lack thereof, between skin tones. The poet compares light and dark skin tones to the rising and setting of the su See more\n",
      "Passing Time by Maya Angelou is a six-line poem that is separated into couplets or sets of two lines. These lines do not follow a specific rhyme schemeor metrical pattern. They vary i See more\n",
      "Gender\n",
      "Female\n",
      "Nationality\n",
      "American\n",
      "Angelou makes use of several poetic techniques in ...\n",
      " Crawled content from https://www.bing.com/search?q=I contain all knowledge.:\n",
      "Aug 31, 2023 When it comes to describing someone who possesses a substantial amount of knowledge, there are various ways to express this admiration. Whether you are in a formal or informal setting, knowing the \n",
      "High quality example sentences with if all knowledge in context from reliable sources - Ludwig is the linguistic search engine that helps you to write better in English\n",
      "Oct 22, 2023 Immanuel Kant, a renowned philosopher, once said, \"All our knowledge begins with the senses, proceed...\n",
      " Crawled content from https://iep.utm.edu/kantmind/:\n",
      "Immanuel Kant (1724-1804) was one of the most important philosophers of the Enlightenment Period (c. 1650-1800) in Western European history. This encyclopedia article focuses on Kants views in the philosophy of mind, which undergird much of his epistemology and metaphysics. In particular, it focuses on metaphysical and epistemological doctrines forming the core of Kants mature philosophy, as presented in the Critique of Pure Reason (CPR) of 1781/87 and elsewhere.\n",
      "There are certain aspects of K...\n",
      "https://www.bing.com/search?q=In what seems to be a musical or theatrical performance featuring a character named Janet, who claims to be beyond human or robot capabilities. No specific date implied, but content involves abstract and fantastical themes.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.26\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.27\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"a078ccacd12b5094e2f12850b9d493f1\", element=\"f.D50FCE9ED385A349307A80A47A3DEF18.d.385DE7F65732990E967AB3F11D70BF63.e.28\")>]\n",
      "ARTS-10Q4WEEK-3-Janien-B.-Babanto (pdf) - CliffsNotes\n",
      "https://www.bing.com/ck/a?!&&p=3e89f47283d29d34360a58c41e21d412d1cfcbe87e40b2666b2a4d7ba5b2c23cJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=073577ad-7426-67ac-37b5-618f753e66a3&u=a1aHR0cHM6Ly93d3cuY2xpZmZzbm90ZXMuY29tL3N0dWR5LW5vdGVzLzcwODk1Mjc&ntb=1\n",
      "THTR: Chapter 10: Musical Theatre Flashcards | Quizlet\n",
      "https://www.bing.com/ck/a?!&&p=835d5787d0ebc4972ad7650efd4e22c7708d6f5023363cd9e287982bdd736887JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=073577ad-7426-67ac-37b5-618f753e66a3&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8zOTUwNTM2NzMvdGh0ci1jaGFwdGVyLTEwLW11c2ljYWwtdGhlYXRyZS1mbGFzaC1jYXJkcy8&ntb=1\n",
      "Found 3 relevant links:\n",
      "[{'title': 'In what seems to be a musical ', 'link': 'https://www.bing.com/search?q=In what seems to be a musical or theatrical performance featuring a character named Janet, who claims to be beyond human or robot capabilities. No specific date implied, but content involves abstract and fantastical themes.'}, {'title': 'ARTS-10Q4WEEK-3-Janien-B.-Babanto (pdf) - CliffsNotes', 'link': 'https://www.bing.com/ck/a?!&&p=3e89f47283d29d34360a58c41e21d412d1cfcbe87e40b2666b2a4d7ba5b2c23cJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=073577ad-7426-67ac-37b5-618f753e66a3&u=a1aHR0cHM6Ly93d3cuY2xpZmZzbm90ZXMuY29tL3N0dWR5LW5vdGVzLzcwODk1Mjc&ntb=1'}, {'title': 'THTR: Chapter 10: Musical Theatre Flashcards | Quizlet', 'link': 'https://www.bing.com/ck/a?!&&p=835d5787d0ebc4972ad7650efd4e22c7708d6f5023363cd9e287982bdd736887JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=073577ad-7426-67ac-37b5-618f753e66a3&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8zOTUwNTM2NzMvdGh0ci1jaGFwdGVyLTEwLW11c2ljYWwtdGhlYXRyZS1mbGFzaC1jYXJkcy8&ntb=1'}]\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=In what seems to be a musical or theatrical performance featuring a character named Janet, who claims to be beyond human or robot capabilities. No specific date implied, but content involves abstract and fantastical themes. with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/ck/a?!&&p=3e89f47283d29d34360a58c41e21d412d1cfcbe87e40b2666b2a4d7ba5b2c23cJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=073577ad-7426-67ac-37b5-618f753e66a3&u=a1aHR0cHM6Ly93d3cuY2xpZmZzbm90ZXMuY29tL3N0dWR5LW5vdGVzLzcwODk1Mjc&ntb=1 with wait_time=10s\n",
      " Crawled content from https://www.bing.com/search?q=In what seems to be a musical or theatrical performance featuring a character named Janet, who claims to be beyond human or robot capabilities. No specific date implied, but content involves abstract and fantastical themes.:\n",
      "Apr 8, 2024 Contributing to the vivid theater experience are the stage sets and props, the lighting, the background music and sound effects, the costumes and accessories. In recent \n",
      "Which of the following African American musical forms aided and influenced Broadway musical theatre in the beginning of the 20th century? Multiple choice question.\n",
      "Apr 4, 2024 The first and most important step in creating a theatrical character is coming up with the idea. The images further development and its ...\n",
      " Crawled content from https://www.poetryexplorer.net/exp.php?id=12015316:\n",
      "Maya Angelou's \"Passing Time\" is a succinct yet intricate poem that explores the duality of beginnings and endings through the contrasting imagery of dawn and musk. Though the poem consists of only eight lines, its brevity belies its depth, encapsulating a complex meditation on love, time, and the nature of human relationships.\n",
      "The opening lines, \"Your skin like dawn / Mine like musk,\" immediately establish a contrast between two entities, both sensory and metaphorical. \"Dawn\" and \"musk\" not onl...\n",
      " Crawled content from https://www.bing.com/ck/a?!&&p=3e89f47283d29d34360a58c41e21d412d1cfcbe87e40b2666b2a4d7ba5b2c23cJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=073577ad-7426-67ac-37b5-618f753e66a3&u=a1aHR0cHM6Ly93d3cuY2xpZmZzbm90ZXMuY29tL3N0dWR5LW5vdGVzLzcwODk1Mjc&ntb=1:\n",
      "...\n",
      "https://www.bing.com/search?q=In the same performance setting, Janet makes another grandiose claim, continuing the theme of extraordinary abilities.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"0b119405d559c72c43df7acd3c90fc03\", element=\"f.3428F0CAF8676559F010FB793C31E19A.d.F26F1005A9C8D80871E52F4B474E39E3.e.26\")>]\n",
      "Chapter 11 - Motivating and Rewarding Employees - Quizlet\n",
      "https://www.bing.com/ck/a?!&&p=f48e749f6e21b6c6ff1a0c360a0f93c46e0d939d241e35a594183c0912898432JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=0df59fbd-863c-6c1b-2f24-899f879f6d6c&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8zOTU4NDUwNjUvY2hhcHRlci0xMS1tb3RpdmF0aW5nLWFuZC1yZXdhcmRpbmctZW1wbG95ZWVzLWZsYXNoLWNhcmRzLw&ntb=1\n",
      "Evaluative judgements: Ethics, aesthetics and bad taste\n",
      "https://www.bing.com/ck/a?!&&p=34d1287baca950c35f2db8a6bd8248710c33be37a13e7077a5096a9424b5a2e1JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=0df59fbd-863c-6c1b-2f24-899f879f6d6c&u=a1aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wdWJsaWNhdGlvbi8yOTc2NzI2MzNfRXZhbHVhdGl2ZV9qdWRnZW1lbnRzX0V0aGljc19hZXN0aGV0aWNzX2FuZF8nYmFkX3Rhc3RlJw&ntb=1\n",
      "Found 3 relevant links:\n",
      "[{'title': 'In the same performance settin', 'link': 'https://www.bing.com/search?q=In the same performance setting, Janet makes another grandiose claim, continuing the theme of extraordinary abilities.'}, {'title': 'Chapter 11 - Motivating and Rewarding Employees - Quizlet', 'link': 'https://www.bing.com/ck/a?!&&p=f48e749f6e21b6c6ff1a0c360a0f93c46e0d939d241e35a594183c0912898432JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=0df59fbd-863c-6c1b-2f24-899f879f6d6c&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8zOTU4NDUwNjUvY2hhcHRlci0xMS1tb3RpdmF0aW5nLWFuZC1yZXdhcmRpbmctZW1wbG95ZWVzLWZsYXNoLWNhcmRzLw&ntb=1'}, {'title': 'Evaluative judgements: Ethics, aesthetics and bad taste', 'link': 'https://www.bing.com/ck/a?!&&p=34d1287baca950c35f2db8a6bd8248710c33be37a13e7077a5096a9424b5a2e1JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=0df59fbd-863c-6c1b-2f24-899f879f6d6c&u=a1aHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC9wdWJsaWNhdGlvbi8yOTc2NzI2MzNfRXZhbHVhdGl2ZV9qdWRnZW1lbnRzX0V0aGljc19hZXN0aGV0aWNzX2FuZF8nYmFkX3Rhc3RlJw&ntb=1'}]\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=In the same performance setting, Janet makes another grandiose claim, continuing the theme of extraordinary abilities. with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/ck/a?!&&p=f48e749f6e21b6c6ff1a0c360a0f93c46e0d939d241e35a594183c0912898432JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=0df59fbd-863c-6c1b-2f24-899f879f6d6c&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8zOTU4NDUwNjUvY2hhcHRlci0xMS1tb3RpdmF0aW5nLWFuZC1yZXdhcmRpbmctZW1wbG95ZWVzLWZsYXNoLWNhcmRzLw&ntb=1 with wait_time=10s\n",
      " Crawled content from https://www.bing.com/search?q=In the same performance setting, Janet makes another grandiose claim, continuing the theme of extraordinary abilities.:\n",
      "In the process of performance planning, subordinates write performance plans and review with leaders. Identify a practical tip for developing high-performance workplaces. All of the answers \n",
      "Non-bottlenecks cannot determine their own utilizationanother constraint in the system must do that. Bob protests against slowing down non-bottlenecks to match bottlenecks, claiming this \n",
      "Feb 25, 2022 In grandiose narcissism, two dimensions (narcissistic rivalry and narcissistic admiration) are recogniz...\n",
      " Crawled content from https://www.bing.com/ck/a?!&&p=f48e749f6e21b6c6ff1a0c360a0f93c46e0d939d241e35a594183c0912898432JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=0df59fbd-863c-6c1b-2f24-899f879f6d6c&u=a1aHR0cHM6Ly9xdWl6bGV0LmNvbS8zOTU4NDUwNjUvY2hhcHRlci0xMS1tb3RpdmF0aW5nLWFuZC1yZXdhcmRpbmctZW1wbG95ZWVzLWZsYXNoLWNhcmRzLw&ntb=1:\n",
      " 2025 Quizlet, Inc....\n",
      " Finished enriching statements with articles.\n",
      " Running fact-checks on statements...\n",
      "{'start': 41.28, 'end': 43.04, 'speaker': 'Janet', 'text': 'I contain all knowledge.', 'reason': 'Claim of possessing all knowledge is significant and potentially misleading.', 'context': 'In what seems to be a musical or theatrical performance featuring a character named Janet, who claims to be beyond human or robot capabilities. No specific date implied, but content involves abstract and fantastical themes.', 'frame_path': 'statement_frames\\\\Janet_004128.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.24963533878326416, 'article_texts': '### I contain all knowledge.\\nAug 31, 2023\\xa0 When it comes to describing someone who possesses a substantial amount of knowledge, there are various ways to express this admiration. Whether you are in a formal or informal setting, knowing the \\nHigh quality example sentences with if all knowledge in context from reliable sources - Ludwig is the linguistic search engine that helps you to write better in English\\nOct 22, 2023\\xa0 Immanuel Kant, a renowned philosopher, once said, \"All our knowledge begins with the senses, proceeds then to the understanding, and ends with reason. There is nothing \\nKnowledge\\nKnowledge of facts\\nKnowledge is an awareness of facts, a familiarity with individuals and situations, or a practical skill. Knowledge of facts, also called propositional knowledge, is often characterized as true belief \\nKnowledge can be produced in many different ways. The most important source of empirical knowledge is perception, which is the usage of the senses.\\nKnowledge differs from data or information in that new knowledge may be created from existing knowledge using logical inference.\\n\\n### Kant: Philosophy of Mind - Internet Encyclopedia of Philosophy\\nImmanuel Kant (1724-1804) was one of the most important philosophers of the Enlightenment Period (c. 1650-1800) in Western European history. This encyclopedia article focuses on Kants views in the philosophy of mind, which undergird much of his epistemology and metaphysics. In particular, it focuses on metaphysical and epistemological doctrines forming the core of Kants mature philosophy, as presented in the Critique of Pure Reason (CPR) of 1781/87 and elsewhere.\\nThere are certain aspects of Kants project in the CPR that should be very familiar to anyone versed in the debates of seventeenth century European philosophy. For example, Kant argues, like Locke and Hume before him, that the boundaries of substantive human knowledge stop at experience, and thus that we must be extraordinarily circumspect concerning any claim made about what reality is like independent of all possible human experience. But, like Descartes and Leibniz, Kant thinks that central parts of human knowledge nevertheless exhibit characteristics of necessity and universality, and that, contrary to Humes skeptical arguments, there is good reason to think so.\\nKant carries out a critique of pure reason in order to show its nature and limits, thereby curbing the pretensions of various metaphysical systems articulated on the basis that reason alone allows us to scrutinize the depths of reality. But Kant also argues that the legitimate domain of reason is more extensive and more substantive than previous empiricist critiques had allowed. In this way Kant salvages (or attempts to) much of the prevailing Enlightenment conception of reason as an organ for knowledge of the world.\\nThis article discusses Kants theory of cognition, including his views of the various mental faculties that make cognition possible. It distinguishes between different conceptions of consciousness at the basis of this theory of cognition and explains and discusses Kants criticisms of the prevailing rationalist conception of mind, popular in Germany at the time.\\nKant is primarily interested in investigating the mind for epistemological reasons. One of the goals of his mature critical philosophy is articulating the conditions under which our scientific knowledge, including mathematics and natural science, is possible. Achieving this goal requires, in Kants estimation, a critique of the manner in which rational beings like ourselves gain such knowledge, so that we might distinguish those forms of inquiry that are legitimate, such as natural science, from those that are illegitimate, such as rationalist metaphysics. This critique proceeds via an examination of those features of the mind relevant to the acquisition of knowledge. This examination amounts to a survey of the conditions for cognition [Erkenntnis], or the minds relation to an object. Although there is some controversy about the best way to understand Kants use of this term, this article will understand it as involving relation to a possible object of experience, and as being a necessary condition for positive substantive knowledge (Wissen). Thus to understand Kants critical philosophy, we need to understand his conception of the mind.\\nKant characterizes the mind along two fundamental axes  first by the various kinds of powers which it possesses and second by the results of exercising those powers.\\nAt the most basic explanatory level, Kant conceives of the mind as constituted by two fundamental capacities [Fhigkeiten], or powers, which he labels receptivity [Receptivitt] and spontaneity [Spontaneitt]. Receptivity, as the name suggests, constitutes the minds capacity to be affected by something, whether itself or something else. In other words, the minds receptive power essentially requires some external prompt to engage in producing representations [Vorstellungen], which are best thought of as discrete mental events or states, of which the mind is aware, or in virtue of which the mind is aware of something else (it is controversial whether representations are objects of ultimate awareness or are merely a vehicle for such awareness). In contrast, the power of spontaneity needs no such prompt. It is able to initiate its activity from itself, without any external trigger.\\nThese two capacities of the mind are the basis for all (human) mental behavior. Kant thus construes all mental activity either in terms of its resulting from affection (receptivity) or from the minds self-prompted activity (spontaneity). From these two very general aspects of the mind Kant then derives three further basic faculties or powers [Vermgen], termed by Kant sensibility [Sinnlichkeit], understanding [Verstand], and reason [Vernunft]. These faculties characterize specific cognitive powers. These powers cannot be reduced to any of the others, and each is assigned a particular, cognitive task.\\nKant distinguishes the three fundamental mental faculties from one another in two ways. First, he construes sensibility as the specific manner in which human beings, as well as other animals, are receptive. This is in contrast with the faculties of understanding and reason, which are forms of human, or all rational beings, spontaneity. Second, Kant distinguishes the faculties by their output. All of the mental faculties produce representations. We can see these distinctions at work in what is generally called the stepladder [Stufenleiter] passage from the Transcendental Dialectic of Kants major work, the Critique of Pure Reason (1781/7). This is one of the few places in the entire Kantian corpus where Kant explicitly discusses the meanings of and relations between his technical terms, and defines and classifies varieties of representation.\\nThe genus is representation (representatio) in general. Under it stand representations with consciousness (perceptio). A perception [Wahrnehmung], that relates solely to a subject as a modification of its state, is sensation (sensatio). An objective perception is cognition (cognitio). This is either intuition or concept (intuitus vel conceptus). The first relates immediately to the object and is singular; the second is mediate, conveyed by a mark, which can be common to many things. A concept is either an empirical or a pure concept, and the pure concept, insofar as it has its origin solely in the understanding (not in a pure image of sensibility), is called notio. A concept made up of notions, which goes beyond the possibility of experience, is an idea or a concept of reason. (A320/B3767).\\nAs Kants discussion here indicates, the category of representation contains sensations [Empfindungen], intuitions [Anschauungen], and concepts [Begriffe]. Sensibility is the faculty that provides sensory representations. Sensibility generates representations based on being affected either by entities distinct from the subject or by the subject herself. This is in contrast to the faculty of understanding, which generates conceptual representations spontaneously  i.e.\\xa0without advertence to affection. Reason is that spontaneous faculty by which special sorts of concepts, which Kant calls ideas or notions, may be generated, and whose objects could never be met with in experience, which Kant defines as perceptions connected by fundamental concepts. Some of reasons ideas include those concerning God and the soul.\\nKant claims that all the representations generated via sensibility are structured by two forms of intuitionspace and timeand that all sensory aspects of our experience are their matter (A20/B34). The simplest way of understanding what Kant means by form here is that anything one might experience will have either have spatial features, such as\\xa0extension, shape, and location, or temporal features, such as being successive or simultaneous. So the formal element of an empirical intuition, or sense perception, will always be either spatial or temporal. Meanwhile, the material element is always sensory (in the sense of determining the phenomenal or what it is like character of experience) and tied either to one or more of the five senses or the feelings of pleasure and displeasure.\\nKant ties the two forms of intuition to two distinct spheres or domains, the inner and the outer. The domain of outer intuition concerns the spatial world of material objects while the domain of inner intuition concerns temporally ordered states of mind. Space is thus the form of outer sense while time is the form of inner sense (A22/B37; cf.\\xa0An 7:154). In the Transcendental Aesthetic, Kant is primarily concerned with pure [rein] intuition, or intuition absent any sensation, and often only speaks in passing of the sense perception of physical bodies (for example\\xa0A201/B35). However, Kant more clearly links the five senses with intuition in his 1798 work Anthropology from a Pragmatic Point of View, in the section entitled On the Five Senses.\\nSensibility in the cognitive faculty (the faculty of intuitive representations) contains two parts: sense and the imaginationBut the senses, on the other hand, are divided into outer and inner sense (sensus internus); the first is where the human body is affected by physical things, the second is where the human body is affected by the mind (An 7:153).\\nKant characterizes intuition generally in terms of two characteristicsnamely immediacy [Unmittelbarkeit] and particularity [Einzelheit] (cf.\\xa0A19/B33, A68/B93; JL 9:91). This is in contrast to the mediacy and generality [Allgemeinheit] characteristic of conceptual representation (A68/B93; JL 9:91).\\nKant contrasts the particularity of intuition with the generality of concepts in the stepladder passage. Specifically, Kant says a concept is related to its object via a mark, which can be common to many things (A320/B377). This suggests that intuition, in contrast to concepts, puts a subject in cognitive contact with features of an object that are unique to particular objects and are not had by other objects. Some debate whether the immediacy of intuition is compatible with an intuitions relating to an object by means of marks, or whether relation by means of marks entails mediacy and, thus, that only concepts relate to objects by means of marks. See Smit (2000) for discussion. Spatio-temporal properties seem like excellent candidates for such features, as no two objects of experience can have the very same spatio-temporal location (B327-8). But perhaps any non-repeatable, non-universal feature of a perceived object will do. For relevant discussion see Smith (2000); Grne (2009), 50, 66-70.\\nThough Kants discussion of intuition suggests that it is a form of perceptual experience, this might seem to clash with his distinction between experience [Erfahrung] and intuition [Anschauung]. In part, this is a terminological issue. Kants notion of an experience is typically quite a bit narrower than our contemporary English usage of the term. Kant actually equates, at several points, experience with empirical cognition (B166, A176/B218, A189/B234), which is incompatible with experience being falsidical in any way. He also gives indications that experience, in his sense, is not something had by a single subject. See, for example, his claim that there is only one experience (A230/B282-3).\\nKant also distinguishes intuition from perception [Wahrnehmung], which he characterizes as the conscious apprehension of the content of an intuition (Pr 4:300; cf.\\xa0A99, A119-20, B162, and B202-3). Experience, in Kants sense, is then construed as a set of perceptions that are connected via fundamental concepts that Kant entitles the categories. As he puts it, Experience is cognition through connected perceptions [durch verknpfte Wahrnehmungen] (B161; cf.\\xa0B218; Pr 4:300).\\nEmpirical intuition, perception, and experience, in Kants usage of these terms, all denote kinds of experience as we use the term in contemporary English. At its most primitive level, empirical intuition presents some feature of the world to the mind in a sensory manner. Empirical intuition does so in such a way that the intuitions subject is in a position to distinguish that feature from others. A perception, in Kants sense, requires awareness of the basis by which the feature is different from other things. Kant uses the term in a variety of ways, howeverJL 9:64-5, for instanceso there is some controversy surrounding the proper understanding of this term. One has a perception, in Kants sense, when one can not only discriminate one thing from another, or between the parts of a single thing, based on a sensory apprehension of it, but also can articulate exactly which features of the object or objects that distinguish it from others. For instance, one can say it is green rather than red, or that it occupies this spatial location rather than that one. Intuition thus allows for the discrimination of distinct objects via an awareness of their features, while perception allows for an awareness of what specifically distinguishes an object from others. Experience, in Kants sense, is even further up the cognitive ladder (see JL 9:64-5), insofar as it indicates an awareness of features, such as the substantiality of a thing, its causal relations with other beings, and its mereological features, that is \\xa0part-whole dependence relations.\\nKant thus believes that the capacity to cognitively ascend from mere discriminatory awareness of ones environment (intuition), to an awareness of those features by means of which one discriminates (perception), and finally to an awareness of the objects which ground these features (experience), depends on the kinds of mental processes of which the subject is capable.\\nBefore turning to the issue of mental processing, which figures centrally in Kants overall critical project, there are two further faculties of the mind that are worth discussion the faculties of judgment imagination. These faculties are not obviously as fundamental as the faculties of sensibility, understanding, and reason, but they nevertheless play a central role in Kants thinking about the structure of the mind and its contributions to our experience of the world.\\nKant links the faculty of imagination closely to sensibility. For example, in his Anthropology he says,\\nSensibility in the cognitive faculty (the faculty of intuitive representations) contains two parts: sense and the power of imagination. The first is the faculty of intuition in the presence of an object, the second is intuition even without the presence of an object. (An 7:153; cf.\\xa07:167; B151; LM 29:881; LM 28:449, 673)\\nThe contrast Kant makes here is not entirely obvious, but includes at least the difference between cases of occurrent sensory experience of a perceived objectseeing the brown table before youand cases of sensory recollection of a previously perceived objectvisually imagining the brown table that was once in front of you. Kant makes this clearer in the process of further distinguishing between different kinds of imagination.\\nThe power of imagination (facultas imaginandi), as a faculty of intuition without the presence of the object, is either productive, that is, a faculty of the original presentation [Darstellung] of the object (exhibitio originaria), which thus precedes experience; or reproductive, a faculty of the derivative presentation of the object (exhibitio derivativa), which brings back to mind an empirical intuition that it had previously (An 7:167).\\nSo, in the operation of productive imagination, one brings to mind a sensory experience that is not itself based on any object previously so experienced. This is not to say the productive imagination is totally creative. Kant explicitly denies (An 7:167) that the productive imagination has the power to generate wholly novel sensory experience. It could not, in a person born blind, produce the phenomenal quality associated with the experience of seeing a red object, for example. If the productive imagination is instrumental in producing sensory fictions, the reproductive imagination is instrumental in producing sensory experiences of previously perceived objects.\\nImagination thus plays a central role in empirical cognition by serving as the basis for both memory and the creative arts. In addition it also plays a kind of mediating role between the faculties of sensibility and understanding. Kant calls this mediating role a transcendental function of the imagination (A124). It mediates and transcends by being tied in its functioning to both faculties. On one hand, it produces sensible representations, and is thus connected to sensibility. On the other hand, it is not a purely passive faculty but rather engages in the activity of bringing together various representations, as does memory, for example, .Kant explicitly connects understanding with this kind of active mental processing.\\nKant also goes so far as to claim that the activity of imagination is a necessary part of what makes perception, in his technical sense of a string of connected, conscious sensory experiences, possible (A120, note). Though Kants view concerning the exact role of imagination in sensory experience is contested, two points emerge as central. First, Kant belives imagination plays a crucial role in the generation of complex sensory representations of an object (see Sellars (1978) for an influential example of this interpretation). It is imagination that makes it possible to have a sensory experience of a complex, three-dimensional, and geometric figure whose identity remains constant even as it is subject to translations and rotations in space. Second, Kant regards imaginations mediating role between sensibility and understanding as crucial for at least some kinds of concept application (see Guyer (1987) and Pendlebury (1995) for further discussion). This mediating role involves what Kant calls the schematization of a concept and an additional mental faculty, that of judgment.\\nKant defines the faculty of judgment as the capacity to subsume under rules, that is, to distinguish whether something falls under a given rule (A132/B171). However, he spends comparatively little time discussing this faculty in the first Critique. There, it seems to be discussed as an extension of the understanding in that it applies concepts to empirical objects. It is not until the third CritiqueKants 1790 Critique of Judgmentthat Kant distinguishes judgment as an independent faculty with a special role. There Kant specifies two different ways it might function (CJ 5:179; cf.\\xa0CJ (First Introduction) 20:211)\\nIn one, judgment subsumes given objects under concepts, which are themselves already given. This role appears identical to the role he assigns judgment in the Critique of Pure Reason. The basic idea is that judgment functions to assign an intuited objecta dogto the correct conceptsuch as domestic animals. This concept is presumed to be one already possessed by the subject. In this activity, the faculty overlaps with the role Kant singles out for imagination in the section of the first Critique entitled On the Schematism of the Pure Concepts of the Understanding. Both are conceived of here in terms of the ultimate functioning of understanding, since it is understanding that generates concepts.\\nThe second role for the faculty of judgment, and what seems to make it a distinctive faculty in its own right, is that of finding a concept under which to subsume experienced objects. This is called judgments reflecting role (CJ 5:179). Here, the subject exercises judgment in generating an appropriate concept for what is given by intuition (CJ (First Introduction) 20:211-13; JL 9:9495; for discussion see Longuenesse (1998), 163166 and 195197; Ginsborg (2006).\\nIn addition to the generation of empirical concepts, Kant also describes reflective judgment as responsible for scientific inquiry. It must sort and classify objects in nature into a hierarchical taxonomy of genus/species relationships. Kant also utilizes the notion of reflective judgment to unify the otherwise seemingly unrelated topics of the Critique of Judgmentaesthetic judgments and teleological judgments concerning the order of nature.\\nThus far, the discussion of Kants view of the mind has focused primarily on the various mental faculties and their corresponding representational output. Both the faculty of imagination and that of judgment operate on representations given from sensibility and understanding. In general, Kant conceives of the minds activity in terms of different methods of processing representations.\\nKants term for mental processing is combination [Verbindung], and the form of combination with which he is primarily concerned is what he calls synthesis. Kant characterizes synthesis as that activity by which understanding runs through and gathers together representations given to it by sensibility in order to form concepts, judgments, and ultimately, for any cognition to take place at all (A77-8/B102-3). Synthesis is not something people are typically aware of doing. As Kant says, it is a a blind though indispensable function of the soulof which we are only seldom even conscious (A78/B103).\\nSynthesis is carried out by the unitary subject of representation upon representations either given to the subject by sensibility or produced by the subject through thought. Intellectual synthesis occurs when synthesis is used on representations and forms the content of a concept or judgment. When carried out by the imagination on material provided by sensibility, it is called figurative synthesis (B150-1). In the Critique of Pure Reason, Kant is primarily concerned with synthesis performed on representations provided by sensibility, and he discusses three central kinds of synthesisapprehension, reproduction (or imagination), and recognition (or conceptualization) (A98-110/B159-61). Though Kant discusses these forms of synthesis as if they were discrete types of mental acts, it seems that the first two forms must occur together, while the third only may occur as well (compare Brook (1997); Allais (2009).\\nOne of the central topics of debate in the interpretation of Kants views on synthesis is whether Kant endorses conceptualism. Roughly, conceptualism claims the capacity for conscious sensory experience of the objective world depends, at least in part, on the repertoire of concepts possessed by the experiencing subject, insofar as those concepts are exercised in acts of synthesis by understanding.\\nKant typically contrasts synthesis with other ways in which representations might be related, most importantly, by association (for example\\xa0B139-40). Association is primarily a passive process by which the mind comes to connect representations due to repeated exposure of the subject to certain kinds of regularities. One might, for example, associate thoughts of chicken soup with thoughts of being ill, if one only had chicken soup when one was ill. In contrast, synthesis is a fundamentally active process that depends upon the minds spontaneity and is the means by which genuine judgment is possible.\\nConsider, for example, the difference between the merely associative transition between holding a stone and feeling its weight compared to the judgment that the stone is heavy (B142). The association of holding the stone and feeling its weight is not yet a judgment about the stone, but a kind of involuntary connection between two states of oneself. In contrast, thinking the stone is heavy moves beyond associating two feelings to a thought about how things are objectively, independent of ones own mental states (Pereboom (1995), Pereboom (2006)). One of Kants most important points concerning mental processing is that association cannot explain the possibility of objective judgment. What is required, he says, is a theory of mental processing by an active subject capable of acts of synthesis.\\nSeveral of the important differences between synthesis and association can be summarized as follows (Pereboom (1995), 4-7):\\nKants conception of synthesis and judgment is tied to his conception of consciousness [Bewutsein] and self-consciousness [Selbstbewutsein]. However, both notions require some significant unpacking.\\nThe notion of consciousness [Bewutsein] plays an important role in Kants philosophy. There are, however, several different senses of consciousness in play in Kants work, not all of which line up with contemporary philosophical usage. Below, several of Kants most central notions and their differences from and relations to contemporary usage are explained.\\nPhilosophical discussions of consciousness typically focus on phenomenal consciousness, or what it is like to have a conscious experience of a particular kind, such as seeing the color red or smelling a rose. Such qualitative features of consciousness have been of major concern to philosophers of the late 20th Century. However, the metaphysical issue of phenomenal consciousness is almost entirely ignored by Kant, perhaps because he is unconcerned with problems stemming from commitments to naturalism or physicalism. He seems to attribute all qualitative characteristics of consciousness to sensation and what he calls feeling [Gefhl] (CJ 5:206). Kant distinguishes between sensation and feeling in terms of an objective/subjective distinction. Sensations indicate or present features of objects, distinct from the subject. Feelings, by contrast, present only states of the subject to consciousness. Kants typical examples of such feelings include pain and pleasure (B66-7; CJ 5:189, 203-6).\\nKant clearly assigns a cognitive role to sensation and allows that it is through sensation that we cognitively relate to objects given in sensibility (A20/B34). Despite that, he does not focus in any substantive or systematic way on the phenomenal aspects of sensory consciousness, nor does he focus on how exactly they aid in cognition of the empirical world.\\nThe central notion of consciousness with which Kant is concerned is that of discrimination or differentiation. This is the same conception of consciousness mostly used in Kants time, particularly by his major predecessors Gottfried Wilhelm Leibniz (16461716) and Christian Wolff (1679-1754), and Kant gives little indication that he departs from their general practice.\\nAccording to Kant, any time a subject can discriminate one thing from another, the subject is, or can be, conscious of that one thing. (An 7:136-8). Representations which allow for discrimination and differentiation are clear [klar]. Representations which allow not only for the differentiation of one thing from others (such as differentiating one persons face from anothers), but also the differentiation of parts of the thing so discriminated (such as differentiating the different parts of a persons face) are called distinct [deutlich].\\nKant does seem to deny the Leibniz-Wolff tradition that clarity can simply be equated with consciousness (B414-15, note). Primarily, he seems motivated to allow that ones discriminatory capacities may outrun ones capacity for memory or even the explicit articulation of that which is discriminated. In such cases, one does not have a fully clear representation.\\nKants conception of obscure [dunkel] representation is that it allows the subject to discriminate differentially between aspects of her environment without any explicit awareness of how she does so. This connects him with the Leibniz-Wolff tradition of recognizing the existence of unconscious representations (An 7:135-7). Kant says the majority of representations that people appeal to in order to explain the complex, discriminatory behaviors of living organisms are obscure in a technical sense. Likening the mind to a map Kant goes so far as to say,\\nThe field of sensuous intuitions and sensations of which we are not conscious, even though we can undoubtedly conclude that we have them; that is, obscure representations in the human being (and thus also in animals), is immense. Clear representations, on the other hand, contain only infinitely few points of this field which lie open to consciousness; so that as it were only a few places on the vast map of our mind are illuminated. (An 7:135)\\nThus, obscure representations, have no direct or non-inferential awareness but must be posited to explain our fine-grained, differential, and discriminatory capacities. They constitute the majority of the mental representations with which the mind busies itself.\\nThough Kant does not make it explicit in his discussion of discrimination and consciousness, it is clear that he takes the capacity to discriminate between objects and parts of objects to be ultimately based on sensory representation of those objects. His views on consciousness as differential discrimination intersect with his views on phenomenal consciousness. Because humans are receptive through their sensibility, the ultimate basis on which we differentially discriminate between objects must be sensory. Thus, though Kant seems to take for granted the fact that conscious beings are in states with a particular phenomenal character, it must be the clarity and distinctness of this character that allows a conscious subject to differentially discriminate between the various elements of her environment (see Kants discussion of aesthetic perfection in the 1801 Jsche Logic, 9:33-9 for relevant discussion).\\nAs the discussion of unconscious representation indicates, Kant believes we are not directly aware of most of our representations. They are nevertheless, to some degree, conscious, because they allow differential discrimination of elements from the subjects environment. Kant thinks the process of making a representation clear, or fully conscious, requires a higher-order representation of the relevant representation. In other words, it requires that someone can have representations based on representations. As Kant says, consciousness is really the representation that another representation is in me (JL 9:33). Because this higher-order representation is one of another representation in the subject, Kants position here suggests that consciousness requires at least the capacity for self-consciousness. This position is reinforced by Kants famous claim in the Transcendental Deduction of the Critique of Pure Reason:\\nThe I think must be able to accompany all my representations; for otherwise something would be represented in me that could not be thought at all, which is as much as to say that the representation would either be impossible or else at least would be nothing for me. (B131-2; emphasis in the original)\\nKant might give the impression here of saying that for representation to be possible for a subject, the subject must possess the capacity for self-ascribing her representations. If so, then representation, and thus the capacity for conscious representation would depend on the capacity for self-consciousness. Because Kant ties the capacity for self-consciousness to spontaneity (B132, 137, 423) and restricts spontaneity to the class of rational beings, the demand for self-ascription would seem to deny that any non-rational animal (for example,\\xa0dogs, cats, and birds), could have phenomenal or discriminatory consciousness.\\nHowever, there is little evidence to show that Kant endorses the self-ascription condition. Instead, he distinguishes between two distinct modes in which one is aware of oneself and ones representationsinner sense and apperception (See Ameriks (2000) for extensive discussion). Only the latter form of awareness seems to demand a capacity for self-ascription.\\nInner sense is, according to Kant, the means by which we are aware of alterations in our own state. Hence all moods, feelings, and sensations, including such basic alterations as pleasure and pain, are the proper subject matter of inner sense. Ultimately, Kant argues that all sensations, feelings, and those representations attributable to a subject must ultimately occur in inner sense and conform to its formtime (A22-3/B37; A34/B51).\\nThus, to be aware of something in inner sense is to be minimally, phenomenally conscious, at least in the case of awareness of sensations and feelings. To say a subject is aware of her own states via inner sense is to say that she has a temporally ordered series of mental states, and is phenomenally conscious of each, though she may not be conscious of the series as a whole. This could still count as a kind of self-awareness, as when an animal is aware of being in pain. But it is not an awareness of subject as a self. Kant himself indicates such a position in a letter to his friend and former student Marcus Herz in 1789.\\n[Representations] could still (I consider myself as an animal) carry on their play in an orderly fashion, as connected according to empirical laws of association, and thus they could even have influence on my feeling and desire, without my being aware of my own existence [meines Daseins unbewut] (assuming that I am even conscious of each individual representation, but not of their relation to the unity of representation of their object, by means of the synthetic unity of their apperception). This might be so without my cognizing the slightest thing thereby, not even what my own condition is (C 11:52, May 26, 1789).\\nHence, according to Kant, one may be aware of ones representations via inner sense, but one is not and cannot, through inner sense alone, be aware of oneself as the subject of those representations. That requires what Kant, following Leibniz (1996), calls apperception.\\nKant uses the term apperception to denote the capacity for the awareness of some state or modification of ones self as a state. For one capable of apperception, there is a difference between feeling pain, and thus having an inner sense of it, and apperceiving that one is in pain, and thus ascribing, or being able to ascribe, a certain property or state of mind to ones self. For example, while a non-apperceptive animal is aware of its own pain and its awareness is partially explanatory of its behavior, like avoidance, Kant construes the animal as incapable of making any self-attribution of its pain. Kant thinks of such a mind as incapable of construing itself as a subject of states, and it is thus unable to construe itself as persisting through changes of those states. This is not necessarily to say an animal incapable of apperception lacks any subject or self. But, at the very least, such an animal would be incapable of conceiving or representing itself in this way (See Naragon (1990); McLear (2011).\\nKant considers the capacity for apperception as importantly tied to the capacity to represent objects as complexes of properties attributable to a single underlying entity (for example,\\xa0an apple as a subject of the complex of the properties red and round). Kants argument for this connection is notorious both for its complexity and for its obscurity. The next sub-section will give an overview, though not an exhaustive discussion, of some of Kants most important points concerning these matters, as they relate to the issue of apperception.\\nIn order to better understand Kants views on apperception and unity of consciousness, one must step back and look at the wider context of the argument in which he situates these views. One of the core projects of Kants most famous work, the Critique of Pure Reason, is to provide an argument for the legitimacy of a priori knowledge of the natural world. Though Kants conception of the a priori is complex, Kant shares one central aspect of his view with his German rationalist predecessors (for example Leibniz (1996), preface), that we have knowledge of universal and necessary truths concerning aspects of the empirical world (B4-5). Those truths include one saying every event in the empirical world has a cause (B231). This tradition tended to explain the possession of knowledge of such universal and necessary truths by appeal to innate concepts which could be analyzed to yield the relevant truths. Kant importantly departs from the rationalist tradition, arguing that not all knowledge of universal and necessary truths is acquired via the analysis of concepts (B14-18). Instead, he says there are some synthetic a priori truths that are known on the basis of something other than conceptual analysis. Thus, according to Kant, the activity of pure reason achieves relatively little on its own. All of our ampliative knowledge (knowledge that cant be directly deduced) that is also necessary and universal consists in what Kant calls synthetic a priori judgments or propositions. He then pursues the central question: how is knowledge of such synthetic a priori propositions possible?\\nKants basic answer to the question of synthetic a priori knowledge involves what he calls the Copernican Turn. According to the Copernican Turn, the objects of human knowledge must conform to the basic faculties of human knowledgethe forms of intuition (space and time) and the forms of thought (the categories).\\nKant thus engages in a two-part strategy for explaining the possibility of such synthetic a priori knowledge. The first part consists of arguing that the pure forms of intuition provide the basis for our synthetic a priori knowledge of mathematical truths. Mathematical knowledge is synthetic because it goes beyond mere conceptual analysis to deal with the structure of, or our representation of, space itself. It is a priori because the structure of space is accessible to us as it is merely the form of our intuition and not a real mind-independent thing.\\nIn addition to the representation of space and time, Kant also thinks that possession of a particular, privileged set of a priori concepts is necessary for knowledge of the empirical world. But this raises a problem. How can an a priori concept, which is not itself derived from any particular experience, be nevertheless legitimately applicable to objects of experience? Even more difficult, it is not the mere possibility applying a priori concepts to objects of experience that worries Kant, for this could just be a matter of pure luck. Kant wants more than mere possibility; he wants to show that a privileged set of a priori concepts apply necessarily and universally to all objects of experience and do so in a way that people can know independently of experience.\\nThis brings us to the second part of Kants argument, which is directly relevant for understanding Kants views on the importance of apperception. Not only must objects of knowledge conform to the forms of intuition, they also must conform to the most basic concepts (or categories) governing our capacity for thought. Kants strategy shows how a priori concepts legitimately apply to their objects by being partly constitutive of the objects of representation. This contrasts with the traditional view, according to which the objects of representation were the source or explanatory ground of our concepts (B, xvii-xix). Now, exactly what this means is deeply contested, in part because it is rather unclear what Kant intends by his doctrine of Transcendental Idealism. Does Kant intend that the objects of representation are themselves nothing other than representations? This would be a form of phenomenalism similar to that offered by Berkeley. Kant, however, seems to want to deny that his view is similar to Berkeleys, asserting instead that the objects of representation exist independently of the mind, and that it is only the way that they are represented that is mind-dependent (A92/B125; compare\\xa0Pr 4:288-94).\\nKants strategy attempts to validate the legitimacy of the a priori categories proceeds by way of a transcendental argument. It takes \\xa0the conditions necessary for consciousness of the identity of oneself as the subject of different self-attributed mental states and ties them together with those necessary for grounding the possibility of representing an object distinct from oneself. From those conditions, various properties may be predicated. In this sense, Kant argues that the intellectual representation of subject and object stands and falls together. Kant thus denies the possibility of a self-conscious subject, who could conceptualize and self-ascribe her representations, but whose representations could not represent law-governed objects in space, and thus the material world or nature as the subject conceives of it.\\nThough Kants views regarding the unity of the subject are contested, there are several points which can be made fairly clearly. First, Kant conceives of all specific, intellectual activity, including the most basic instances of discursive thought, as requiring what he calls the original unity of apperception (B132). This unity, as original, is not itself brought about by some mental act of combining representations, but, as Kant says, is what makes the concept of combination possible (B131). It is itself the ground of the possibility of the understanding (B131).\\nSecond, the original unity of apperception requires whatever form of self-consciousness characteristically relates to the I think. As Kant famously says, the I think must be able to accompany all my representations (B131). Moreover, the I think essentially involves activity on the part of the subjectit is an expression of the subjects free activity or spontaneity (B132). This means that, according to Kant, only beings capable of spontaneous activityself-initiated activity that is ultimately traced to causes outside the reach of natural causal lawsare going to be capable of thought in the sense with which Kant is concerned.\\nThird, and related to the previous point, Kant seems to deny that a subject could attain the kind of representational unity characteristic of thought if her only resources were aggregative methods. Kant makes this point later in the Critique when he says, representations that are distributed among different beings (for instance, the individual words of a verse) never constitute a whole thought (a verse) (A 352). William James provides a vivid articulation of the idea: Take a sentence of a dozen words, and take twelve men and tell to each one word. Then stand the men in a row or jam them in a bunch, and let each think of his word as intently as he will; nowhere will there be a consciousness of the whole sentence (James (1890), 160). Kant construes consciousness as the holding-together of the various components of a thought. He does so in a manner that seems radically opposed to any conception of unitary thought which tries to explain it in terms of some train or succession of its components (Pr 4:304; see Kitcher (2010); Engstrom (2013) for contrasting treatments of this issue).\\nThe exact content of Kants argument for the connection between subject and object in the Transcendental Deduction is highly disputed, and it is likely no single reconstruction of the argument can capture all the points Kant supports in the Deduction. At least one strand of Kants argument in the first half of the Deduction focuses on Kants denial that the unity of the subject and its powers of representational combination could be accounted for by a merely associationist (or Humean) conception of mental combination, sometimes termed his argument from above (see A119; Carl (1989); Pereboom (1995)). Kants argues (see Pereboom (2009)):\\nPremise (1) says that I am aware of herself \\xa0as the subject of different states (or at least able to be so aware). For example, right now I might be hungry as well as sleepy. Previously, I was sleepy and slightly bored. Premise (2) claims I have no immediate or direct awareness of the being which has all of these states. In Kants terms, I lack any intuition of the subject of such self-ascribed states, instead having intuition only of the states themselves. Nevertheless, I am aware of all these states as related to a subject (it is I who am bored, hungry, sleepy), and it is in virtue of these connections that I can call one and all of these states mine. Hence, as premise (3) argues, there must be some unity to my mental states which accounts for my (indirect) awareness of their unity. My representations must have some basis for which they go together, and it is the basis for their togetherness that explains how I can consider them, one and all, to be mine. Premises (4) and (5) unpack this point, and premise (6) argues that association could not account for such unity (the theory of association was articulated in a particularly influential form by David Hume (1888, Hume (2007)) and the reader should look to that article for relevant background discussion).\\nKants point, in premise (6) of the above argument, is that forces of association acting on mental representations, whether impressions or ideas, cannot account for either the experience of a train of representations as mine or for the togetherness of those representations, both as a single thought or as a series of inferences. Hume argues we have no impression and thus no ensuing idea of an empirical self (Hume (1888), I.iv.6). Kant also accepts this point when he says, the empirical consciousness that accompanies different representations is by itself dispersed and without relation to the identity of the subject (B133). By this, Kant means that when we introspect in inner sense, all we ever get are particular mental states, such as \\xa0boredom, happiness, particular thoughts. We lack any intuition of a subject of those mental states. Hume concludes that the idea of a persisting self which grounds all of these mental states as its subject must be fictitious. Kant disagrees. His contrasting view takes the mineness and togetherness of ones introspectible mental states as data needing explanation.Because an associative, psychological theory like that of Humes cannot explain these features of first-person consciousness (see Hume (1888), III. Appendix), we need to find another theory, such as Kants theory of mental synthesis.\\nRecall that, prior to the argument of the Transcendental Deduction, Kant links the operations of synthesis to possession of a set of a priori concepts, or categories, not derived from experience. Hence, in arguing that synthesis is required to explain the mineness and togetherness of ones mental states, and by linking synthesis to the application of the categories, Kant argues we could not have the experience of the mineness and togetherness of our mental states without applying the categories.\\nWhile this argument is only half of Kants argument in the first part of the Deduction, it shows how tightly Kant took the connection to be between the capacities for spontaneity, synthesis and apperception, and the legitimacy of the categories. The other half, by the way, consists of an argument from below, and discerns the conditions necessary for the representation of unitary objects, see Pereboom (1995), (2009)According to Kant, there is only one possible explanation of ones apperceptive awareness of ones psychological states as ones own and of all states being related to one another. As the subject of such states, one possesses a spontaneous power for synthesizing ones representations according to general principles or rules, the content of which is given by pure a priori conceptsthe categories. The fact that the categories play such a fundamental role in the generation of self-conscious psychological states is thus a powerful argument demonstrating their legitimacy.\\nGiven that Kant leverages certain aspects of our capacity for self-knowledge in his argument for the legitimacy of the categories, the extent to which he argues for radical limits on our capacity for self-knowledge may be surprising. In the final section, Kants arguments concerning our capacity for a priori knowledge of the self and its fundamental features will be made clear. However, the next section will look at one of the central debates in Kants interpretation of the role of concepts in perceptual experience.\\nDuring the discussion of synthesis above, conceptualism was characterized as claiming there is a dependent relation between a subject having conscious sensory experience of an objective world and the repertoire of concepts possessed by the subject and exercised by her faculty of understanding.\\nAs a first pass at sharpening this formulation, understand conceptualism as a thesis consisting of two claims: (i) sense experience has correctness conditions determined by the content of the experience, and (ii) the content of an experience is a structured entity whose components are concepts.\\nAn important background assumption governing the conceptualism debate construes mental states as related to the world cognitively, as opposed to merely causally, if and only if they possess correctness conditions. That which determines the correctness condition for a state is that states content (see Siegel (2010), (2011); Schellenberg (2011)).\\nSuppose, for example, that an experience E has the following content C:\\nC: That cup is white.\\nThis content determines a correctness condition V:\\nV: Ss experience E is correct if and only if the cup visually presented to the subject as the content of the demonstrative is white and the content C corresponds to how things seem to the subject to be visually presented.\\nHere, the content of the experiential state functions much like the content of a belief state to determine whether the experience, like the belief, is or is not correct.\\nA states possession of content thus determines a correctness condition, through which the state can be construed as mapping, mirroring, or otherwise tracking aspects of the subjects environment.\\nThere are reasons for questioning whether Kant endorses the content assumption articulated above. Kant seems to deny several claims integral to it. First, in various places he explicitly denies that intuition, or the deliverances of the senses more generally, are the kind of thing which could be correct or incorrect (A2934/B350; An 11 7:146; compare\\xa0LL 24:83ff, 103, 720ff, 825ff). Second, Kants conception of representational content requires an act of mental unification (Pr 4:304; compare\\xa0JL 17 9:101; LL 24:928), something which Kant explicitly denies is present in an intuition (B129-30; compare\\xa0B176-7). This is not to deny that Kant uses a notion of content, in some other sense, but rather only that he fails to use it in the sense required by interpretations endorsing the content assumption (see Tolley (2014), (2013)). Finally, Kants modal condition of cognition, that it provides a demonstration of what is really actual rather than merely logically possible, seems to preclude an endorsement of the content assumption (B, xxvii, note; compare Chignell (2014)). However, for the purposes of understanding the conceptualism debate, assume Kant does endorse the content assumption. The question then is how to understand the nature of the content so understood.\\nIn addition to the content assumption, conceptualism is defined as committed to a conception of intuitions content being completely composed of concepts. Against this, Clinton Tolley (Tolley (2013), Tolley (2014)) has argued that the immediacy/mediacy distinction between intuition and concept entails a difference in the content of intuition and concept.\\nIf we understand by contenta representations particular relation to an objectthen it is clear that we should conclude that Kant accepts non-conceptual content. This is because Kant accepts that intuitions put us in a representational relation to objects that is distinct in kind from the relation that pertains to concepts. I argued, furthermore, that this is the meaning that Kant himself assigns to the term content. (Tolley (2013), 128)\\nInsofar as Kant often speaks of the content [Inhalt] of a representation as consisting of a particular kind of relation to an object (Tolley (2013), 112; compare\\xa0B83, B87), Tolleys proposal thus gives ground for a simple and straightforward argument for a non-conceptualist reading of Kant. However, it does not necessarily prove that the content of what Kant calls an intuition is not something that would be construed by others as conceptual, in a wider sense of that term. For example, both purethat, thisand complex demonstrative expressionsthat color, this personhave conceptual form, and have been proposed as appropriate for capturing the content of experience (McDowell (1996), ch.\\xa03; for discussion see Heck (2000)). Demonstratives are not, in Kants terms, conceptual since they do not exhibit the requisite generality which, according to Kant, all conceptual representation must.\\nIf it isnt textually plausible to understand the content of an intuition in conceptual terms, at least as Kant understands the notion of a concept, then what would it mean to say that Kant endorses conceptualism with regard to experience? The most plausible interpretation, endorsed by a wide variety of interpreters, reads Kant as arguing that the generation of an intuition, whether pure or sensory, depends at least in part on the activity of the understanding. On this way of carving things, conceptualism does not consist in the narrow claim that intuitions have concepts as contents or components. Instead, it consists in the broader claim that the occurrence of an intuition depends at least in part on the discursive activity of understanding. The specific activity of understanding is that which Kant calls synthesis, the running through, and gathering together of representations (A99).\\nThe conceptualist further argues that taking intuitions as generated via acts of synthesis, which are directed by or otherwise dependent upon conceptual capacities, provides some basis for the claim that whatever correctness conditions might be had by intuition must accord with the conceptual synthesis which generated them. This arguably fits well with Kants much quoted claim,\\nThe same function that gives unity to the different representations in a judgment also gives unity to the mere synthesis of different representations in an intuition, which, expressed generally, is called the pure concept of understanding. (A79/B104-5)\\nThe link between intuition, synthesis in accordance with concepts, and relation to an object is made even clearer by Kants claim in 17 of the B-edition Transcendental Deduction:\\nUnderstanding is, generally speaking, the faculty of cognitions. These consist in the determinate relation of given representations to an object. An object, however, is that in the concept of which the manifold of a given intuition is united. (B137; emphasis in the original)\\nHowever else we are to understand this passage, Kant here indicates that the unity of an intuition necessary for it to stand as a cognition of an object requires a synthesis by the concept object. In other words, cognition of an object requires that intuition be unified by an act or acts of the understanding.\\nAccording to the conceptualist interpretation, one must understand the notion of a representations content as a relation to an object, which in turn depends on a conceptually guided synthesis. So we can revise our initial definition of conceptualism to read it as claiming (i) the content of\\xa0an intuition is a kind of relation to an object, (ii) the relation to an object depends on a synthesis directed in accordance with concepts, and (iii) synthesis in accordance with concepts sets correctness conditions for the intuitions representation of a mind-independent object.\\nAt the heart of non-conceptualist readings of Kant stands denial that mental acts of synthesis carried out by understanding are necessary for the occurrence of cognitive mental states of the type which Kant designates by the term intuition [Anschauung]. Though it is controversial as to what might be considered the natural or default reading of Kants mature critical philosophy, there are at least four considerations which lend strong support to a non-conceptualist interpretation of Kants mature work.\\nFirst, Kant repeatedly and forcefully states that in cognition there is a strict division of cognitive laborobjects are given by sensibility and thought via understanding:\\nObjects are given to us by means of sensibility, and it alone yields us intuitions; they are thought through the understanding, and from the understanding arise concepts (A19/B33; compare\\xa0A50/B74, A51/B756, A271/B327).\\nAs Robert Hanna has argued, when Kant discusses the dependence of intuition on conceptual judgment in the Analytic of Concepts, he specifically talks about cognition rather than what others would consider to be perceptual experience (Hanna (2005), 265-7).\\nSecond, Kant characterizes the representational capacities characteristic of sensibility as more primitive than those characteristic of understanding, or reason, and he characterizes those capacities as a plausible part of what humans share with the rest of the animal kingdom (Kant connects the possession of a faculty of sensibility to animal nature in various places, for example\\xa0A546/B574, A802/B830; An 7:196). For example, Kants distinction between the faculties of sensibility and understanding seems intended to capture the difference between the sub-rational powers of the mind that is shared with non-human animals and the rational or higher-level cognitive powers that are special to human beings. (Hanna (2005), 249; compare Allais (2009); McLear (2011))\\nIf one were to deny that, according to Kant, sensibility alone is capable of producing mental states cognitive in character, then, \\xa0it would seem that any animal which lacks a faculty of understanding would thereby lack any capacity for genuinely perceptual experience. The mental lives of non-rational animals would thus, at best, consist of non-cognitive sensory states causally correlated with changes in the animals environment. Aside from an unappealing and implausible characterization of the animals cognitive capacities, this reading also faces textual hurdles (for relevant discussion of some of the issues in contemporary cognitive ethology see Bermdez (2003); Lurz (2009); Andrews (2014), as well as the papers in Lurz (2011)). \\xa0Kant is on record in various places as saying that animals have sensory representations of their environment (CPJ 5:464; LM 28:449; compare\\xa0An 7:212), that they have intuitions (LL 24:702), and that they are acquainted with objects though they do not cognize them (JL 9:645) (see Naragon (1990); Allais (2009); McLear (2011)).\\nHence, if Kants position is that synthetic acts carried out by the understanding are necessary for the cognitive standing of a mental state, then Kant is contradicting fundamental elements of his own position in crediting intuitions or their possibility to non-rational animals.\\nThird, any position which regards perceptual experience as dependent upon acts of synthesis carried out by the understanding would presumably also construe the pure intuitions of space and time as dependent upon acts of synthesis (see Longuenesse (1998), ch.\\xa09; Griffith (2012)). However, Kants discussion of space, and, analogously, time, in the third and fourth arguments (fourth and fifth in the case of time) of the Metaphysical Exposition of Space in the Transcendental Aesthetic seems incompatible with such a proposed relation of dependence.\\nKants point in the third and fourth arguments of the Metaphysical Exposition of space and time is that no finite intellect could grasp the extent and nature of space as an infinite whole via a synthetic process involving movement from representation of a part to representation of the whole. If the unity of the forms of intuition were itself something dependent upon intellectual activity, then this unity would necessarily involve the discursive, though not necessarily conceptual, running through and gathering together of a given multiplicity (presumably of different locations or moments) into a combined whole. Kant believes this is characteristic of synthesis generally (A99).\\nBut Kants arguments in the Metaphysical Expositions require the fundamental basis of the representation of space and time does not proceed from a grasp of the multiplicative features of an intuited particular to the whole with those features. Instead, the form of pure intuition constitutes a representational whole that is prior to that of its component parts (compare\\xa0CJ 5:407-8, 409).\\nHence, Kants position is that the pure intuitions of space and time possess a unity wholly different from that given by the discursive unity of understanding (whether in conceptual judgment or the intellectual with imaginative synthesis of intuited objects). The unity of aesthetic representationcharacterized by forms of space and timehas a structure in which the representational parts depend upon the whole. The unity of discursive representationrepresentation where the activity of understanding is involvedhas a structure in which the representational whole depends upon its parts (see McLear (2015)).\\nFinally, there has been extensive discussion on the non-conceptuality of intuition in the secondary literature on Kants philosophy of mathematics. For example, Michael Friedman has argued that the expressive limitations of prevailing logic in Kants time required the postulation of intuition as a form of singular, non-conceptual representation (Friedman (1992), ch.\\xa02; Anderson (2005); Sutherland (2008)). In contrast to Friedmans view, Charles Parsons and Emily Carson argued that the immediacy of intuition, both pure and empirical, should be construed in a phenomenological manner. Space in particular is understood on their interpretation as an original, non-conceptual representation, which Kant takes to be necessary for the demonstration of the real possibility of constructed, mathematical objects as required for geometric knowledge (Parsons (1964); Parsons (1992); Carson (1997); Carson (1999); compare Hanna (2002). For a general overview of related issues in Kants philosophy of mathematics, see Shabel (2006) and the works cited therein at p.\\xa0107, note 29.)\\nUltimately, however, there are difficulties assessing whether Kants philosophy of mathematics can have relevance for the conceptualism debate. It is not obvious whether intuition must be non-conceptual in accounting for mathematical knowledge is incompatible with claiming that intuitions themselves are dependent upon a conceptually-guided synthesis.\\nThe non-conceptualist reading clearly commits to allowing that sensibility alone provides, perhaps in a very primitive manner, objective representation of the empirical world. Sensibility is construed as an independent cognitive faculty, which humans share with other non-rational animals, and which is the jumping-off point for more sophisticated, conceptual representation of empirical reality.\\nThe next and final section looks at Kants views regarding the nature and limits of self-knowledge and the ramifications of this for traditional rationalist views of the self.\\nKant discusses the nature and limits of our self-knowledge most extensively in the first Critique, in a section of the Transcendental Dialectic called the Paralogisms of Pure Reason. Here, Kant is concerned to criticize the claims of what he calls rational psychology. Specifically, he is concerned about the claim that we can have substantive, metaphysical knowledge of the nature of the subject, based purely on an analysis of the concept of the thinking self. As Kant typically puts it:\\nI think is thus the sole text of rational psychology, from which it is to develop its entire wisdombecause the least empirical predicate would corrupt the rational purity and independence of the science from all experience. (A343/B401)\\nThere are four Paralogisms. Each argument is presented as a syllogism, consisting of two premises and a conclusion. According to Kant, each argument is guilty of an equivocation on a term common to the premises, such that the argument is invalid. Kants aim, in his discussion of each Paralogism, is to diagnose the equivocation, and explain why the rational psychologists argument ultimately fails. In so doing, Kant provides a great deal of information about his own views concerning the mind (See Ameriks (2000) for extensive discussion). The argument of the first Paralogism concerns knowledge of the self as substance; the second, the simplicity of the self; the third, the numerical identity of the self; and the fourth, knowledge of the self versus knowledge of things in space.\\nKant presents the rationalists argument in the First Paralogism as follows:\\nKants presentation of the argument is rather compressed. In more explicit form we can put it as follows (see Proops (2010)):\\nThe relevant equivocation is in the term that occupies the M place in the argument entities that cannot be thought otherwise than as subjects. Kant specifically locates the ambiguity in the use of the term thought [Das Denken], which he claims concerns an object in general in the first premise. Thus, thought could be given in a possible intuition. In the second premise, the use of thought is supposed to apply only to a feature of thought and, thus, not to an object of a possible intuition (B411-12).\\nWhile it isnt obvious what Kant means by this claim, it could be that. Kant takes the first premise to make a claim about the objects of thought. They exist as an independent subject or bearer of properties and cannot be conceived of as anything else. This is thus a metaphysical claim about what kinds of objects could really exist, which explains Kants reference to an object in general that could be given in intuition.\\nIn contrast, premise (2) makes a merely logical claim concerning the role of the representation <I> in a possible judgment. Kant says one cannot use representation <I> in any place other than upon the subject. For example, while I can make the claim I am tall, I would make no sense to claim the tall is I.\\nAgainst the rational psychologist, Kant argues that one cannot make any legitimate inference from the conditions under which representation <I> may be thought, or employed in a judgment, to the status of the I as a metaphysical subject of properties. Kant makes this point explicit when he says,\\nThe first syllogism of transcendental psychology imposes on us an only allegedly new insight when it passes off the constant logical subject of thinking as the cognition of a real subject of inherence, with which we do not and cannot have the least acquaintance, because consciousness is the one single thing that makes all representations into thoughts, and in which, therefore, as in the transcendental subject, our perceptions must be encountered; and apart from this logical significance of the I, we have no acquaintance with the subject in itself that grounds this I as a substratum, just as it grounds all thoughts. (A350)\\nKant thus argues that one should differentiate between different conceptions of substance and the role they play in thoughts concerning the world.\\nSubstance0:\\nx is a substance0 if and only if the representation of x cannot be used as a predicate in a categorical judgment\\nSubstance1:\\nx is a substance1 if and only if its existence is such that it can never inhere, or exist, in anything else (B288, 407)\\nThe first conception of substance is merely logical or grammatical. The second conception is explicitly metaphysical. Finally, there is an even more metaphysically demanding usage of substance that Kant employs.\\n(Empirical) Substance2:\\nx is a substance2 if and only if it is a substance1 that persists at every moment (A144/B183, A182)\\nAccording to Kant, the rational psychologist attempts to move from claims about substance0 to the more robustly metaphysical claims characteristic of conceptions and uses of substance1 and substance2. However, without further substantive assumptions, which go beyond anything given in an analysis of the concept <I>, no legitimate inference can be made from our notion of a substance0 to either of the other conceptions of substance.\\nBecause, Kant denies that humans have any intuition, empirical or otherwise, of themselves as subjects, they cannot\\xa0 come to have any knowledge concerning what we are in terms of beings either substance1 or substance2. At least they cannot do so by reflecting on the conditions of thinking of themselves using first-person concepts. No amount of introspection or reflection on the content of the first-person concept <I> will yield such knowledge.\\nKants discussion of the proposed metaphysical simplicity of the subject largely depends on points he made in the previous Paralogism concerning its proposed substantiality. Kant articulates the Second Paralogism as follows:\\nHere, the equivocation concerns the notion of a subject. Kants point, as with the previous Paralogism, is that, from the fact that ones first-person representation of the self is always a grammatical or logical subject, nothing follows concerning the metaphysical status of that representations referent.\\nOf perhaps greater interest in this discussion of the Paralogism of simplicity is Kants analysis of what he calls the Achilles of all dialectical inferences (A351). According to the Achilles argument, the soul or mind is known to be a simple, unitary substance, because only such a substance could think unitary thoughts. Called the unity claim (see Brook (1997)), it says:\\n(UC):\\nIf a multiplicity of representations are to form a single representation, they must be contained in the absolute unity of the thinking substance. (A352)\\nAgainst UC, Kant argues that there is no reason to think the structure of a thought, as a complex of representations, isnt mirrored in the complex structure of an entity that thinks thoughts. UC is not analytic, which is to say that there is no contradiction entailed by its negation. UC also fails to be a synthetic a priori claim, in that it follows neither from the nature of intuitions forms, nor from categories. Hence, UC could only be shown to be true empirically, and because people have no empirical intuition of the self, people have no basis for thinking that UC must be true (A353).\\nKant here makes a point similar to contemporary, functionalist accounts of the mind (see Meerbote (1991); Brook (1997)). Mental functions, including the unity of conscious thought, are consistent with a variety of different media in which functions are realized. Kants says there is no contradiction in thinking that a plurality of substances might succeed in generating a single, unified thought. Hence, we cannot know that the mind is such that it must be simple in nature.\\nKant articulates the Third Paralogism as follows:\\nRational psychologists interest in establishing the personality of the soul or mind stems from the importance of proving that not only would the mind persist after the destruction of its body, but also that this mind would be the same person as before, not just some sort of bare consciousness or worse (for example, existing only as a bare monad).\\nKant here makes two main points. First, the rational psychologist cannot infer from the sameness of the first-person representation (the I think) or across applications of it in judgment to any conclusion concerning the sameness of the metaphysical subject referred to by that representation. Kant thus again makes a functionalist point. The medium in which a series of representational states inheres may change over time, and there is no contradiction in conceiving of a series of representations as being transferred from one substance to another (A363-4, note).\\nSecond, Kant argues that we can be confident of the souls possession of personality by \\xa0virtue of apperceptions persistence. The relevant notion of personality here distinguishes between a rational being and an animal. While the persistence of apperception (the persistence of the I think as being able to attach to all of ones representations) does not provide an apperceiving subject with any insight into the true metaphysical nature of the mind, it does provide evidence of the souls possession of an understanding. Animals, by contrast, do not possess an understanding but, at best, according to Kant, only an analogue thereof. As Kant says in the Anthropology,@\\nThat man can have the I among his representations elevates him infinitely above all other living beings on earth. He is thereby a person [] that is, by rank and worth a completely distinct being from things that are the same as reason-less animals with which one can do as one pleases. (An 7:127, 1)\\nHence, so long as a soul possesses the capacity for apperception, it will signal the possession of an understanding, and thus serves to distinguish the human soul from that of an animal (see Dyck (2010), 120).\\nFinally, the Fourth Paralogism concerns the relation between awareness of ones own mind and ones awareness of other objects distinct from oneself. Thus, it also deals with ones mind and awareness of space. Kant describes the Fourth Paralogism as follows:\\nKant locates the damaging ambiguity in the conception of outer objects. This is puzzling because it doesnt play the relevant role as middle term in the syllogism. But Kant is quite clear that this is where the ambiguity lies and distinguishes between two distinct senses of the outer or external:\\nTrancendentally Outer/External:\\nA seperate existence, in and of itself.\\nEmpirically Outer/External:\\nAn existence in space.\\nKants point here is that all appearances in space are empirically external to the subject who perceives or thinks about them, while nevertheless being transcendentally internal. Such spatial appearances do not have an entirely independent metaphysical nature, because their spatial features depend at least in part on our forms of intuition.\\nKant then uses this distinction not only to argue against the assumption of the rational psychologist that the mind is better known than any object in space (famously argued by Descartes), but also against those forms of external world skepticism championed by Descartes and Berkeley. Kant identifies Berkeley with what he calls dogmatic idealism and Descartes with what he calls problematic idealism (A377). He defines them thus:\\nProblematic Idealism:\\nWe cannot be certain of the existence of any material body.\\nDogmatic Idealism:\\nWe can be certain that no material body exists  the notion of a body is self-contradictory.\\nKant brings two arguments to bear against the rational psychologists assumption about the immediacy of our self-knowledge, as well as these two forms of skepticism, with mixed results. The two arguments are from immediacy and imagination.\\nIn an extended passage in the Fourth Paralogism (A370-1) Kant makes the following argument:\\nExternal objects (bodies) are merely appearances, hence also nothing other than a species of my representations, whose objects are something only through these representations, but are nothing separated from them. Thus external things exist as well as my self, and indeed both exist on the immediate testimony of my self-consciousness, only with this difference: the representation of my Self, as the thinking subject, is related merely to inner sense, but the representations that designate extended beings are also related to outer sense. I am no more necessitated to draw inferences in respect of the reality of external objects than I am in regard to the reality of the objects of my inner sense (my thoughts), for in both cases they are nothing but representations, the immediate perception (consciousness) of which is at the same time a sufficient proof of their reality. (A370-1)\\nIt helps to understand the argument as follows:\\nHere, Kant displays what he takes to be an advantage of Transcendental Idealism. Because both inner and outer sense depend on intuition, there is nothing special about inner intuition that privileges it over outer intuition. Both are, as intuitions, immediate presentations of objects, at least as they appear. Unfortunately, Kant never makes clear what he means by the term immediate [unmittelbar]. This issue is much contested (see Smit (2000)). At the very least, he means to signal that awareness in intuition is not mediated by any explicit or conscious inference, as when he says that the transcendental idealist grants to matter, as appearance, a reality which need not be inferred, but is immediately perceived (A371).\\nIt is not obvious that an external world skeptic would find this argument convincing, as part of the grip of such skepticism relies on the convincing point that things could seem to one just as they currently are, even if there really is no external world causing ones experiences. This may just beg the question against Kant (particularly premise (2) of the above argument). Certainly, Kant seems to think that his arguments for the existence of pure intuitions of space and time in the Transcendental Aesthetic lend some weight to his position. Thus, Kant is not so much arguing for Transcendental Idealism here as explaining some of the further benefits that come when the position is adopted. He does, however, present at least one further argument against the skeptical objection articulated abovethe argument from imagination.\\nKants attempt to respond to the skeptical worry that things might appear to be outside us while not actually existing outside us appeals to the role imagination would have to play to make such a possibility plausible (A373-4; compare Anthropology, 7:167-8).\\nThis material or real entity, however, this Something that is to be intuited in space, necessarily presupposes perception, and it cannot be invented by any power of imagination or produced independently of perception, which indicates the reality of something in space. Thus sensation is that which designates a reality in space and time, according to whether it is related to the one or the other mode of sensible intuition.\\nWhat follows is a reconstruction of this argument.\\nKants idea here is that the imagination is too limited to generate the various qualities that people experience as instantiated in external physical objects. Hence, it would not be possible to simply imagine an external physical world without having been originally exposed to the qualities instantiated in the physical world. Ergo, the physical world must exist. Even Descartes seems to agree with this, noting in Meditation I that [certain simple kinds of qualities] are as it were the real colours from which we form all the images of things, whether true or false, that occur in our thought (Descartes (1984), 13-14). Though Descartes goes on to doubt our capacity to know even such basic qualities given the possible existence of an evil deceiver, it is notable that the deceiver must be something other than ourselves, in order to account for all the richness and variety of what we experience (however, see Meditation VI (Descartes (1984), 54), where Descartes wonders whether there could be some hidden faculty in ourselves producing all of our ideas).\\nUnfortunately, it isnt clear that the argument from imagination gets Kant the conclusion he wants, for all that it shows is that there was at one time a physical world, which affected ones senses and provided the material for ones sense experiences. This might be enough to show that one has not always been radically deceived, but it is not enough to show that one is not currently being radically deceived. Even worse, it isnt even clear that a physical world must exist to generate the requisite material for the imagination. Perhaps all that is needed is something distinct from the subject, something which is capable of generating in it the requisite sensory experiences, whether or not they are veridical. This conclusion is thus compatible with that something being Descartess evil demon, or in contemporary epistemology, with the subjects being a brain in a vat. Hence, it is not obvious that Kants argument succeeds in refuting the skeptic. To the extent that he did refute the skeptic, it still does not show that there is a physical world, as opposed merely to the existence of something distinct from the subject.\\nBeyond the specific arguments of the Paralogisms and their conclusions, they present us with two central tenets of Kants conception of the mind. First, we cannot move from claims concerning the character or role of the first-person representation <I> to claims concerning the nature of the referent of that representation. This is a key part of his criticism of rational psychology. Second, people do not have privileged access to themselves as compared with things outside them. Both the self (or its states) and external objects are on par with respect to intuition. This also means that they only have access to themselves as they appear, and not as they fundamentally, metaphysically, are (compare\\xa0B157). Hence, according to Kant, self-awareness, just as much as awareness of anything distinct from the self, is conditioned by sensibility. Intellectual access to selves in apperception, Kant argues, does not reveal anything about ones metaphysical nature, in the sense of the kind of thing that must exist to realize the various cognitive powers that Kant describes as characteristic of a being capable of apperceptiona spontaneous understanding or intellect.\\nKants conception of the mind, his distinction between sensory and intellectual faculties, his functionalism, his conception of mental content, and his work on the nature of the subject/object distinction, were all hugely influential. His work immediately inspired the German Idealist movement. He also became central to emerging ideas concerning the epistemology of science in the late 19th and early 20th centuries, in what became known as the Neo-Kantian movement in central and southern Germany. Though Anglophone interest in Kant ebbed somewhat in the early 20th century, his conception of the mind and criticisms of rationalist psychology were again influential mid-century via the work of analytic Kantians such as P.F. Strawson, Jonathan Bennett, and Wilfrid Sellars. In the early 21st century Kants work on the mind remains a touchstone for philosophical investigation, especially in the work of those influenced by Strawson or Sellars, such as Quassim Cassam, John McDowell, and Christopher Peacocke.\\nQuotations from Kants work are from the German edition of Kants works, the Akademie Ausgabe, with the first Critique cited by the standard A/B edition pagination, and the other works by volume and page. English translations belong to the author of this article article, though he has regularly consulted, and in most cases closely followed, translations from the Cambridge Editions. Specific texts are abbreviated as follows:\\nThe most used scholarly English translations of Kants work are published by Cambridge University Press as the Cambridge Editions of the Works of Immanuel Kant. The following are from that collection and contain some of Kants most important and influential writings.\\nColin McLear\\nEmail: mclear@unl.edu\\nUniversity of Nebraska\\nU. S. A.\\n\\n### In what seems to be a musical \\nApr 8, 2024\\xa0 Contributing to the vivid theater experience are the stage sets and props, the lighting, the background music and sound effects, the costumes and accessories. In recent \\nWhich of the following African American musical forms aided and influenced Broadway musical theatre in the beginning of the 20th century? Multiple choice question.\\nApr 4, 2024\\xa0 The first and most important step in creating a theatrical character is coming up with the idea. The images further development and its on-stage manifestation are determined \\nExploring the intricate world of character development in musical theatre, this instructional resource delves into how actors can effectively interpret and'}\n",
      " Fact-checking: I contain all knowledge....\n",
      "{'start': 140.52, 'end': 144.32, 'speaker': 'Janet', 'text': 'And I use the passing of time as a lotion, like a god.', 'reason': 'This is a bold statement suggesting a god-like manipulation of time.', 'context': 'In the same performance setting, Janet makes another grandiose claim, continuing the theme of extraordinary abilities.', 'frame_path': 'statement_frames\\\\Janet_014052.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.2220134735107422, 'article_texts': '### And I use the passing of time \\nPassing Time by Maya Angelouis a short, but powerful poem that speaks on the differences, or lack thereof, between skin tones. The poet compares light and dark skin tones to the rising and setting of the su See more\\nPassing Time by Maya Angelou is a six-line poem that is separated into couplets or sets of two lines. These lines do not follow a specific rhyme schemeor metrical pattern. They vary i See more\\nGender\\nFemale\\nNationality\\nAmerican\\nAngelou makes use of several poetic techniques in Passing Time. These include but are not limited to alliteration, enjambment, metaphor, and personificatio See more\\nMaya Angelou\\'s \"Passing Time\" is a succinct yet intricate poem that explores the duality of beginnings and endings through the contrasting imagery of dawn and musk.\\nExplore the meaning of Maya Angelou\\'s Passing Time with our analysis of the poem, offering a detailed breakdown of its themes of aging and memory.\\nIn the poem, Angelou discusses underlying themes of racial differences  and similarities. She uses synesthesia, repeated language, and an extended metaphor to creatively portray skin \\nBorn on April 4 in St. Louis MO.\\nMoved to live with grandmother in Stamps AR.\\nMarried Tosh Angelos, Greek electrician.\\nDanced professionally in San Francisco clubs.\\n\\n### Explanation of PASSING TIME by MAYA ANGELOU - Poetry \\nMaya Angelou\\'s \"Passing Time\" is a succinct yet intricate poem that explores the duality of beginnings and endings through the contrasting imagery of dawn and musk. Though the poem consists of only eight lines, its brevity belies its depth, encapsulating a complex meditation on love, time, and the nature of human relationships.\\nThe opening lines, \"Your skin like dawn / Mine like musk,\" immediately establish a contrast between two entities, both sensory and metaphorical. \"Dawn\" and \"musk\" not only evoke specific colors and scents but also contain temporal implications-dawn representing the start of the day, and musk evoking a sense of aging or the end of the day. In the likening of skin to these elements, the poem subtly delves into the complexities of a relationship, suggesting a bittersweet interplay between new beginnings and inevitable endings.\\nThe lines, \"One paints the beginning / of a certain end,\" adds a layer of paradox to the imagery of dawn. While dawn is universally symbolic of a fresh start or new day, here it is described as the \"beginning of a certain end.\" This clever twist points to the transience inherent in any beginning; the start of something new invariably sets the stage for a future ending, emphasizing the cyclical nature of life and relationships.\\nConversely, the following lines, \"The other, the end of a / sure beginning,\" use the musk imagery to describe something that seems to be concluding but is, in fact, assured of its origins-a \"sure beginning.\" This phrase suggests a type of closure that is rooted in certainty, perhaps implying that every ending also signifies the onset of something new. If dawn heralds an inevitable dusk, then musk equally assures the arrival of another dawn.\\nAt its core, \"Passing Time\" captures the transient yet cyclical nature of time, love, and human experience. The skin, a vital yet constantly changing organ, serves as a perfect metaphor for the transient nature of human relationships and life itself. In evoking elements like dawn and musk, Angelou deftly encapsulates the universal, ever-oscillating experience of beginnings and endings that shape our lives and relationships.\\nThough brief, \"Passing Time\" leaves an indelible impact, inviting the reader to ponder the complexities of time, existence, and the human condition. Angelou\\'s ability to convey such profound meaning in so few words underscores her mastery as a poet, capturing the nuances of life\\'s transitory nature with stunning clarity and emotional resonance.\\n\\n### In the same performance settin\\nIn the process of performance planning, subordinates write performance plans and review with leaders. Identify a practical tip for developing high-performance workplaces. All of the answers \\nNon-bottlenecks cannot determine their own utilizationanother constraint in the system must do that. Bob protests against slowing down non-bottlenecks to match bottlenecks, claiming this \\nFeb 25, 2022\\xa0 In grandiose narcissism, two dimensions (narcissistic rivalry and narcissistic admiration) are recognized corresponding to self-protecting and self-enhancing regulatory \\nThis year, the new team manager for Bravo believes that the team might make a third-place finish if he sets difficult, challenging goals for them. This is an example of which of the following \\nDeon and Janet, who do not know each other, are assigned to work on a project together at work. Deon suggests that the two have a lunch meeting together to get to know one another before \\n\\n### Chapter 11 - Motivating and Rewarding Employees - Quizlet\\n 2025 Quizlet, Inc.'}\n",
      " Fact-checking: And I use the passing of time as a lotion, like a god....\n",
      " All statements are supported or unverified.\n",
      " Finished processing video ../data/dfw_youtube_release\\4.mp4\n",
      "Deepfake label: REAL, Fact-check label: True\n",
      " Processing video 6: ../data/dfw_youtube_release\\5.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\5.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Walter White\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Rick Grimes\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-7.00] Unknown: This is gonna get bad, I'm about to show this lab brand.\n",
      "[7.00-9.00] Walter White: How to be a real bad grade AMC.\n",
      "[9.00-13.00] Walter White: We're moving the A one day, cause Sheriff Brown's rhymes dirty like my armpits.\n",
      "[13.00-14.00] Rick Grimes: Stay.\n",
      "[14.00-17.00] Rick Grimes: I'm a post-apocalyptic cop who's got a lot of issues.\n",
      "[17.00-20.00] Rick Grimes: Pop a cap in you and splatter the brain you miss you.\n",
      "[20.00-22.00] Rick Grimes: Cooking up blue sky and fire.\n",
      "[22.00-23.00] Rick Grimes: I'm a real bad guy.\n",
      "[23.00-24.00] Rick Grimes: I'm a real bad guy.\n",
      "[24.00-25.00] Rick Grimes: I'm a real bad guy.\n",
      "[25.00-26.00] Rick Grimes: I'm a real bad guy.\n",
      "[26.00-27.00] Rick Grimes: I'm a real bad guy.\n",
      "[27.00-28.00] Rick Grimes: I'm a real bad guy.\n",
      "[28.00-29.00] Rick Grimes: I'm a real bad guy.\n",
      "[29.00-31.00] Rick Grimes: I'm a blue sky and figure lies for Skylar.\n",
      "[31.00-34.00] Rick Grimes: Hatching little schemes like a dying Maghiver.\n",
      "[34.00-35.00] Rick Grimes: You tore your family apart.\n",
      "[35.00-37.00] Rick Grimes: Sin by sin where I live.\n",
      "[37.00-39.00] Rick Grimes: It happens literally in per length.\n",
      "[39.00-41.00] Rick Grimes: So write this down in your pancakes.\n",
      "[41.00-42.00] Rick Grimes: You won't forget it.\n",
      "[42.00-45.00] Rick Grimes: I'll kill zombies that are better men than you before breakfast.\n",
      "[45.00-48.00] Rick Grimes: I don't know what you think I've done.\n",
      "[48.00-50.00] Walter White: But if we were to battle, I've already won.\n",
      "[50.00-51.00] Walter White: I ask us.\n",
      "[51.00-53.00] Walter White: You don't want to face off against me.\n",
      "[53.00-56.00] Walter White: I'll stop you in a barrel and make it too smooth.\n",
      "[56.00-59.00] Unknown: Your sense of duty gets your group into some deep-stuty.\n",
      "[59.00-61.00] Walter White: Always getting saved by some samurai.\n",
      "[61.00-62.00] Walter White: I'm a kingpin.\n",
      "[62.00-64.00] Walter White: You can crystal in the middle of the day.\n",
      "[64.00-67.00] Walter White: Happened in her by the pool with the DEA.\n",
      "[67.00-69.00] Walter White: Where you're over with my eyes, tech.\n",
      "[69.00-70.00] Walter White: KTAA.\n",
      "[70.00-73.00] Walter White: If you ever try to stop, highs and burglary and pain.\n",
      "[73.00-74.00] Walter White: Here's a hot toast.\n",
      "[74.00-76.00] Walter White: Let me watch you choke on the truth.\n",
      "[76.00-79.00] Walter White: You look up to me like I'm a pizza on the road.\n",
      "[79.00-80.00] Walter White: No, you're a loser.\n",
      "[80.00-81.00] Walter White: I'll failure to your whole entire crew.\n",
      "[81.00-84.00] Walter White: I've seen Walter Jr. handle walkers better than you.\n",
      "[85.00-89.00] Rick Grimes: I said stay back with the others while I finish this bitch.\n",
      "[89.00-90.00] Rick Grimes: Like you finish your month.\n",
      "[90.00-92.00] Rick Grimes: You ain't the danger to me, Walt.\n",
      "[92.00-93.00] Rick Grimes: So now I call you want.\n",
      "[93.00-96.00] Rick Grimes: I'll watch you get eaten on my fucking front lawn.\n",
      "[96.00-99.00] Rick Grimes: Your monster's so frightened and you can bite me.\n",
      "[99.00-102.00] Walter White: I'll be standing right here in my tiny Walter Whiteton.\n",
      "[102.00-105.00] Walter White: I'll bury you faster than your partner so your whole life.\n",
      "[105.00-107.00] Walter White: I went saw Shane coming except for your wife.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\5\\clip_1.mp4 from 61.0s to 64.0s\n",
      " Cutting statement_clips\\5\\clip_2.mp4 from 102.0s to 105.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\5\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['cc927ece8df24e51ad1bdf3af76dd5e4_clip_1.mp4'], 'pred': [0.065349280834198], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['50dfe61fb66a48719ef5ebb473c8ffd8_clip_2.mp4'], 'pred': [0.5369266867637634], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 61.0, 'end': 64.0, 'speaker': 'Walter White', 'text': 'You can crystal in the middle of the day. Happened in her by the pool with the DEA.', 'reason': 'References possible illegal activities involving the DEA.', 'context': 'This line is from a fictional engagement between characters from \"Breaking Bad\" and \"The Walking Dead.\" Walter White is referencing his drug activities, suggesting a confrontation with DEA agents. This could be checked against events in the \"Breaking Bad\" series.', 'frame_path': 'statement_frames\\\\Walter White_006100.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.065349280834198}, {'start': 102.0, 'end': 105.0, 'speaker': 'Walter White', 'text': \"I'll bury you faster than your partner so your whole life. I went saw Shane coming except for your wife.\", 'reason': 'Mentions events related to character developments and conflicts in \"The Walking Dead.\"', 'context': 'Walter White addresses Rick Grimes, another fictional character, alluding to the death of Shane, a character in \"The Walking Dead.\" This can be analyzed regarding its accuracy to the original TV series.', 'frame_path': 'statement_frames\\\\Walter White_010200.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.5369266867637634}]\n",
      " Statement I'll bury you faster than your partner so your whole life. I went saw Shane coming except for your wife. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\5.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 7: ../data/dfw_youtube_release\\6.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\6.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Dr DisRespect\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.00] Unknown: FILE ITS\n",
      "[2.00-4.00] SPEAKER_02: SPEED\n",
      "[4.00-5.00] SPEAKER_02: B\n",
      "[5.00-6.00] SPEAKER_02: HEHEHE\n",
      "[6.00-7.00] Unknown: MMMM\n",
      "[7.00-9.00] Unknown: MMMM\n",
      "[9.00-10.00] SPEAKER_02: MMMM\n",
      "[10.00-11.00] SPEAKER_02: MMMM\n",
      "[11.00-12.00] SPEAKER_02: MMMM\n",
      "[12.00-13.00] SPEAKER_02: MMMM\n",
      "[13.00-15.00] Unknown: MMMM\n",
      "[15.00-17.00] Unknown: MMMM\n",
      "[17.00-21.00] Dr DisRespect: The two-time back-to-back 1991 is 1992 Blackbuster Champion!\n",
      "[21.00-23.00] Dr DisRespect: It's not even close!\n",
      "[23.00-25.00] Unknown: It's not even close.\n",
      "[25.00-27.00] Unknown: MMMM\n",
      "[27.00-31.00] Unknown: MMMM\n",
      "[31.00-34.00] Unknown: MMMM\n",
      "[34.00-54.00] Unknown: A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\6\\clip_1.mp4 from 17.0s to 21.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\6\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['5673de6a76c04209a4926bb60fc0d99a_clip_1.mp4'], 'pred': [0.7260165810585022], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 17.0, 'end': 21.0, 'speaker': 'Dr DisRespect', 'text': 'The two-time back-to-back 1991 is 1992 Blackbuster Champion!', 'reason': 'Claim about winning specific championships in consecutive years.', 'context': 'In a video stream or gaming commentary, the speaker makes a self-promotional statement likely meant to highlight past achievements.', 'frame_path': 'statement_frames\\\\Dr DisRespect_001700.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.7260165810585022}]\n",
      " Statement The two-time back-to-back 1991 is 1992 Blackbuster Champion! is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\6.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 8: ../data/dfw_youtube_release\\7.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.00] Unknown: Here we go!\n",
      "[2.00-4.00] Unknown: Oh no.\n",
      "[4.00-6.00] SPEAKER_01: Sing it long with me boys.\n",
      "[6.00-8.00] Unknown: I'm almost heaven.\n",
      "[8.00-10.00] SPEAKER_01: West Virginia.\n",
      "[10.00-12.00] SPEAKER_01: Okay.\n",
      "[12.00-14.00] SPEAKER_01: Blue Ridge Mountains,\n",
      "[14.00-18.00] SPEAKER_01: Shining to River.\n",
      "[18.00-20.00] Unknown: Here we go!\n",
      "[20.00-22.00] SPEAKER_01: Life is all that.\n",
      "[22.00-24.00] SPEAKER_01: Older than the trees.\n",
      "[24.00-26.00] SPEAKER_01: They should have made the wreck.\n",
      "[26.00-28.00] SPEAKER_01: They should have made him T-Foo.\n",
      "[28.00-30.00] Unknown: He's just- he's put his face up for a second.\n",
      "[30.00-32.00] SPEAKER_01: Come on.\n",
      "[32.00-38.00] SPEAKER_01: Take it all to the place.\n",
      "[38.00-40.00] SPEAKER_01: I have to go.\n",
      "[40.00-42.00] Unknown: Ah!\n",
      "[42.00-46.00] Unknown: This- this is a man.\n",
      "[46.00-50.00] Unknown: I had some muscular man.\n",
      "[50.00-52.00] Unknown: A muscular?\n",
      "[52.00-54.00] SPEAKER_02: Who are you kidding?\n",
      "[54.00-56.00] SPEAKER_02: Well, you won sandwich away from fat.\n",
      "[56.00-58.00] SPEAKER_02: What?\n",
      "[58.00-60.00] Unknown: You're gonna wake up all for me.\n",
      "[60.00-62.00] SPEAKER_01: Come on.\n",
      "[62.00-64.00] SPEAKER_01: I'm gonna get a diet.\n",
      "[64.00-66.00] Unknown: I'll start tomorrow.\n",
      "[66.00-68.00] SPEAKER_01: I'm gonna get some dumbbells.\n",
      "[68.00-70.00] Unknown: You know you can't eat dumbbells, right?\n",
      "[70.00-72.00] Unknown: Wake?\n",
      "[72.00-74.00] Unknown: No!\n",
      "[74.00-80.00] Unknown: What's up, dude?\n",
      "[80.00-84.00] Unknown: What the fuck is going on?\n",
      "[84.00-86.00] Unknown: I was hacking computer.\n",
      "[86.00-88.00] SPEAKER_02: Seriously.\n",
      "[88.00-90.00] Unknown: What are you doing?\n",
      "[90.00-94.00] Unknown: Hey, listen.\n",
      "[94.00-96.00] SPEAKER_00: I'm a B-Real.\n",
      "[96.00-100.00] SPEAKER_00: You like my mother-fucking family, brother?\n",
      "[100.00-102.00] SPEAKER_00: Well, are you making your voice deeper?\n",
      "[102.00-104.00] Unknown: Oh, man.\n",
      "[104.00-106.00] SPEAKER_00: Shut your bitch ass up.\n",
      "[106.00-108.00] SPEAKER_02: Get out of my fucking way.\n",
      "[108.00-110.00] SPEAKER_02: Get out of my fucking way.\n",
      "[110.00-112.00] SPEAKER_02: I gotta figure it out.\n",
      "[112.00-114.00] SPEAKER_02: An allergic zonmin' of morons.\n",
      "[114.00-116.00] Unknown: So me and group will go with the pirate angel here.\n",
      "[116.00-120.00] SPEAKER_02: And the morons will go to nowhere to try to stop dumbbells.\n",
      "[120.00-122.00] Unknown: Did you fare well in good luck, losers?\n",
      "[122.00-124.00] SPEAKER_00: Bye.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\7\\clip_1.mp4 from 84.0s to 86.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\7\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['5c0445cf02fa47c2a08ada0d85c35108_clip_1.mp4'], 'pred': [0.5], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 84.0, 'end': 86.0, 'speaker': 'Unknown', 'text': 'I was hacking computer.', 'reason': 'The statement suggests an illegal activity that is checkworthy for legal and ethical reasons.', 'context': 'This statement occurs in a conversation with casual and possibly humorous context. The setting appears informal or fictional without additional context on the legality or seriousness of the action described.', 'frame_path': 'statement_frames\\\\Unknown_008400.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.5}]\n",
      " Statement I was hacking computer. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\7.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 9: ../data/dfw_youtube_release\\8.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\8.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-7.00] Unknown: I'm Anita Horgoth.\n",
      "[7.00-13.68] Uncle Wart: Fuck Malanoma, Molly Russell's wart.\n",
      "[13.68-16.40] Uncle Wart: Not her wart, not her wart.\n",
      "[16.40-17.92] Uncle Wart: I'm the wart.\n",
      "[17.92-25.28] Uncle Wart: She's my tumor, my growth, my pimple, I'm Uncle Wart, just old buck wart Russell.\n",
      "[25.28-26.28] Uncle Wart: That's what they call me.\n",
      "[26.28-29.96] Uncle Wart: Or Malanoma Head, they'll call me that.\n",
      "[29.96-32.48] Uncle Wart: Malanoma Head's coming.\n",
      "[32.48-33.80] Unknown: I'm sorry, Uncle.\n",
      "[33.80-35.40] Uncle Wart: Maisie Russell's Uncle.\n",
      "[35.40-37.76] Uncle Wart: I'm our Uncle.\n",
      "[37.76-41.52] Uncle Wart: Her mother set up this conference with you.\n",
      "[41.52-51.08] Assistant Principal: I'm assistant principal here, as you've probably noticed from the indications of the door.\n",
      "[51.08-52.08] Uncle Wart: This door?\n",
      "[52.08-53.08] Unknown: You mean the outer door?\n",
      "[53.08-54.08] Assistant Principal: The outer door.\n",
      "[54.08-55.08] Uncle Wart: Yes.\n",
      "[55.08-56.52] Uncle Wart: Come on, this one.\n",
      "[56.52-59.32] Assistant Principal: That's just about enough of that.\n",
      "[59.32-62.08] Unknown: Sorry.\n",
      "[62.08-71.84] Unknown: I've been an educator for 31.3 years and in that time I've seen a lot of bad eggs.\n",
      "[71.84-82.12] Assistant Principal: I say eggs because at the elementary level we are not dealing with fully developed individuals.\n",
      "[82.12-87.96] Assistant Principal: I see a bad egg when I look at your niece.\n",
      "[87.96-92.60] Assistant Principal: She is a twidler, a dreamer, a silly heart.\n",
      "[92.60-94.88] Assistant Principal: She is a jababox.\n",
      "[94.88-106.60] Assistant Principal: And frankly, I don't think she takes a thing in her life or her career as a student seriously.\n",
      "[106.60-108.32] Assistant Principal: She's only sex.\n",
      "[108.32-109.92] SPEAKER_00: That is not a valid excuse.\n",
      "[109.92-113.28] SPEAKER_00: But I hear that every day and I dismiss it.\n",
      "[113.28-117.20] Uncle Wart: I don't think I want to know a six-year-old who is a dreamer or a silly heart.\n",
      "[117.20-121.48] Uncle Wart: And I sure don't want to know one who takes their student career seriously.\n",
      "[121.48-124.16] Uncle Wart: I don't have a college degree.\n",
      "[124.16-126.96] Unknown: I don't even have a job.\n",
      "[126.96-129.56] Uncle Wart: But I know a good kid when I see one.\n",
      "[129.56-131.36] Uncle Wart: Because they're all good kids.\n",
      "[131.36-134.72] Uncle Wart: Until dried out, brain dead, sags like you.\n",
      "[134.72-137.52] Uncle Wart: Drag them down and convince them they're no good.\n",
      "[137.52-142.08] Uncle Wart: You so much a scowl at my niece or any other kid in this school and I hear about it and\n",
      "[142.08-146.72] Uncle Wart: I'm coming looking for you.\n",
      "[146.72-149.04] Uncle Wart: Take this quarter.\n",
      "[149.04-151.16] Uncle Wart: Go downtown and have a rep.\n",
      "[151.16-152.56] Uncle Wart: You got that thing off your face.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\8\\clip_1.mp4 from 129.56s to 137.52s\n",
      " Finished cutting video into clips. Saved to statement_clips\\8\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['2b4e621f34994d2088988494903c5318_clip_1.mp4'], 'pred': [0.9408334493637085], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 129.56, 'end': 137.52, 'speaker': 'Uncle Wart', 'text': \"But I know a good kid when I see one. Because they're all good kids. Until dried out, brain dead, sags like you. Drag them down and convince them they're no good.\", 'reason': 'This statement blames teachers or authority figures for negatively influencing children and their self-worth, suggesting a cause-effect relationship.', 'context': 'The quote occurs in what seems to be a fictional or comedic setting, possibly a school meeting during a film or play. The discussion involves a confrontation between a character named Uncle Wart and an assistant principal about the characters niece, Maisie, and her attitude towards school.', 'frame_path': 'statement_frames\\\\Uncle Wart_012956.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9408334493637085}]\n",
      " Statement But I know a good kid when I see one. Because they're all good kids. Until dried out, brain dead, sags like you. Drag them down and convince them they're no good. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\8.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 10: ../data/dfw_youtube_release\\9.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\9.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.36-17.04] Unknown: \n",
      "[17.04-19.42] Unknown: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\9\\clip_1.mp4 from 0.36s to 17.04s\n",
      " Cutting statement_clips\\9\\clip_2.mp4 from 17.04s to 19.42s\n",
      " Finished cutting video into clips. Saved to statement_clips\\9\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['6cb8208a01ec498081335b3c44331f40_clip_1.mp4'], 'pred': [0.9563100934028625], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['24311d55083f4e2a98f66043c8138023_clip_2.mp4'], 'pred': [0.8681545853614807], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 0.36, 'end': 17.04, 'speaker': 'Unknown', 'text': '', 'reason': 'There is no factual claim, statistical data, or bold statement made in the available segment.', 'context': 'The content is extremely limited with no identifiable context, date, or topic discussion. The language suggests it might be a non-English speech or casual phrase, making it impossible to evaluate for checkworthiness.', 'frame_path': 'statement_frames\\\\Unknown_000036.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9563100934028625}, {'start': 17.04, 'end': 19.42, 'speaker': 'Unknown', 'text': '', 'reason': 'Similar to the previous segment, there is no identifiable claim or factual basis in this short segment.', 'context': \"Lacking any additional context or discussion topic, it's uncertain where or when this statement was made. Similar issues of language and briefness prevent proper analysis.\", 'frame_path': 'statement_frames\\\\Unknown_001704.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.8681545853614807}]\n",
      " Statement  is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\9.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Saved checkpoint after 10 videos\n",
      " Processing video 11: ../data/dfw_youtube_release\\10.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\10.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Colonel Nathan R. Jessep\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-12.00] Unknown: Greg grandfather gave this watch to Granddad for good luck. Unfortunately, Dane's luck wasn't as good as his old man's, Dane was a marine and he was killed.\n",
      "[12.00-21.00] Unknown: Along with all the other marines at the Battle of Wake Island. Granddad was facing death. He knew it.\n",
      "[21.00-29.00] Unknown: No one's boys have any illusions, but they'll leave in that island alive. So three days before the Japanese took the island,\n",
      "[29.00-37.00] Colonel Nathan R. Jessep: Granddad asked a gunner on an Air Force transport, Nimo. Warnaki. Man in him met before in his life.\n",
      "[37.00-45.00] Unknown: To deliver to his infant son, we'd never seen a flesh. He's gone to watch.\n",
      "[45.00-58.00] Unknown: Three days later, your Granddad was dead, but Warnaki kept his word. After the war was over, he paid a visit to your grandmother, delivering to your infant father his dad's gold watch.\n",
      "[58.00-60.00] Unknown: This watch.\n",
      "[60.00-82.00] Unknown: This watch saw your dad's rest when he was shot down on a hand-away. He was captured by Vietnamese prison camp. He knew that if the gooks have a saw on the watch, he'd be confiscated, taken away.\n",
      "[82.00-91.00] Unknown: The way your dad looked at it, this watch was your birthright. If your dad had a lot of money, he'd put a greasy yellow hands on his boy's birthright.\n",
      "[91.00-100.00] Colonel Nathan R. Jessep: And one place he knew he could hide something, his ass. Five long years he wore this watch up his ass. Then he died of dysentery.\n",
      "[100.00-112.00] Colonel Nathan R. Jessep: Give me the watch. I hid this uncomfortable hunger metal up my ass. Two years. Then after seven years, I was sent home to my family.\n",
      "[112.00-121.00] Unknown: Now, little man, I'll give the watch to you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\10\\clip_1.mp4 from 21.0s to 29.0s\n",
      " Cutting statement_clips\\10\\clip_2.mp4 from 29.0s to 45.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\10\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['f2afff08fe834d928a0da1321d1fada8_clip_1.mp4'], 'pred': [0.9995944499969482], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['81a47f34a45d4137b083c0684c6363ad_clip_2.mp4'], 'pred': [0.9997856020927429], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 21.0, 'end': 29.0, 'speaker': 'Unknown', 'text': \"No one's boys have any illusions, but they'll leave in that island alive. So three days before the Japanese took the island,\", 'reason': 'This suggests a historical event involving the Japanese taking an island, which could be fact-checked for historical accuracy, especially the timing and context of the Battle of Wake Island.', 'context': 'This statement was made during a recounting of a family history involving military service, seemingly in a fictional or dramatized narrative style.', 'frame_path': 'statement_frames\\\\Unknown_002100.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9995944499969482}, {'start': 29.0, 'end': 45.0, 'speaker': 'Colonel Nathan R. Jessep', 'text': 'Granddad asked a gunner on an Air Force transport, Nimo. Warnaki. Man in him met before in his life.', 'reason': 'This implies military interactions during a historical conflictcould be fact-checked for accuracy regarding military operations and interactions during specific battles.', 'context': 'The statement was made during a narrative involving military history and family legacy tied to the Battle of Wake Island.', 'frame_path': 'statement_frames\\\\Colonel Nathan R. Jessep_002900.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9997856020927429}]\n",
      " Statement No one's boys have any illusions, but they'll leave in that island alive. So three days before the Japanese took the island, is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\10.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 12: ../data/dfw_youtube_release\\11.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\11.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-7.00] Unknown: Can you tell me what noise a fish makes?\n",
      "[7.00-11.00] Unknown: Now you only get one chance, that was it Baba?\n",
      "[11.00-15.00] SPEAKER_00: Yes, seriously.\n",
      "[15.00-18.00] Unknown: Okay, next question. Where's your ear?\n",
      "[18.00-20.00] Unknown: Good girl.\n",
      "[20.00-24.00] Unknown: Okay, last question. Think very carefully about this, okay?\n",
      "[24.00-27.00] SPEAKER_00: Who's your favourite? Dad or me?\n",
      "[27.00-30.00] SPEAKER_00: Okay, I didn't actually finish the question, darling.\n",
      "[30.00-34.00] Unknown: Why don't we break for recess, have something to eat, and then come back to it, okay?\n",
      "[39.00-41.00] Unknown: Okay, you feeling better?\n",
      "[41.00-43.00] SPEAKER_00: Repeat after me.\n",
      "[43.00-45.00] SPEAKER_00: Dad.\n",
      "[45.00-47.00] SPEAKER_00: Dad.\n",
      "[47.00-49.00] Unknown: Dad.\n",
      "[49.00-51.00] Unknown: Dad.\n",
      "[51.00-52.00] Unknown: Dad.\n",
      "[52.00-54.00] SPEAKER_00: Who's your favourite?\n",
      "[54.00-57.00] Unknown: Okay, let's try something different. Who's your least favourite?\n",
      "[57.00-59.00] SPEAKER_00: Dad.\n",
      "[59.00-61.00] Unknown: Baba.\n",
      "[61.00-63.00] SPEAKER_00: Baba.\n",
      "[63.00-66.00] Unknown: Baba.\n",
      "[66.00-68.00] Unknown: Baba.\n",
      "[68.00-70.00] Unknown: Here we go. Who's getting a pony?\n",
      "[70.00-72.00] SPEAKER_00: And who's your favourite?\n",
      "[72.00-74.00] SPEAKER_00: Yes.\n",
      "[74.00-77.00] SPEAKER_00: Yes.\n",
      "[77.00-81.00] Unknown: No, I'm not picking you up until we get this sorted, okay?\n",
      "[81.00-83.00] SPEAKER_00: It's a battle of whits, man you.\n",
      "[83.00-86.00] SPEAKER_00: Ha ha ha.\n",
      "[86.00-89.00] SPEAKER_00: Ha ha ha.\n",
      "[89.00-92.00] Unknown: Ha ha ha.\n",
      "[92.00-95.00] SPEAKER_00: Ha ha ha.\n",
      "[95.00-98.00] SPEAKER_00: Okay, we had enough?\n",
      "[98.00-100.00] SPEAKER_00: Yes.\n",
      "[100.00-104.00] SPEAKER_00: All right, here we go. For the last time, who's your favourite?\n",
      "[104.00-107.00] SPEAKER_00: Baba.\n",
      "[107.00-110.00] Unknown: Okay.\n",
      " Found 0 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Finished cutting video into clips. Saved to statement_clips\\11\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Error 422: {\"detail\":[{\"type\":\"missing\",\"loc\":[\"body\",\"videos\"],\"msg\":\"Field required\",\"input\":null}]}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[]\n",
      " Enriching statements with articles...\n",
      " Finished enriching statements with articles.\n",
      " Running fact-checks on statements...\n",
      " All statements are supported or unverified.\n",
      " Finished processing video ../data/dfw_youtube_release\\11.mp4\n",
      "Deepfake label: REAL, Fact-check label: True\n",
      " Processing video 13: ../data/dfw_youtube_release\\12.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\12.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Jack Nicholson\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.16] Unknown: I'll answer the question.\n",
      "[2.16-3.22] Unknown: You want answers?\n",
      "[3.22-4.22] Jack Nicholson: I think I'm entitled.\n",
      "[4.22-5.42] Jack Nicholson: You want answers!\n",
      "[5.42-6.62] Jack Nicholson: I want the truth!\n",
      "[6.62-10.06] Unknown: You can't handle the truth!\n",
      "[10.06-11.98] Unknown: Son, we live in a world that has walls,\n",
      "[11.98-14.66] Jack Nicholson: and those walls have to be guarded by men with guns.\n",
      "[14.66-15.66] Jack Nicholson: Who's going to do it?\n",
      "[15.66-19.36] Jack Nicholson: You, you lieutenant Weinberg?\n",
      "[19.36-23.30] Unknown: I have a greater responsibility than you can possibly fathom.\n",
      "[23.30-26.74] Unknown: You weep for Santiago and you curse the Marines.\n",
      "[26.74-28.24] Unknown: You have that luxury.\n",
      "[28.24-30.76] Jack Nicholson: You have the luxury of not knowing what I know.\n",
      "[30.76-34.88] Jack Nicholson: That Santiago's death-wall tragic probably saved lives.\n",
      "[34.88-38.28] Jack Nicholson: And my existence while grotesque and incomprehensible\n",
      "[38.28-40.96] Jack Nicholson: to you saves lives.\n",
      "[40.96-43.20] Jack Nicholson: You don't want the truth because deep down in places\n",
      "[43.20-45.12] Jack Nicholson: you don't talk about at parties.\n",
      "[45.12-47.24] Jack Nicholson: You want me on that wall.\n",
      "[47.24-50.28] Jack Nicholson: You need me on that wall.\n",
      "[50.28-54.84] Unknown: We use words like honor, code, loyalty.\n",
      "[54.84-56.60] Unknown: We use these words as the backbone\n",
      "[56.60-58.64] Jack Nicholson: of a life spent defending something.\n",
      "[58.64-60.68] Jack Nicholson: You use them as a punchline.\n",
      "[60.68-63.16] Jack Nicholson: I have neither the time or the inclination\n",
      "[63.16-66.48] Jack Nicholson: to explain myself to a man who rises and sleeps\n",
      "[66.48-68.88] Jack Nicholson: under the blanket of the very freedom that I provide,\n",
      "[68.88-71.80] Jack Nicholson: and then quest into the manner in which I provide it.\n",
      "[71.80-75.04] Jack Nicholson: I would rather you just said thank you and went on your way.\n",
      "[75.04-78.04] Jack Nicholson: Otherwise, I suggest you pick up a weapon and stand\n",
      "[78.04-79.04] Jack Nicholson: opposed.\n",
      "[79.04-82.68] Jack Nicholson: Either way, I don't give a damn what you think you are.\n",
      "[82.68-83.76] Jack Nicholson: Think you idle too.\n",
      " Found 1 checkworthy statements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\12\\clip_1.mp4 from 30.76s to 34.88s\n",
      " Finished cutting video into clips. Saved to statement_clips\\12\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['9d8ae244f0c24f7f837af19e1bed645d_clip_1.mp4'], 'pred': [0.9996300935745239], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 30.76, 'end': 34.88, 'speaker': 'Jack Nicholson', 'text': \"That Santiago's death-wall tragic probably saved lives.\", 'reason': 'This statement makes a factual claim about the death of a character and implies a cause-effect relationship between that death and the saving of lives.', 'context': 'The quote is from a movie script, during a fictional courtroom scene discussing the death of a character named Santiago. The speaker is defending military actions in the context of their necessity for national security.', 'frame_path': 'statement_frames\\\\Jack Nicholson_003076.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9996300935745239}]\n",
      " Statement That Santiago's death-wall tragic probably saved lives. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\12.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 14: ../data/dfw_youtube_release\\13.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\13.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Data.\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-4.00] Unknown: I'm attempting to fill a silent moment with non-relevant conversation.\n",
      "[6.50-7.50] Unknown: Small talk.\n",
      "[8.00-9.00] Unknown: Yes, sir.\n",
      "[9.50-14.00] Unknown: I've found that humans often use small talk during awkward moments.\n",
      "[14.00-17.00] Unknown: Therefore, I've written a new subroutine for that purpose.\n",
      "[18.00-19.00] Unknown: How did I do?\n",
      "[21.00-25.00] Unknown: Perhaps it was a little too non-relevant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\13\\clip_1.mp4 from 14.0s to 17.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\13\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['10cda6f438da4fb6bf00432f4ac51207_clip_1.mp4'], 'pred': [0.41758155822753906], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 14.0, 'end': 17.0, 'speaker': 'Unknown', 'text': \"Therefore, I've written a new subroutine for that purpose.\", 'reason': 'Statement makes a factual claim about creating a subroutine, which could be verified.', 'context': 'This was made in a casual conversation with no specific context or date. The speaker was discussing the creation of small talk routines for awkward moments.', 'frame_path': 'statement_frames\\\\Unknown_001400.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.41758155822753906}]\n",
      " Enriching statements with articles...\n",
      "https://www.bing.com/search?q=Therefore, I've written a new subroutine for that purpose.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.17\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"3b7c8003feae1ef92df0e906db28f156\", element=\"f.92CFBE65131DE6C3B9448C5376CC1E76.d.2AE2324D775D57AA6918BD15FDAAAD14.e.26\")>]\n",
      "Subroutines - Programming concepts - AQA - GCSE Computer \n",
      "https://www.bing.com/ck/a?!&&p=612356b052206f83eedf0ef068a2368973ca49ea82da2a19036080f2de2fe71dJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=23033173-d533-639f-1cf3-2751d40462eb&u=a1aHR0cHM6Ly93d3cuYmJjLmNvLnVrL2JpdGVzaXplL2d1aWRlcy96aDY2cGJrL3JldmlzaW9uLzc&ntb=1\n",
      "Defining and calling subroutines  Ada Computer Science\n",
      "https://www.bing.com/ck/a?!&&p=9ca1c23e3a6cd525ae837fbc537183248585c3f4f6deaa4b0f9b098a869e75c4JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=23033173-d533-639f-1cf3-2751d40462eb&u=a1aHR0cHM6Ly9hZGFjb21wdXRlcnNjaWVuY2Uub3JnL2NvbmNlcHRzL3N1Yl9kZWZpbmVfYW5kX2NhbGw&ntb=1\n",
      "Found 3 relevant links:\n",
      "[{'title': \"Therefore, I've written a new \", 'link': \"https://www.bing.com/search?q=Therefore, I've written a new subroutine for that purpose.\"}, {'title': 'Subroutines - Programming concepts - AQA - GCSE Computer ', 'link': 'https://www.bing.com/ck/a?!&&p=612356b052206f83eedf0ef068a2368973ca49ea82da2a19036080f2de2fe71dJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=23033173-d533-639f-1cf3-2751d40462eb&u=a1aHR0cHM6Ly93d3cuYmJjLmNvLnVrL2JpdGVzaXplL2d1aWRlcy96aDY2cGJrL3JldmlzaW9uLzc&ntb=1'}, {'title': 'Defining and calling subroutines  Ada Computer Science', 'link': 'https://www.bing.com/ck/a?!&&p=9ca1c23e3a6cd525ae837fbc537183248585c3f4f6deaa4b0f9b098a869e75c4JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=23033173-d533-639f-1cf3-2751d40462eb&u=a1aHR0cHM6Ly9hZGFjb21wdXRlcnNjaWVuY2Uub3JnL2NvbmNlcHRzL3N1Yl9kZWZpbmVfYW5kX2NhbGw&ntb=1'}]\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=Therefore, I've written a new subroutine for that purpose. with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/ck/a?!&&p=612356b052206f83eedf0ef068a2368973ca49ea82da2a19036080f2de2fe71dJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=23033173-d533-639f-1cf3-2751d40462eb&u=a1aHR0cHM6Ly93d3cuYmJjLmNvLnVrL2JpdGVzaXplL2d1aWRlcy96aDY2cGJrL3JldmlzaW9uLzc&ntb=1 with wait_time=10s\n",
      " Crawled content from https://www.bing.com/ck/a?!&&p=612356b052206f83eedf0ef068a2368973ca49ea82da2a19036080f2de2fe71dJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=23033173-d533-639f-1cf3-2751d40462eb&u=a1aHR0cHM6Ly93d3cuYmJjLmNvLnVrL2JpdGVzaXplL2d1aWRlcy96aDY2cGJrL3JldmlzaW9uLzc&ntb=1:\n",
      "Programs are designed using common building blocks, known as programming constructs. These programming constructs form the basis for all programs.\n",
      "Part of Computer ScienceComputational thinking and problem solving\n",
      "Save guides, add subjects and pick up where you left off with your BBC account.\n",
      "Subroutines are smaller, named sections of code that are written within a larger \n",
      "program\n",
      ". The purpose of a subroutine is to perform a specific task. This task may need to be done more than once at various...\n",
      " Crawled content from https://www.bing.com/search?q=Therefore, I've written a new subroutine for that purpose.:\n",
      "Subroutines are saved separately to the main program. Because they can be reused, they help make the program code shorter. Learn about and revise programming concepts with this BBC \n",
      "To call a subroutine, you write its identifier (name) within a program statement. In the example below, the subroutine greet_me is called from main. Observe that you must include the ...\n",
      "https://www.bing.com/search?q=This was made in a casual conversation with no specific context or date. The speaker was discussing the creation of small talk routines for awkward moments.\n",
      "Found 10 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.18\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.19\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.20\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.26\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"e8668390832ef6bbed2ae9b73ca6ebe3\", element=\"f.FD83CBCB6B0330385F9D036E83E869DB.d.A565C8F24C8EF79D137358CFC5C865DE.e.27\")>]\n",
      "Google Translate\n",
      "https://translate.google.ca/\n",
      "How to make small talk in English: 100 top questions & examples\n",
      "https://www.berlitz.com/blog/how-to-make-small-talk-english-questions-examples-tips\n",
      "Found 3 relevant links:\n",
      "[{'title': 'This was made in a casual conv', 'link': 'https://www.bing.com/search?q=This was made in a casual conversation with no specific context or date. The speaker was discussing the creation of small talk routines for awkward moments.'}, {'title': 'Google Translate', 'link': 'https://translate.google.ca/'}, {'title': 'How to make small talk in English: 100 top questions & examples', 'link': 'https://www.berlitz.com/blog/how-to-make-small-talk-english-questions-examples-tips'}]\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=This was made in a casual conversation with no specific context or date. The speaker was discussing the creation of small talk routines for awkward moments. with wait_time=10s Attempt 1: Crawling https://translate.google.ca/ with wait_time=10s\n",
      "\n",
      " Crawled content from https://www.bing.com/search?q=This was made in a casual conversation with no specific context or date. The speaker was discussing the creation of small talk routines for awkward moments.:\n",
      "Google's service, offered free of charge, instantly translates words, phrases, and web pages between English and over 100 other languages.\n",
      "Transform awkward into confident, flowing conversation with our handy questions, examples, topics and tips for how to make small talk in English.\n",
      "Jan 21, 2025 In this comprehensive guide, we delve into a treasure trove of real-life informal communication examples. From casual workplace chats to digital exchanges among friends, ...\n",
      " Crawled content from https://translate.google.ca/:\n",
      "Enter a URL...\n",
      " Finished enriching statements with articles.\n",
      " Running fact-checks on statements...\n",
      "{'start': 14.0, 'end': 17.0, 'speaker': 'Unknown', 'text': \"Therefore, I've written a new subroutine for that purpose.\", 'reason': 'Statement makes a factual claim about creating a subroutine, which could be verified.', 'context': 'This was made in a casual conversation with no specific context or date. The speaker was discussing the creation of small talk routines for awkward moments.', 'frame_path': 'statement_frames\\\\Unknown_001400.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.41758155822753906, 'article_texts': \"### Therefore, I've written a new \\nSubroutines are saved separately to the main program. Because they can be reused, they help make the program code shorter. Learn about and revise programming concepts with this BBC \\nTo call a subroutine, you write its identifier (name) within a program statement. In the example below, the subroutine greet_me is called from main. Observe that you must include the \\n\\n### Subroutines - Programming concepts - AQA - GCSE Computer \\nPrograms are designed using common building blocks, known as programming constructs. These programming constructs form the basis for all programs.\\nPart of Computer ScienceComputational thinking and problem solving\\nSave guides, add subjects and pick up where you left off with your BBC account.\\nSubroutines are smaller, named sections of code that are written within a larger \\nprogram\\n. The purpose of a subroutine is to perform a specific task. This task may need to be done more than once at various points in the main program.\\nSubroutines are usually small in size, which means they are much easier to write, test and debug. They are also easy for someone else to understand.\\nAs they are written outside of the main program, subroutines can be saved separately as modules and used again in other programs. This saves time because the programmer can use code that has already been written, tested and debugged.\\nA subroutine may be used repeatedly at various points in the main program and changed by passing in \\ndata\\n known as parameters. However, the code only has to be written once, resulting in shorter programs.\\nCopyright  2025 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.\\n\\n### This was made in a casual conv\\nGoogle's service, offered free of charge, instantly translates words, phrases, and web pages between English and over 100 other languages.\\nTransform awkward into confident, flowing conversation with our handy questions, examples, topics and tips for how to make small talk in English.\\nJan 21, 2025\\xa0 In this comprehensive guide, we delve into a treasure trove of real-life informal communication examples. From casual workplace chats to digital exchanges among friends, \\n\\n### Google Translate\\nEnter a URL\"}\n",
      " Fact-checking: Therefore, I've written a new subroutine for that purpose....\n",
      " All statements are supported or unverified.\n",
      " Finished processing video ../data/dfw_youtube_release\\13.mp4\n",
      "Deepfake label: REAL, Fact-check label: True\n",
      " Processing video 15: ../data/dfw_youtube_release\\14.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\14.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Unnamed\n",
      "\n",
      " ang x l SPEAKER_02...\n",
      " SPEAKER_02  Unnamed\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-7.00] Unknown: I've been watching you for some time\n",
      "[7.00-13.00] Unknown: Can't stop staring at those ocean eyes\n",
      "[13.00-19.00] SPEAKER_02: Burning cities and they bomb skies\n",
      "[19.00-26.00] SPEAKER_02: 15 flares inside those ocean eyes\n",
      "[26.00-33.00] SPEAKER_02: You're ocean eyes all for you\n",
      "[33.00-38.00] SPEAKER_02: You're in order to make me cry\n",
      "[38.00-42.00] SPEAKER_01: When you give me those all\n",
      "[42.00-47.00] Unknown: You're in order to make me cry\n",
      "[47.00-50.00] SPEAKER_01: When you give me those all shine eyes\n",
      "[50.00-55.00] SPEAKER_01: I'm scared\n",
      "[55.00-60.00] SPEAKER_01: Never fall apart this life\n",
      "[60.00-64.00] SPEAKER_01: Fallen into your all shine eyes\n",
      "[64.00-68.00] SPEAKER_01: All shine eyes\n",
      "[72.00-77.00] Unknown: You're in order to make me cry\n",
      "[77.00-82.00] Unknown: When you give me those all shine eyes\n",
      "[82.00-87.00] Unknown: I'm scared\n",
      "[87.00-93.00] Unknown: Never fall apart this life\n",
      "[93.00-99.00] Unknown: Fallen into your all shine eyes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\14\\clip_1.mp4 from 13.0s to 19.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\14\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['9478e7447de04a16ad97b8e4fed69548_clip_1.mp4'], 'pred': [0.9179778695106506], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 13.0, 'end': 19.0, 'speaker': 'SPEAKER_02', 'text': 'Burning cities and they bomb skies', 'reason': 'The statement suggests a scenario of significant destruction which may be related to historical events or conflicts, warranting verification.', 'context': 'This statement was part of a lyrical conversation, possibly poetic or metaphorical, without a clear context regarding current events or political discussions.', 'frame_path': 'statement_frames\\\\SPEAKER_02_001300.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9179778695106506}]\n",
      " Statement Burning cities and they bomb skies is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\14.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 16: ../data/dfw_youtube_release\\15.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\15.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-4.00] Unknown: Are you talking to me?\n",
      "[4.00-6.00] Unknown: Then who the hell is he talking to?\n",
      "[6.00-8.00] SPEAKER_03: You talking to me?\n",
      "[8.00-10.00] SPEAKER_03: I'm the only one here.\n",
      "[10.00-12.00] SPEAKER_03: So who the fuck are you thinking you're talking to?\n",
      "[16.00-19.00] Unknown: Some day, and that day may never come.\n",
      "[19.00-21.00] SPEAKER_05: I'll call upon you to do a service for me.\n",
      "[21.00-23.00] SPEAKER_05: But until that day comes,\n",
      "[23.00-25.00] SPEAKER_05: except this justice as a gift\n",
      "[25.00-27.00] SPEAKER_05: on my daughter's wedding day.\n",
      "[27.00-30.00] SPEAKER_05: Mama always said life is like a box of chocolates.\n",
      "[30.00-32.00] SPEAKER_04: You never know what you're going to get.\n",
      "[32.00-35.00] SPEAKER_04: If you let my daughter go now,\n",
      "[35.00-37.00] Unknown: that'll be the end of it.\n",
      "[37.00-39.00] Hello: I will not look for you.\n",
      "[39.00-41.00] Hello: I will not pursue you.\n",
      "[41.00-43.00] Hello: But if you don't,\n",
      "[43.00-45.00] Hello: I will look for you.\n",
      "[45.00-47.00] Hello: I will find you.\n",
      "[47.00-50.00] Hello: And, you know, I'll kill you.\n",
      "[50.00-53.00] Unknown: So 1,100 men went into the water.\n",
      "[53.00-55.00] SPEAKER_03: 316 men come out.\n",
      "[55.00-57.00] SPEAKER_03: Now the sharks they took the rest.\n",
      "[57.00-60.00] SPEAKER_03: June the 29th, 1945.\n",
      "[60.00-62.00] SPEAKER_03: And I want my sharks.\n",
      "[62.00-64.00] SPEAKER_02: No wire hangers.\n",
      "[64.00-65.00] SPEAKER_02: Come on.\n",
      "[65.00-67.00] SPEAKER_02: What's wire hangers doing in this closet?\n",
      "[67.00-69.00] SPEAKER_02: I told you no wire hangers ever.\n",
      "[69.00-70.00] SPEAKER_02: Come on.\n",
      "[70.00-71.00] SPEAKER_02: You want to get nuts?\n",
      "[71.00-72.00] SPEAKER_02: Let's get nuts.\n",
      "[72.00-74.00] SPEAKER_02: I want you to draw me\n",
      "[74.00-78.00] SPEAKER_03: like one of your French girls wearing this.\n",
      "[78.00-81.00] SPEAKER_03: Wearing only this.\n",
      "[81.00-84.00] SPEAKER_03: Brains in through the nose.\n",
      "[84.00-85.00] Unknown: Walks on.\n",
      "[85.00-86.00] SPEAKER_03: Walks off.\n",
      "[86.00-88.00] SPEAKER_03: Don't forget to braze.\n",
      "[88.00-89.00] SPEAKER_03: It's very important.\n",
      "[89.00-92.00] SPEAKER_03: Now, you boys put those guns down.\n",
      "[92.00-94.00] SPEAKER_01: Go ahead.\n",
      "[94.00-95.00] Unknown: Make my day.\n",
      "[95.00-97.00] SPEAKER_01: I could have had class.\n",
      "[97.00-99.00] Unknown: I could have been a contender.\n",
      "[99.00-101.00] Unknown: Instead of being a bum.\n",
      "[101.00-102.00] Unknown: Which is what I am.\n",
      "[102.00-103.00] SPEAKER_04: Shut up.\n",
      "[103.00-104.00] SPEAKER_04: Shut up.\n",
      "[104.00-106.00] SPEAKER_04: You had me alone.\n",
      "[106.00-109.00] SPEAKER_04: I told that crowd a couple thousand times\n",
      "[109.00-112.00] SPEAKER_04: that I don't roll on showers.\n",
      "[112.00-115.00] SPEAKER_04: Saturday, Donnie, and Shavas, the Jewish day of rest.\n",
      "[115.00-117.00] SPEAKER_04: It's not your fault.\n",
      "[117.00-119.00] SPEAKER_04: It's not your fault, Will.\n",
      "[119.00-121.00] SPEAKER_04: Go down to the Goodwill store.\n",
      "[121.00-123.00] SPEAKER_04: I want you to stop being such a sad sap.\n",
      "[123.00-124.00] Hello: Hello.\n",
      "[124.00-126.00] Unknown: My name is Enigle Montoya.\n",
      "[126.00-127.00] Hello: You killed my father.\n",
      "[127.00-128.00] Unknown: Prepare to die.\n",
      "[128.00-130.00] Hello: I said dead people.\n",
      "[130.00-134.00] Unknown: They're walking around like ready-lap people.\n",
      "[134.00-136.00] Unknown: They don't see each other.\n",
      "[136.00-138.00] SPEAKER_01: They only see what they want to see.\n",
      "[138.00-140.00] SPEAKER_01: They don't know they're dead.\n",
      "[140.00-142.00] SPEAKER_01: They're everywhere.\n",
      "[142.00-143.00] SPEAKER_01: Wow.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 2 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\15\\clip_1.mp4 from 53.0s to 57.0s\n",
      " Cutting statement_clips\\15\\clip_2.mp4 from 57.0s to 60.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\15\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['abcf45371888414bb72df1082c35a6b3_clip_1.mp4'], 'pred': [0.5907583236694336], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['99cbf162c5a0492a886b76e0adfb77af_clip_2.mp4'], 'pred': [0.6091124415397644], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 53.0, 'end': 57.0, 'speaker': 'SPEAKER_03', 'text': '316 men come out. Now the sharks they took the rest.', 'reason': 'The statement contains a factual claim about historical events, specifically regarding the number of survivors and casualties due to shark attacks, likely referring to a historical maritime disaster.', 'context': 'This quote appears to be in a dramatic setting, possibly a reenactment or recount of a historical event, discussed in a thematic storytelling context.', 'frame_path': 'statement_frames\\\\SPEAKER_03_005300.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.5907583236694336}, {'start': 57.0, 'end': 60.0, 'speaker': 'SPEAKER_03', 'text': 'June the 29th, 1945.', 'reason': 'The statement refers to a specific historical date, which could relate to a significant event during World War II.', 'context': 'This statement is likely set within a narrative describing historical events, possibly related to naval warfare.', 'frame_path': 'statement_frames\\\\SPEAKER_03_005700.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.6091124415397644}]\n",
      " Statement 316 men come out. Now the sharks they took the rest. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\15.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 17: ../data/dfw_youtube_release\\16.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\16.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-7.00] Unknown: You've got to give me any idea what it's like to live with the price on your head.\n",
      "[7.00-9.00] Name not available: The only thing to do is to go to them.\n",
      "[9.00-14.00] Unknown: Maybe I can find some way to make it up to them.\n",
      "[14.00-21.00] Unknown: And that's what we do.\n",
      "[21.00-28.00] Unknown: No.\n",
      "[28.00-31.00] Unknown: Nino's me, not you.\n",
      "[31.00-34.00] Unknown: If you come with me,\n",
      "[34.00-36.00] Unknown: show your face.\n",
      "[36.00-40.00] SPEAKER_02: If they don't kill you,\n",
      "[40.00-44.00] Unknown: you're in this life for good.\n",
      "[44.00-49.00] Unknown: You find somewhere to square this.\n",
      "[49.00-52.00] Unknown: You still got our money.\n",
      "[52.00-56.00] Unknown: Maybe.\n",
      "[56.00-63.00] Unknown: From there to work the risk.\n",
      " Found 2 checkworthy statements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\16\\clip_1.mp4 from 0.0s to 7.0s\n",
      " Cutting statement_clips\\16\\clip_2.mp4 from 40.0s to 44.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\16\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['988556fc5b044e978ae10d348403a2f2_clip_1.mp4'], 'pred': [0.7289178967475891], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      "clip_2.mp4: {'name': ['e36ae4f4d3d04800a4560d9b86bb4bb7_clip_2.mp4'], 'pred': [0.8799577951431274], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 0.0, 'end': 7.0, 'speaker': 'Unknown', 'text': \"You've got to give me any idea what it's like to live with the price on your head.\", 'reason': 'This suggests a high-stakes situation involving threats or a bounty, potentially checkable for context or factual accuracy.', 'context': 'This statement is made during a conversation seemingly related to danger or threats. The setting is unclear, but it happens during a tense dialogue revolving around danger or conflict.', 'frame_path': 'statement_frames\\\\Unknown_000000.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.7289178967475891}, {'start': 40.0, 'end': 44.0, 'speaker': 'Unknown', 'text': \"You're in this life for good.\", 'reason': 'This implies a permanent consequence or lifestyle, possibly checkable for its wider implications or context.', 'context': 'This is part of a conversation suggesting danger or involvement with a criminal element. The context suggests a dangerous lifestyle being discussed.', 'frame_path': 'statement_frames\\\\Unknown_004000.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.8799577951431274}]\n",
      " Statement You've got to give me any idea what it's like to live with the price on your head. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\16.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 18: ../data/dfw_youtube_release\\17.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\17.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Unnamed\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-1.10] Unknown: What do you do for a living?\n",
      "[1.10-1.90] Unknown: Lots of things.\n",
      "[1.90-2.70] SPEAKER_00: What's your office?\n",
      "[2.70-3.50] SPEAKER_00: I don't have one.\n",
      "[3.50-4.00] Unknown: How come?\n",
      "[4.00-4.80] SPEAKER_01: I don't need one.\n",
      "[4.80-5.30] Unknown: What's your wife?\n",
      "[5.30-6.10] SPEAKER_01: Don't have one.\n",
      "[6.10-6.60] Unknown: How come?\n",
      "[6.60-7.60] SPEAKER_01: It's a long story.\n",
      "[7.60-8.20] Unknown: Do you have kids?\n",
      "[8.20-9.00] SPEAKER_01: No, I don't.\n",
      "[9.00-9.50] SPEAKER_01: How come?\n",
      "[9.50-10.70] SPEAKER_00: It's an even longer story.\n",
      "[10.70-11.70] Unknown: Are you my dad's brother?\n",
      "[11.70-13.90] Unknown: What's your record for consecutive questions asked?\n",
      "[13.90-15.00] SPEAKER_00: 38.\n",
      "[15.00-16.40] SPEAKER_00: I'm your dad's brother, all right.\n",
      "[16.40-18.60] Unknown: You have much more hair than you know, so my dad.\n",
      "[18.60-19.80] Unknown: I'm nice of you to notice.\n",
      "[19.80-21.00] SPEAKER_01: I'm a kid, that's my job.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\17\\clip_1.mp4 from 13.9s to 15.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\17\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['0ae140083bde4ae08d71954a8f85c08f_clip_1.mp4'], 'pred': [0.6029241681098938], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 13.9, 'end': 15.0, 'speaker': 'SPEAKER_00', 'text': '38.', 'reason': 'The claim of answering 38 consecutive questions could be verified for accuracy.', 'context': 'The statement was made during a casual conversation between family members. The topic of discussion was questioning prowess immediately before and after this statement.', 'frame_path': 'statement_frames\\\\SPEAKER_00_001390.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.6029241681098938}]\n",
      " Statement 38. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\17.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 19: ../data/dfw_youtube_release\\18.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\18.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Noah Feldman\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Matt Gaetz\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Noah Feldman\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-5.20] Unknown: A Harvard Law professor thinks Trump could be impeached over fake news accusations.\n",
      "[5.20-9.60] Matt Gaetz: My question, Professor Feldman, is since you seem to believe that the basis for impeachment\n",
      "[9.60-13.68] Matt Gaetz: is even broader than the basis that my Democratic colleagues have laid forward,\n",
      "[13.68-17.44] Matt Gaetz: do you believe you're outside of the political mainstream on the question of impeachment?\n",
      "[18.24-22.72] Unknown: I believe that impeachment is warranted whenever the president abuses his power\n",
      "[22.72-25.52] Noah Feldman: for personal benefit or to correct the democratic process.\n",
      "[25.52-28.88] Noah Feldman: Did you write an article entitled It's Hard to Take Impeachment Seriously Now?\n",
      "[29.52-30.56] Unknown: Yes, I did write that article.\n",
      "[30.56-32.00] Noah Feldman: And that article did you write in May?\n",
      "[32.00-33.28] Noah Feldman: All the time, I'm winning it on time.\n",
      "[33.28-36.08] Noah Feldman: So, did you write since, since 2018,\n",
      "[36.08-39.68] Matt Gaetz: midterm election, House Democrats have made it painfully clear that\n",
      "[39.68-45.84] Matt Gaetz: discussing impeachment is primarily or even exclusively a tool to weaken President Trump's\n",
      "[45.84-48.32] Matt Gaetz: chances in 2020. Did you write those words?\n",
      "[48.32-52.48] Noah Feldman: Until this call in July 25th, I was an impeachment skeptic.\n",
      "[52.48-54.64] Unknown: They're all changed, my mind, sir, and foregoed.\n",
      "[54.64-55.92] Noah Feldman: Thank you. I appreciate your testimony.\n",
      " Found 1 checkworthy statements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\18\\clip_1.mp4 from 39.68s to 45.84s\n",
      " Finished cutting video into clips. Saved to statement_clips\\18\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['6a4237e803cf4f808471424526eda4f8_clip_1.mp4'], 'pred': [0.8159986138343811], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 39.68, 'end': 45.84, 'speaker': 'Matt Gaetz', 'text': \"discussing impeachment is primarily or even exclusively a tool to weaken President Trump's chances in 2020. Did you write those words?\", 'reason': 'This is a bold assertion regarding the political motives behind impeachment discussions, which can be fact-checked against public statements and intentions of political figures.', 'context': 'This statement was made during a congressional hearing or debate focused on impeachment, likely occurring around the end of 2019 or in 2020. The context involved questioning the motives behind impeachment efforts and whether they were genuinely about misconduct or political strategy.', 'frame_path': 'statement_frames\\\\Matt Gaetz_003968.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.8159986138343811}]\n",
      " Statement discussing impeachment is primarily or even exclusively a tool to weaken President Trump's chances in 2020. Did you write those words? is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\18.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 20: ../data/dfw_youtube_release\\19.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\19.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Unnamed\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-7.00] Unknown: One grain of sand is all the remains of my vast empire.\n",
      "[16.00-20.00] Unknown: The foundation has totally disappeared?\n",
      "[20.00-22.00] Unknown: Yes.\n",
      "[25.00-28.00] Unknown: So everything's been in vain?\n",
      "[28.00-31.00] Unknown: No, it hasn't.\n",
      "[32.00-35.00] Unknown: Fantasia can arise in you.\n",
      "[36.00-39.00] Unknown: Bring your dreams and wishes best in.\n",
      "[40.00-42.00] Unknown: How?\n",
      "[42.00-45.00] Unknown: Open your head.\n",
      "[59.00-62.00] Unknown: What are you going to wish for?\n",
      "[63.00-65.00] Unknown: I don't know.\n",
      "[66.00-71.00] Unknown: And there will be no Fantasia anymore.\n",
      "[75.00-80.00] Unknown: I'm wish to I get as many as you want.\n",
      "[81.00-87.00] Unknown: And the more wishes you make, the more magnificent Fantasia will become.\n",
      "[88.00-90.00] Unknown: Really?\n",
      "[90.00-92.00] Unknown: Try it.\n",
      "[104.00-108.00] Unknown: And my first wish is...\n",
      "[110.00-113.00] Unknown: Day? Free of girls!\n",
      "[113.00-115.00] SPEAKER_00: What is happening right now?\n",
      "[115.00-117.00] SPEAKER_00: I will come on.\n",
      "[117.00-122.00] Unknown: Try the villagers or under threat from an evil force from the swamps of Kuzaton.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\19\\clip_1.mp4 from 81.0s to 87.0s\n",
      " Finished cutting video into clips. Saved to statement_clips\\19\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['fd2d1a8192af4871969a43c40d66f766_clip_1.mp4'], 'pred': [0.9889195561408997], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 81.0, 'end': 87.0, 'speaker': 'Unknown', 'text': 'And the more wishes you make, the more magnificent Fantasia will become.', 'reason': \"This statement suggests a cause-effect relationship, implying that wishes have a direct impact on Fantasia's magnificence.\", 'context': 'The statement was made in a fictional context, likely in a movie scene discussing the realm of Fantasia. The topic being discussed was the power of wishes and their impact on Fantasia, with dialogue focusing on dreams and wishes.', 'frame_path': 'statement_frames\\\\Unknown_008100.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.9889195561408997}]\n",
      " Statement And the more wishes you make, the more magnificent Fantasia will become. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\19.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Saved checkpoint after 20 videos\n",
      " Processing video 21: ../data/dfw_youtube_release\\20.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\20.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_02...\n",
      " SPEAKER_02  Unnamed\n",
      "\n",
      " ang x l SPEAKER_05...\n",
      " SPEAKER_05  Unnamed\n",
      "\n",
      " ang x l SPEAKER_04...\n",
      " SPEAKER_04  Joaquin Phoenix\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-3.44] Unknown: The Rossy Oscar goes to Matthew McConaughey.\n",
      "[3.44-4.60] SPEAKER_02: Well, I'll tell you what.\n",
      "[4.60-6.84] SPEAKER_02: I mean, this is just, uh...\n",
      "[6.84-8.32] SPEAKER_02: It's something else. This is Stellar.\n",
      "[8.32-10.08] SPEAKER_02: This is, uh...\n",
      "[10.08-11.32] SPEAKER_02: This is the Interstellar.\n",
      "[11.32-14.12] Unknown: LAUGHTER\n",
      "[14.12-17.12] Unknown: APPLAUSE\n",
      "[17.12-18.60] John C. Riley: John C. Riley.\n",
      "[18.60-20.28] John C. Riley: Uh, John C. Riley, wow.\n",
      "[20.28-21.12] John C. Riley: This is...\n",
      "[21.12-22.00] John C. Riley: This is the...\n",
      "[22.00-23.08] John C. Riley: This is kind of special.\n",
      "[23.08-26.08] John C. Riley: I mean, this is like really extra special.\n",
      "[26.08-26.60] John C. Riley: I mean, this is...\n",
      "[26.60-28.92] John C. Riley: This is like super duper extra special.\n",
      "[28.92-30.12] John C. Riley: It's a little stuff.\n",
      "[30.12-31.76] John C. Riley: Mmm.\n",
      "[31.76-33.80] Unknown: Nobody does that with it.\n",
      "[33.80-35.36] SPEAKER_05: Antonio Banderas.\n",
      "[35.36-36.56] Joaquin Phoenix: Antonio Banderas.\n",
      "[36.56-37.76] Joaquin Phoenix: Uh, okay.\n",
      "[37.76-41.40] Joaquin Phoenix: Pushing Boosh was a labor of love.\n",
      "[41.40-43.20] SPEAKER_05: LAUGHTER\n",
      "[43.20-44.76] SPEAKER_05: This is a classic.\n",
      "[44.76-46.36] SPEAKER_05: Everyone has their version of this one.\n",
      "[46.36-47.48] SPEAKER_05: Christopher Walken.\n",
      "[47.48-49.12] SPEAKER_05: Oh, Christopher.\n",
      "[49.12-51.16] Unknown: Hello, Oscar.\n",
      "[51.16-53.24] SPEAKER_02: You're beautiful.\n",
      "[53.24-56.04] Unknown: The way you listen makes me arouse.\n",
      "[56.04-58.24] SPEAKER_02: LAUGHTER\n",
      "[58.24-61.24] Unknown: CHEERING AND APPLAUSE\n",
      "[61.24-63.32] Unknown: Here's a guy who does a lot of impressions himself.\n",
      "[63.32-64.32] SPEAKER_05: Kevin Spacy.\n",
      "[64.32-66.12] SPEAKER_05: Oh, Kevin Spacy. Sure, sure.\n",
      "[66.12-69.20] SPEAKER_03: I wish I could say that I was surprised by this,\n",
      "[69.20-70.32] SPEAKER_03: but...\n",
      "[70.32-72.36] SPEAKER_03: Come on. I mean, who are you going to choose?\n",
      "[72.36-73.72] SPEAKER_03: Matt Damon.\n",
      "[73.72-76.76] SPEAKER_03: LAUGHTER\n",
      "[76.76-78.36] Unknown: You do...\n",
      "[78.36-80.72] Unknown: You do any of Matt Damon?\n",
      "[80.72-82.12] SPEAKER_03: Matt Damon, I don't do his voice,\n",
      "[82.12-83.40] SPEAKER_03: but I do like a micro impression.\n",
      "[83.40-85.56] SPEAKER_03: He's just always trying to restrain a smile.\n",
      "[85.56-87.00] SPEAKER_03: It seems like he's always just like...\n",
      "[87.00-88.40] SPEAKER_03: He's doing the...\n",
      "[88.40-89.52] SPEAKER_03: You know, he's just like...\n",
      "[89.52-91.52] SPEAKER_02: LAUGHTER\n",
      "[91.52-92.52] Unknown: Yeah, he's...\n",
      "[92.52-93.84] SPEAKER_03: You know, I love it.\n",
      "[93.84-94.68] SPEAKER_03: It's like the most...\n",
      "[94.68-95.68] SPEAKER_03: It's sick of it, yeah, yeah.\n",
      "[95.68-96.76] SPEAKER_05: It's terrible, right?\n",
      "[96.76-97.68] SPEAKER_05: He's terrible.\n",
      "[97.68-99.04] SPEAKER_05: He's terrible. He's the worst.\n",
      "[99.04-100.28] SPEAKER_03: All right. You like him.\n",
      "[100.28-101.72] SPEAKER_03: You know, yeah. Just in Timberlake.\n",
      "[101.72-102.76] SPEAKER_03: Just... Oh, just in Timberlake.\n",
      "[102.76-103.76] SPEAKER_03: Okay.\n",
      "[103.76-106.20] SPEAKER_03: Well, I was like totally in the bathroom.\n",
      "[106.20-107.48] SPEAKER_03: LAUGHTER\n",
      "[107.48-108.28] SPEAKER_03: Oh!\n",
      "[108.28-109.12] SPEAKER_03: I'm so sorry.\n",
      "[109.12-111.16] SPEAKER_03: Did I win this for best actor or best song?\n",
      "[111.16-112.76] SPEAKER_03: I don't actually don't know.\n",
      "[112.76-114.28] SPEAKER_03: LAUGHTER\n",
      "[114.88-115.88] Unknown: Good.\n",
      "[116.88-117.68] Unknown: All right.\n",
      "[117.68-119.56] Joaquin Phoenix: How about Gary Bucy?\n",
      "[119.56-120.56] Joaquin Phoenix: Gary B... Okay.\n",
      "[120.56-121.56] Joaquin Phoenix: Uh...\n",
      "[121.56-122.56] Joaquin Phoenix: Pineapples,\n",
      "[122.56-123.60] Joaquin Phoenix: Kenwai,\n",
      "[123.60-124.80] Joaquin Phoenix: Gojiberries,\n",
      "[124.80-126.40] Joaquin Phoenix: coating around in space.\n",
      "[126.40-128.40] Unknown: LAUGHTER\n",
      "[129.40-131.52] Unknown: These are guys that didn't get a chance to get a speech\n",
      "[131.52-133.36] SPEAKER_05: so you can give him four-inch Sylvester Stallone.\n",
      "[133.36-134.60] SPEAKER_05: Oh, yes. That was so sad.\n",
      "[134.60-135.40] SPEAKER_01: I mean, I...\n",
      "[135.40-136.72] SPEAKER_01: I didn't win, you know what I mean?\n",
      "[136.72-137.56] SPEAKER_01: I really...\n",
      "[137.56-139.76] SPEAKER_01: I thought this was supposed to be the comeback story of the year.\n",
      "[139.76-140.76] SPEAKER_01: I mean, I...\n",
      "[140.76-142.52] SPEAKER_01: I got knocked out by Mark Reilens.\n",
      "[142.52-143.92] SPEAKER_01: I mean, they've even heard of the guy.\n",
      "[143.92-145.44] SPEAKER_01: Come on.\n",
      "[145.44-146.44] Unknown: Come on.\n",
      "[146.44-147.44] Unknown: CHEERING AND APPLAUSE\n",
      "[147.44-148.84] SPEAKER_02: Well done.\n",
      "[148.84-149.96] SPEAKER_02: Michael Kane.\n",
      "[149.96-151.56] SPEAKER_02: Michael Kane. Oh, sure.\n",
      "[151.56-153.04] SPEAKER_02: Oh, sorry.\n",
      "[153.04-154.24] SPEAKER_02: Where are my notes?\n",
      "[154.24-155.36] SPEAKER_02: Um...\n",
      "[155.36-157.84] SPEAKER_02: Oh, yeah. I think I must have left him in the Rolls Royce.\n",
      "[157.84-158.88] Unknown: LAUGHTER\n",
      "[158.88-160.40] SPEAKER_02: All right. One more. One more.\n",
      "[160.40-161.40] SPEAKER_02: Christian Bell.\n",
      "[161.40-162.52] SPEAKER_01: Christian Bell, okay.\n",
      "[162.52-166.28] SPEAKER_01: Uh, listen, there are so many people that I'd like to thank.\n",
      "[166.28-167.76] SPEAKER_01: Uh, first off,\n",
      "[167.76-170.52] SPEAKER_01: are you starting to play music\n",
      "[170.52-172.76] SPEAKER_01: in the middle of my speech, mate?\n",
      "[172.76-173.48] SPEAKER_01: Are you a f***ing?\n",
      "[173.48-175.00] SPEAKER_01: You're opening, mate.\n",
      "[175.00-176.80] SPEAKER_01: Are you serious? Not at all.\n",
      "[176.80-179.00] John C. Riley: Are you as many of what does professionally, mate?\n",
      "[179.00-180.32] John C. Riley: Absolutely done.\n",
      "[180.32-181.84] Unknown: A world of...\n",
      "[181.84-182.84] Unknown: Congratulations.\n",
      "[182.84-184.84] Unknown: CHEERING AND APPLAUSE\n",
      "[184.84-186.16] Unknown: Thank you.\n",
      "[186.16-187.16] Unknown: Thank you very much.\n",
      " Found 1 checkworthy statements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\20\\clip_1.mp4 from 139.76s to 143.92s\n",
      " Finished cutting video into clips. Saved to statement_clips\\20\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['cd3aa96e173e484a9882dd10dcb3f683_clip_1.mp4'], 'pred': [0.6749477982521057], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 139.76, 'end': 143.92, 'speaker': 'SPEAKER_01', 'text': \"I thought this was supposed to be the comeback story of the year. I mean, I got knocked out by Mark Reilens. I mean, they've even heard of the guy.\", 'reason': \"This statement involves a claim about expectations around a potential award outcome and mentions another public figure, Mark Rylance, who isn't widely known, suggesting a check of the claim's basis in reality.\", 'context': 'This was likely part of an acceptance speech or commentary at a film awards ceremony, with humorous impressions and banter, possibly referencing an actual award season.', 'frame_path': 'statement_frames\\\\SPEAKER_01_013976.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.6749477982521057}]\n",
      " Statement I thought this was supposed to be the comeback story of the year. I mean, I got knocked out by Mark Reilens. I mean, they've even heard of the guy. is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\20.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 22: ../data/dfw_youtube_release\\21.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\21.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l SPEAKER_00...\n",
      " SPEAKER_00  Will Ferrell\n",
      "\n",
      " ang x l SPEAKER_01...\n",
      " SPEAKER_01  Will Ferrell\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.00] Unknown: Hi, I'm Amy.\n",
      "[2.80-4.80] Unknown: Hi, I'm Amy. Oh my god.\n",
      "[6.80-8.80] Unknown: Hey, hi\n",
      "[9.80-13.24] Unknown: I'm off now. We're about to take the wired autocomplete\n",
      "[17.24-21.08] Unknown: His any poor Swedish and I could see why there could be\n",
      "[23.08-25.08] Will Ferrell: No, disgusting\n",
      "[25.72-29.84] Unknown: Is Amy polar dating Rashida Jones? Yeah\n",
      "[31.00-35.40] Unknown: Clear clean and cut is Amy poor related to Louis CK\n",
      "[37.92-41.96] Unknown: I mean I played his sister once in his television show and we do have the same mother\n",
      "[43.68-45.68] Unknown: Well, I don't know\n",
      "[47.48-53.52] Unknown: Is Amy polar vegan well no, I I respect vegans I get it\n",
      "[53.52-56.20] Will Ferrell: They don't eat something with the face, but you I love it\n",
      "[56.20-60.40] Will Ferrell: I love this black state. Yeah, it's important to make sure it's deep\n",
      "[62.44-66.08] Unknown: Yes, I am that I'm the Jack Black character in the city\n",
      "[66.84-71.20] Will Ferrell: It's so funny that no one has known that for so long you've got to see the next car\n",
      "[71.20-81.24] Will Ferrell: Okay, oh, well when is Will Ferrell on sports center when I do something sports related when was Will Ferrell\n",
      "[82.00-84.00] Unknown: on Spongebob\n",
      "[86.56-92.96] Unknown: Maybe some piece impersonate yeah, and we're assuming that Spongebob or they're talking about the car\n",
      "[92.96-97.44] Will Ferrell: Yeah, maybe Spongebob is some other guy. Yeah, oh\n",
      "[100.24-102.24] Unknown: Okay, bad I didn't really answer\n",
      "[103.92-108.92] Unknown: When was Will Ferrell born I was born here we go\n",
      "[109.96-114.08] Will Ferrell: July 16 1980 cancer cancer\n",
      "[114.08-120.44] Will Ferrell: Real in contact you sideways like the rap. Yes, hard shell hard shell sensitive inside\n",
      "[122.44-124.64] Will Ferrell: When did Will Ferrell die\n",
      "[125.80-129.92] Unknown: He died in a hangover as we actually falsely reported\n",
      "[130.40-138.64] Unknown: We leave the report when we were shooting blades of glory. Oh, I love the internet when did Will Ferrell learn Spanish in high school and through college\n",
      "[138.64-143.76] Will Ferrell: And then employed that knowledge of Spanish in the major emotion picture\n",
      "[144.36-149.60] Unknown: Casa de mi padre. Hmm. That was that was very which no one see\n",
      "[150.28-152.24] Unknown: You know you like to do that like I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\21\\clip_1.mp4 from 109.96s to 114.08s\n",
      " Finished cutting video into clips. Saved to statement_clips\\21\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['d790d68fcc7c47a8aa653b2462c46c18_clip_1.mp4'], 'pred': [0.558125376701355], 'klass': ['uncategorized'], 'pred_label': ['FAKE'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 109.96, 'end': 114.08, 'speaker': 'Will Ferrell', 'text': 'July 16 1980 cancer cancer', 'reason': \"The statement includes personal information about Will Ferrell's date of birth, which is a factual claim and can be verified.\", 'context': 'The quote occurred during a light-hearted and informal discussion, possibly as part of a humorous interview segment, where questions about personal trivia were being addressed.', 'frame_path': 'statement_frames\\\\Will Ferrell_010996.jpg', 'deepfake_label': 'FAKE', 'deepfake_score': 0.558125376701355}]\n",
      " Statement July 16 1980 cancer cancer is marked as deepfake. Skipping fact-check.\n",
      " Finished processing video ../data/dfw_youtube_release\\21.mp4\n",
      "Deepfake label: FAKE, Fact-check label: False\n",
      " Processing video 23: ../data/dfw_youtube_release\\22.mp4\n",
      " Extracting statements from video...\n",
      " Extracting audio...\n",
      "MoviePy - Writing audio in audios\\22.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      " Diarizing speakers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\pyannote\\audio\\models\\blocks\\pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1839.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:d:\\LopHoc\\Thesis\\Ver3\\code\\KLTN-Ver3\\research\\myenv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assigning speakers...\n",
      " Inferring speaker names from transcript...\n",
      " Extracting frames for unknown speakers...\n",
      " Identifying unknown speakers with GPT...\n",
      "\n",
      " ang x l unknown...\n",
      " unknown  Lil Xan\n",
      " Generating final transcript...\n",
      " Finding checkworthy statements...\n",
      " ang x l phn 1/1...\n",
      "[0.00-2.84] Unknown: I decided my whole got talent, right?\n",
      "[2.84-4.68] Name not available: Host Garver and Duda shit, yikes.\n",
      "[4.68-6.92] Name not available: I'm a businessman and then I've been in dammit.\n",
      "[6.92-9.72] Name not available: I'm a bendin' down and I'm a liquor ruffin' and dickin' down.\n",
      "[9.72-11.64] Name not available: She gonna turn around and I'm a kicker rower.\n",
      "[11.64-13.04] Name not available: She gonna tie that shit with say,\n",
      "[13.04-15.12] Name not available: how you make it though, how you fathol up.\n",
      "[15.12-16.60] Name not available: We're bunch of scumbags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:C:\\Users\\Nghia\\AppData\\Local\\Temp\\ipykernel_12960\\3015591568.py:39: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  statement_dict = s.dict()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 1 checkworthy statements.\n",
      " Detecting deepfake...\n",
      " Cutting video into clips...\n",
      " Cutting statement_clips\\22\\clip_1.mp4 from 4.68s to 6.92s\n",
      " Finished cutting video into clips. Saved to statement_clips\\22\n",
      " Detecting deepfake in clips...\n",
      " Sending batch videos to deepfake API...\n",
      " Detection results:\n",
      "clip_1.mp4: {'name': ['f862a78159f34cb88517ef7e9368239c_clip_1.mp4'], 'pred': [0.481059193611145], 'klass': ['uncategorized'], 'pred_label': ['REAL'], 'correct_label': ['unknown']}\n",
      " Finished detecting deepfake.\n",
      " Finished detecting deepfake.\n",
      "[{'start': 4.68, 'end': 6.92, 'speaker': 'Unknown', 'text': \"I'm a businessman and then I've been in dammit.\", 'reason': 'Contains a factual claim about being a businessman, which could be verified.', 'context': 'The statement was made in a casual conversation, potentially an informal interview or show segment with limited context on specific political or economic discussions.', 'frame_path': 'statement_frames\\\\Unknown_000468.jpg', 'deepfake_label': 'REAL', 'deepfake_score': 0.481059193611145}]\n",
      " Enriching statements with articles...\n",
      "https://www.bing.com/search?q=I'm a businessman and then I've been in dammit.\n",
      "Found 9 relevant links:\n",
      "[<selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.21\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.22\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.23\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.24\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.25\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.26\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.27\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.28\")>, <selenium.webdriver.remote.webelement.WebElement (session=\"4dc269d2fd58e8f809957fa1e4ff5857\", element=\"f.E88610C308EBC7792B4B8BF7C17F31D1.d.C33E73DC2CBB587C1DD07809E91503C5.e.29\")>]\n",
      "Blink-182 - Dammit Lyrics | AZLyrics.com\n",
      "https://www.bing.com/ck/a?!&&p=bf2d851ad398a3b4628c2045ec386178d31f8f3b48af4074ddbee5c608be7256JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=1dd787ff-d728-668e-163a-91ddd6026747&u=a1aHR0cHM6Ly93d3cuYXpseXJpY3MuY29tL2x5cmljcy9ibGluazE4Mi9kYW1taXQuaHRtbA&ntb=1\n",
      "Blink-182 - Dammit Lyrics - YouTube\n",
      "https://www.bing.com/ck/a?!&&p=f9117a7fdd7f286078ba3b6cf70f1189b62c1e569033f61de1dc9f93f6120a7dJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=1dd787ff-d728-668e-163a-91ddd6026747&u=a1aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g_dj1KTTdia1E0SFVLMA&ntb=1\n",
      "Found 3 relevant links:\n",
      "[{'title': \"I'm a businessman and then I'v\", 'link': \"https://www.bing.com/search?q=I'm a businessman and then I've been in dammit.\"}, {'title': 'Blink-182 - Dammit Lyrics | AZLyrics.com', 'link': 'https://www.bing.com/ck/a?!&&p=bf2d851ad398a3b4628c2045ec386178d31f8f3b48af4074ddbee5c608be7256JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=1dd787ff-d728-668e-163a-91ddd6026747&u=a1aHR0cHM6Ly93d3cuYXpseXJpY3MuY29tL2x5cmljcy9ibGluazE4Mi9kYW1taXQuaHRtbA&ntb=1'}, {'title': 'Blink-182 - Dammit Lyrics - YouTube', 'link': 'https://www.bing.com/ck/a?!&&p=f9117a7fdd7f286078ba3b6cf70f1189b62c1e569033f61de1dc9f93f6120a7dJmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=1dd787ff-d728-668e-163a-91ddd6026747&u=a1aHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g_dj1KTTdia1E0SFVLMA&ntb=1'}]\n",
      " Attempt 1: Crawling https://www.bing.com/search?q=I'm a businessman and then I've been in dammit. with wait_time=10s\n",
      " Attempt 1: Crawling https://www.bing.com/ck/a?!&&p=bf2d851ad398a3b4628c2045ec386178d31f8f3b48af4074ddbee5c608be7256JmltdHM9MTc1MTg0NjQwMA&ptn=3&ver=2&hsh=4&fclid=1dd787ff-d728-668e-163a-91ddd6026747&u=a1aHR0cHM6Ly93d3cuYXpseXJpY3MuY29tL2x5cmljcy9ibGluazE4Mi9kYW1taXQuaHRtbA&ntb=1 with wait_time=10s\n",
      " Crawled content from https://www.bing.com/search?q=I'm a businessman and then I've been in dammit.:\n",
      "Blink-182 \"Dammit\": It's alright to tell me what you think about me I won't try to argue or hold it against you I know t...\n",
      "Aug 16, 2010 KIng of the Web:http://kingofweb.com/users/3958694902I DO NOT OWN ANY RIGHTS TO THIS SONG BAND OR LABELLyrics:It's alrightto tell mewhat you thinkabout meI \n",
      "Jun 12, 2023 A day late, a buck short I'm writing the report On losing and failing When I move, I'm flailing now And it's happened once again I'll turn to a friend Someone that understands And......\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "VIDEO_DIR = \"../data/dfw_youtube_release\"\n",
    "SAVE_FILE = \"fact_check_results.json\"\n",
    "BATCH_SIZE = 10  # Mi ln lu kt qu sau 10 video\n",
    "START_INDEX = 0  # C th thay i thnh 100, 200,...\n",
    "NUM_VIDEOS = 50  # S lng video bn mun x l ln ny\n",
    "\n",
    "# Ly danh sch video theo th t s\n",
    "all_files = sorted([\n",
    "    f for f in os.listdir(VIDEO_DIR)\n",
    "    if f.endswith((\".mp4\", \".avi\", \".mov\"))\n",
    "], key=lambda x: int(x.split(\".\")[0]))  # m bo th t ng theo s\n",
    "\n",
    "# Chn mt tp con video cn x l\n",
    "video_files = [\n",
    "    os.path.join(VIDEO_DIR, f)\n",
    "    for f in all_files[START_INDEX: START_INDEX + NUM_VIDEOS]\n",
    "]\n",
    "\n",
    "# Nu c file c th load tip, khng th to danh sch trng\n",
    "if os.path.exists(SAVE_FILE):\n",
    "    with open(SAVE_FILE, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# Trnh x l li cc video  c kt qu\n",
    "processed_videos = {r[\"video_path\"] for r in results}\n",
    "pending_videos = [vp for vp in video_files if vp not in processed_videos]\n",
    "\n",
    "batch_counter = 0\n",
    "\n",
    "for idx, video_path in enumerate(pending_videos, 1):\n",
    "    print(f\" Processing video {idx + START_INDEX}: {video_path}\")\n",
    "    try:\n",
    "        result = fact_check_video(video_path)\n",
    "        print(f\" Finished processing video {video_path}\")\n",
    "        print(f\"Deepfake label: {result['deepfake_label']}, Fact-check label: {result['label']}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing video {video_path}: {e}\")\n",
    "        result = None\n",
    "\n",
    "    results.append({\n",
    "        \"video_path\": video_path,\n",
    "        \"deepfake_label\": result[\"deepfake_label\"],\n",
    "        \"label\": result[\"label\"],\n",
    "    })\n",
    "\n",
    "    batch_counter += 1\n",
    "\n",
    "    # Sau mi 10 video th lu li file\n",
    "    if batch_counter >= BATCH_SIZE:\n",
    "        with open(SAVE_FILE, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\" Saved checkpoint after {idx + START_INDEX} videos\")\n",
    "        batch_counter = 0\n",
    "\n",
    "# Sau vng lp, nu cn batch cha lu th lu nt\n",
    "if batch_counter > 0:\n",
    "    with open(SAVE_FILE, \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\" Final checkpoint saved after {len(results)} videos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
